description = "H2O Core"

apply from: '../gradle/java.gradle'
apply from: '../gradle/cp.gradle'

mainClassName = "water.H2O"

dependencies {
  // Required for h2o-core
  compile "joda-time:joda-time:2.3"

  compile("log4j:log4j:1.2.15") { 
    exclude module: "activation" 
    exclude module: "jms" 
    exclude module: "jmxri" 
    exclude module: "jmxtools" 
    exclude module: "mail" 
  }

  // WARNING: turning back on transitive dependencies brings in 19M in 37 jars... 
  // All unneeded to read from HDFS.
  compile("org.apache.hadoop:hadoop-common:2.3.0") { 
    transitive = false
  }
  // guava only needed by hadoop now, not by H2O
  compile "com.google.guava:guava:16.0.1"
  compile "commons-logging:commons-logging:1.1.1"
  compile "commons-collections:commons-collections:3.0"
  compile "org.javassist:javassist:3.18.2-GA"

  testCompile "junit:junit:4.11"
}

javadoc {
  exclude "**/fvec/C*Chunk.java/**"
  exclude "**/nbhm/**"
}

jar {
  manifest {
    attributes 'Main-Class': 'water.H2O'
  }
}

// The default 'test' behavior is broken in that it does not grok clusters.
// For H2O, all tests need to be run on a cluster, where each JVM is
// "free-running" - it's stdout/stderr are NOT hooked by another process.  If
// they are hooked (e.g., by the gradle driver process) then the stdout/err get
// buffered and when all CPUs are maxed out (happens over a large fraction of
// the test run) no output is visible.  If the JVMs then crash (again, common
// enough), we get NO output for the test run.  So instead we need to arrange a
// complete cluster of free-running JVMs and redirect all output (at the OS
// level) to files - then scrape the files later for test results.
test {
  dependsOn cpLibs, testMultiNode, testJar

  // Defeat task 'test' by running no tests.
  exclude '**'
}

apply from: '../gradle/javaIgnoreSymbolFile.gradle'

