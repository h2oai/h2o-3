\section{Deployment}
Since Sparkling Water is designed as a regular Spark application, its deployment cycle is strictly driven by Spark deployment strategies (refer to Spark documentation\footnote{Spark deployment guide \url{http://spark.apache.org/docs/latest/cluster-overview.html}}). Spark applications are deployed by the \texttt{spark-submit}~\footnote{Submitting Spark applications \url{http://spark.apache.org/docs/latest/submitting-applications.html}} script that handles all deployment scenarios:

\begin{lstlisting}[style=Bash]
./bin/spark-submit \
  --class <main-class> \
  --master <master-url> \
  --conf <key>=<value> \
  ... # other options \
  <application-jar> [application-arguments]
\end{lstlisting}

\begin{itemize}
	\item \texttt{--class}: Name of main class with \texttt{main} method to be executed. For example, the \texttt{water.SparklingWaterDriver} application launches H2O services.
	\item \texttt{--master}: Location of Spark cluster
	\item \texttt{--conf}: Specifies any configuration property using the format \texttt{key=value}
	\item \texttt{application-jar}: Jar file with all classes and dependencies required for application execution
	\item \texttt{application-arguments}: Arguments passed to the main method of the class via the \texttt{--class} option
\end{itemize}

\subsection{Referencing Sparkling Water}

\subsubsection{Using Fatjar}
The Sparkling Water archive provided at \url{http://h2o.ai/download} contains a Fatjar with all classes required for Sparkling Water run.

An application submission with Sparkling Water Fatjar is using the \texttt{--jars} option which references included fatjar.

\pagebreak
\begin{lstlisting}[style=Bash]
$SPARK_HOME/bin/spark-submit \
  --jars assembly/build/libs/sparkling-water-assembly-2.2.2-all.jar \
  --class org.apache.spark.examples.h2o.CraigslistJobTitlesStreamingApp \
  /dev/null
\end{lstlisting}

\subsubsection{Using the Spark Package}

Sparkling Water is also published as a Spark package. The benefit of using the package is that you can use it directly from your Spark distribution without need to download Sparkling Water.

For example, if you have Spark version 2.2.0 and would like to use Sparkling Water version 2.2.2 and launch example \texttt{CraigslistJobTitlesStreamingApp}, then you can use the following command:

\begin{lstlisting}[style=Bash]
$SPARK_HOME/bin/spark-submit \
  --packages ai.h2o:sparkling-water-core_2.11:2.2.2,ai.h2o:sparkling-water-examples_2.11:2.2.2,no.priv.garshol.duke:duke:1.2 \
  --class org.apache.spark.examples.h2o.CraigslistJobTitlesStreamingApp \
  /dev/null
\end{lstlisting}

Note: The duke library has to be explicitly added when using Sparkling Water via the \texttt{--packages} option. The library
contains a pom file, which the Spark dependency resolver can't properly resolve, and therefore this library will be missing if not explicitly
specified. This is only required in Sparkling Water 2.2.2 and older, 2.1.16 and older, and 2.0.17 and older. In newer Sparkling Water
versions, the only dependency that needs to be put into \texttt{--packages} is \texttt{sparkling-water-package\_2.11:\{version\}}.
This package contains all the required dependencies.


The Spark option \texttt{--packages} points to coordinate of published Sparkling Water package in Maven repository.

\newpage
The similar command works for spark-shell:

\begin{lstlisting}[style=Bash]
$SPARK_HOME/bin/spark-shell \
 --packages ai.h2o:sparkling-water-core_2.11:2.2.2,ai.h2o:sparkling-water-examples_2.11:2.2.2,no.priv.garshol.duke:duke:1.2
\end{lstlisting}

Note: When you are using Spark packages, you do not need to download Sparkling Water distribution. Spark installation is sufficient.

\subsection{Target Deployment Environments}
Sparkling Water supports deployments to the following Spark cluster types:
\begin{itemize}
	\item{Local cluster}
	\item{Standalone cluster} 
	\item{YARN cluster}
\end{itemize}

\subsubsection{Local cluster}
The local cluster is identified by the following master URLs - \texttt{local}, \texttt{local[K]}, or \texttt{local[*]}. In this case, the cluster is composed of a single JVM and is created during application submission.

For example, the following command will run the ChicagoCrimeApp application inside a single JVM with a heap size of 5g:
\begin{lstlisting}[style=Bash]
$SPARK_HOME/bin/spark-submit \ 
  --conf spark.executor.memory=5g \
  --conf spark.driver.memory=5g \
  --master local[*] \
  --packages ai.h2o:sparkling-water-examples_2.11:2.2.2,no.priv.garshol.duke:duke:1.2 \
  --class org.apache.spark.examples.h2o.ChicagoCrimeApp \
  /dev/null
\end{lstlisting}

\subsubsection{On a Standalone Cluster}
For AWS deployments or local private clusters, the standalone cluster deployment\footnote{Refer to Spark documentation~\url{http://spark.apache.org/docs/latest/spark-standalone.html}} is typical. Additionally, a Spark standalone cluster is also provided by Hadoop distributions like CDH or HDP. The cluster is identified by the URL \texttt{spark://IP:PORT}.

The following command deploys the \texttt{ChicagoCrimeApp} on a standalone cluster where the master node is exposed on IP \texttt{machine-foo.bar.com} and port \texttt{7077}:

\begin{lstlisting}[style=Bash]
$SPARK_HOME/bin/spark-submit \ 
  --conf spark.executor.memory=5g \
  --conf spark.driver.memory=5g \
  --master spark://machine-foo.bar.com:7077 \
  --packages ai.h2o:sparkling-water-examples_2.11:2.2.2,no.priv.garshol.duke:duke:1.2 \
  --class org.apache.spark.examples.h2o.ChicagoCrimeApp \
  /dev/null
\end{lstlisting}

In this case, the standalone Spark cluster must be configured to provide the requested 5g of memory per executor node. 

\subsubsection{On a YARN Cluster}
Because it provides effective resource management and control, most production environments use YARN for cluster deployment.\footnote{See Spark documentation~\url{http://spark.apache.org/docs/latest/running-on-yarn.html}} 
In this case, the environment must contain the shell variable~\texttt{HADOOP\_CONF\_DIR} or \texttt{YARN\_CONF\_DIR} which point to Hadoop configuration directory (e.g., \texttt{/etc/hadoop/conf}).

\begin{lstlisting}[style=Bash]
$SPARK_HOME/bin/spark-submit \ 
  --conf spark.executor.memory=5g \
  --conf spark.driver.memory=5g \
  --num-executors 5 \
  --master yarn \
  --deploy-mode client
  --packages ai.h2o:sparkling-water-examples_2.11:2.2.2,no.priv.garshol.duke:duke:1.2 \
  --class org.apache.spark.examples.h2o.ChicagoCrimeApp \
  /dev/null
\end{lstlisting}

The command in the example above creates a YARN job and requests for 5 nodes, each with 5G of memory. Master is set to \texttt{yarn}, and together with the deploy mode \texttt{client} option forces the driver to run in the client process.

\subsection{DataBricks Cloud}
This section describes how to use Sparkling Water and PySparkling with DataBricks.
The first part describes how to create a cluster for Sparkling Water/PySparkling and then discusses how to use Sparkling Water and PySparkling in Databricks.


DataBricks cloud is Integrated with Sparkling Water and Pysparkling. Currently, only internal Sparkling Water backend may
be used.


\subsubsection{Creating a Cluster}

\textbf{Requirements}:
\begin{itemize}
    \item Databricks Account
    \item AWS Account
\end{itemize}

\textbf{Steps}:
\begin{enumerate}
\item In Databricks, click \textbf{Create Cluster} in the Clusters dashboard.
\item Select your Databricks Runtime Version. (Note that in our demos, we are using `3.0 (includes Apache Spark 2.2.0, Scala 2.11)`.)
\item Select at least 3 workers.
\item Select 0 on-demand workers. On demand workers are currently not supported with Sparkling Water.
\item In the SSH tab, upload your public key.  You can create a public key by running the below command in a terminal session:
\begin{lstlisting}[style=Bash]
ssh-keygen -t rsa -b 4096 -C "your_email@example.com"
\end{lstlisting}
\item Click \textbf{Create Cluster}
\item Once the cluster has started, run the following command in a terminal session:
\begin{lstlisting}[style=Bash]
ssh ubuntu@<ec-2 driver host>.compute.amazonaws.com -p 2200 -i <path to your public/private key> -L 54321:localhost:54321
\end{lstlisting}
This will allow you to use the Flow UI.

(You can find the `ec-2 driver host` information in the SSH tab of the cluster.)

\end{enumerate}


\subsubsection{Running Sparkling Water}

\textbf{Requirements}:
\begin{itemize}
\item Sparkling Water Jar
\end{itemize}

\textbf{Steps}:
\begin{enumerate}
\item Create a new library containing the Sparkling Water jar.
\item Download the selected Sparkling Water version from \url{https://www.h2o.ai/download/}.
\item The jar file is located in the sparkling water zip file at the following location: `assembly/build/libs/sparkling-water-assembly\_*-all.jar`
\item Attach the Sparkling Water library to the cluster.
\item Create a new Scala notebook.
\item Create an H2O cloud inside the Spark cluster:
\begin{lstlisting}[style=Scala]
import org.apache.spark.h2o._
val h2oConf = new H2OConf(spark).set("spark.ui.enabled", "false")
val h2oContext = H2OContext.getOrCreate(spark, h2oConf)
\end{lstlisting}
You can access Flow by going to localhost:54321.
\end{enumerate}

\newpage
\subsubsection{Running PySparkling}

\textbf{Requirements}:
\begin{itemize}
\item PySparkling zip file
\item Python Module: request
\item Python Module: tabulate
\item Python Module: future
\item Python Module: colorama
\end{itemize}

\textbf{Steps}:
\begin{enumerate}
\item Create a new Python library containing the PySparkling zip file.
\item Download the selected Sparkling Water version from \url{https://www.h2o.ai/download/}.
\item The PySparkling zip file is located in the sparkling water zip file at the following location: `py/build/dist/h2o\_pysparkling\_*.zip.`
\item Create libraries for the following python modules: request, tabulate, future and colorama.
\item Attach the PySparkling library and python modules to the cluster.
\item Create a new python notebook.
\item Create an H2O cloud inside the Spark cluster:
\begin{lstlisting}[style=Python]
from pysparkling import *
h2oConf = H2OConf(spark).set("spark.ui.enabled", "false")
hc = H2OContext.getOrCreate(spark, h2oConf)
\end{lstlisting}

\end{enumerate}


To prevent a progress bar error, run the following cell at the beginning of the python notebook:

\begin{lstlisting}[style=Python]
# Patch to workaround progress bar error (this cell must be run before using h2o)
import sys

def isatty(self):
return False

type(sys.stdout).isatty = isatty
type(sys.stdout).encoding = "UTF-8"
\end{lstlisting}

Note: This is only required in Sparkling Water 2.2.2 and older, 2.1.16 and older, and 2.0.17 and older. This is
fixed in newer Sparkling Water versions.


\subsection{Sparkling Water Configuration Properties}
\label{sec:properties}
The following configuration properties can be passed to Spark to configure Sparking Water:

\subsubsection{Configuration Properties not Dependent on Selected Backend}
\textbf{Backend-independent generic parameters}
\begin{footnotesize}
\begin{longtable}[!ht]{l p{2.0cm} p{3.0cm}}
\toprule
Property name & Default & Description \\
\midrule
		
spark.ext.h2o.backend.cluster.mode & internal & This option can be set either to \texttt{internal} or \texttt{external}. When set to \texttt{external}, H2O Context is created by connecting to existing H2O cluster, otherwise H2O cluster located inside Spark is created. That means that each Spark executor will have one H2O instance running in it. The \texttt{internal} mode is not recommended for big clusters and clusters where Spark executors are not stable. \\ \addlinespace

spark.ext.h2o.cloud.name & Generated unique name & Name of H2O cluster. \\ \addlinespace

spark.ext.h2o.nthreads  & -1 & Limit for number of threads used by H2O. -1 means unlimited. \\ \addlinespace

spark.ext.h2o.disable.ga 	& true & Disable Google Analytics tracking for embedded H2O.\\  \addlinespace

spark.ext.h2o.repl.enabled & true & Decides whether H2O REPL is initiated. \\ \addlinespace

spark.ext.scala.int.default.num & 1 & Number of parallel REPL sessions started at the start of Sparkling Water. \\ \addlinespace

spark.ext.h2o.topology.change.listener.enabled & true &  Decides whether the listener the kills the H2O cluster upon the change of the underlying cluster's topology is enabled or not. \\ \addlinespace

spark.ext.h2o.spark.version.check.enabled & true & Enables check if run-time Spark version matches build time Spark version. \\ \addlinespace

spark.ext.h2o.fail.on.unsupported.spark.param & true & If unsupported Spark parameter is detected, then the application is forced to shutdown. \\ \addlinespace

spark.ext.h2o.jks & None & Path to Java KeyStore file. \\ \addlinespace

spark.ext.h2o.jks.pass & None & Password for Java KeyStore file. \\ \addlinespace

spark.ext.h2o.hash.login & false & Enable hash login. \\ \addlinespace

spark.ext.h2o.ldap.login & false & Enable LDAP login. \\ \addlinespace

spark.ext.h2o.kerberos.login & false & Enable Kerberos login. \\ \addlinespace

spark.ext.h2o.login.conf & None & Login configuration file. \\ \addlinespace

spark.ext.h2o.user.name & None & Override user name for cluster. \\ \addlinespace

spark.ext.h2o.internal\_security\_conf & None & Path to a file containing H2O or Sparkling Water internal security configuration. \\ \addlinespace

spark.ext.h2o.node.log.level & INFO & Set H2O node internal logging level. \\ \addlinespace

spark.ext.h2o.node.log.dir  & {user.dir}/h2ologs/ {SparkAppId} or YARN container dir  & Location of h2o logs on executor machine.\\ \addlinespace

spark.ext.h2o.ui.update.interval & 10000ms & Interval for updates of the Spark UI and History server in milliseconds. \\ \addlinespace

spark.ext.h2o.cloud.timeout & 60x1000 & Timeout (in msec) for cluster formation. \\ \addlinespace

spark.ext.h2o.node.enable.web & false & Enable or disable web on H2O worker nodes. It is disabled by default for security reasons. \\

\bottomrule
\end{longtable}
\end{footnotesize}


\textbf{Backend-independent H2O client parameters}
\begin{footnotesize}
\begin{longtable}[!ht]{l p{3.0cm} p{3.0cm}}
\toprule
Property name & Default  & Description \\
\midrule

spark.ext.h2o.client.flow.dir & None & Directory where flows from H2O Flow are saved. \\ \addlinespace

spark.ext.h2o.client.ip & None & IP of H2O client node. \\ \addlinespace

spark.ext.h2o.client.iced.dir & None & Location of iced directory for the driver instance. \\ \addlinespace

spark.ext.h2o.client.log.level & INFO & Set H2O client internal logging level (running inside Spark driver).\\  \addlinespace

spark.ext.h2o.client.log.dir & {user.dir}/h2ologs/ {SparkAppId} & Location of h2o logs on driver machine.\\  \addlinespace

spark.ext.h2o.client.port.base & 54321 & Port on which H2O client publishes its API. If already occupied, the next odd port is tried on so on. \\ \addlinespace

spark.ext.h2o.client.web.port & -1 & Exact client port to access web UI. -1 triggers automatic search for free port starting at spark.ext.h2o.port.base.\\ \addlinespace

spark.ext.h2o.client.verbose & false & The client outputs verbose log output directly into console. Enabling the flag increases the client log level to INFO. \\ \addlinespace

spark.ext.h2o.client.network.mask & None & Subnet selector for H2O client, this disables using IP reported by Spark but tries to find IP based on the specified mask. \\

\bottomrule
\end{longtable}
\end{footnotesize}

\subsubsection{Internal Backend Configuration Properties}
\textbf{Internal backend generic parameters}
\begin{footnotesize}
\begin{longtable}[!ht]{l p{2.0cm} p{3.0cm}}
\toprule
Property name & Default & Description \\
\midrule

spark.ext.h2o.flatfile & true & Use flatfile instead of multicast approach for creating H2O cluster. \\ \addlinespace

spark.ext.h2o.cluster.size & None & Expected number of workers of H2O cluster. Value None means automatic detection of cluster size. This number must be equal to number of Spark executors. \\ \addlinespace

spark.ext.h2o.dummy.rdd.mul.factor & 10 & Multiplication factor for dummy RDD generation. Size of dummy RDD is \texttt{spark.ext.h2o.cluster.size * spark.ext.h2o.dummy.rdd. mul.factor}. \\ \addlinespace

spark.ext.h2o.spreadrdd.retries & 10 & Number of retries for creation of an RDD spread across all existing Spark executors. \\ \addlinespace

spark.ext.h2o.default.cluster.size & 20 & Starting size of cluster in case that size is not explicitly configured. \\ \addlinespace

spark.ext.h2o.subseq.tries & 5 & Subsequent successful tries to figure out size of Spark cluster, which are producing the same number of nodes. \\ \addlinespace

spark.ext.h2o.internal\_secure\_connections & false & Enables secure communications among H2O nodes. The security is based on automatically generated keystore and truststore. This is equivalent for \texttt{-internal\_secure\_conections} option in H2O Hadoop deployments. \\ 


\bottomrule
\end{longtable}
\end{footnotesize}

\textbf{Internal backend H2O node parameters}
\begin{footnotesize}
\begin{longtable}[!ht]{l p{3.0cm} p{3.0cm}}
\toprule
Property name & Default & Description \\
\midrule

spark.ext.h2o.node.port.base & 54321 & Base port used for individual H2O nodes. \\ \addlinespace

spark.ext.h2o.node.iced.dir & None & Location of iced directory for H2O nodes on the Spark executors. \\ \addlinespace

spark.ext.h2o.node.network.mask & None & Subnet selector for H2O running inside Spark executors. This disables using IP reported by Spark but tries to find IP based on the specified mask. \\

\bottomrule
\end{longtable}
\end{footnotesize}



\subsubsection{External Backend Configuration Properties}
\textbf{External backend parameters}
\begin{footnotesize}
\begin{longtable}[!ht]{l l p{3.0cm}}
\toprule
Property name & Default & Description \\
\midrule

spark.ext.h2o.cloud.representative & None & ip:port of arbitrary H2O node to identify external H2O cluster. \\ \addlinespace

spark.ext.h2o.external.cluster.num.h2o.nodes & None & Number of H2O nodes to start in auto mode and wait for in manual mode when starting Sparkling Water in external H2O cluster mode. \\ \addlinespace

spark.ext.h2o.cluster.client.retry.timeout & 60000ms & Timeout in milliseconds specifying how often the check for availability of connected watchdog client is done. \\ \addlinespace

spark.ext.h2o.cluster.client.connect.timeout & 180000ms & Timeout in milliseconds for watchdog client connection. If the client is not connected to the external cluster in the given time, the cluster is killed. \\ \addlinespace

spark.ext.h2o.external.read.confirmation.timeout & 60s & Timeout for confirmation of read operation (H2O frame =\textgreater{} Spark frame) on external cluster. \\ \addlinespace

spark.ext.h2o.external.write.confirmation.timeout & 60s & Timeout for confirmation of write operation (Spark frame =\textgreater{} H2O frame) on external cluster. \\ \addlinespace

spark.ext.h2o.cluster.start.timeout & 120s & Timeout in seconds for starting H2O external cluster. \\ \addlinespace

spark.ext.h2o.cluster.info.name & None & Full path to a file which is used as the notification file for the startup of external H2O cluster. \\ \addlinespace

spark.ext.h2o.hadoop.memory & 6G & Amount of memory assigned to each H2O node on YARN/Hadoop. \\ \addlinespace

spark.ext.h2o.external.hdfs.dir & None & Path to the directory on HDFS used for storing temporary files. \\ \addlinespace

spark.ext.h2o.external.start.mode & manual & If this option is set to \texttt{auto} then H2O external cluster is automatically started using the provided H2O driver JAR on YARN, otherwise it is expected that the cluster is started by the user manually. \\ \addlinespace

spark.ext.h2o.external.h2o.driver & None & Path to H2O driver used during auto start mode. \\ \addlinespace

spark.ext.h2o.external.yarn.queue & None & YARN queue on which external H2O cluster is started.\\

\bottomrule
\end{longtable}
\end{footnotesize}



