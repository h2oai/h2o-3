\section{Sparkling Water Backends}

\subsection{Internal Backend}

In internal backend, H2O cloud is created automatically during the call of \texttt{H2OContext.getOrCreate}. Since it's not technically possible to get the number of executors in Spark, Sparkling Water tries to discover all executors at the initiation of \texttt{H2OContext} and starts H2O instance inside of each discovered executor. This solution is the easiest to deploy; however when Spark or YARN kills the executor, the whole H2O cluster goes down since H2O doesn't support high availability. The
same happens also for the case when a new executors join the cluster as the shape of the H2O cluster can't be changed later.
Internal backend is default for behaviour for Sparkling Water. It can be changed via spark configuration property
\texttt{spark.ext.h2o.backend.cluster.mode} to \textbf{external} or \textbf{internal}. Another way how to change type of backend is by calling \texttt{setExternalClusterMode()} or \texttt{setInternalClusterMode()} method on \texttt{H2OConf} class instance.
\texttt{H2OConf} is a simple wrapper around \texttt{SparkConf} and inherits all properties in spark configuration.

\texttt{H2OContext} can be explicitly started in internal backend mode as

\begin{lstlisting}[style=Scala]
val conf = new H2OConf(spark).setInternalClusterMode()
val h2oContext = H2OContext.getOrCreate(spark, conf)
\end{lstlisting}

If \texttt{spark.ext.h2o.backend.cluster.mode} property was set to \textbf{internal} either on the command line or on the \texttt{SparkConf}, the following call is sufficient:

\begin{lstlisting}[style=Scala]
val h2oContext = H2OContext.getOrCreate(spark)
\end{lstlisting}

or

\begin{lstlisting}[style=Scala]
    val conf = new H2OConf(spark)
    val h2oContext = H2OContext.getOrCreate(spark, conf)
\end{lstlisting}

if we want to pass some additional H2O configuration to Sparkling Water.

\subsection{External Backend}
In external backend, H2O cluster running separately from the rest of Spark application is used. This separation gives the user
more stability since Sparkling Water is no longer affected by Spark executors being killed, which can lead, as in previous mode, to H2O cloud kill as well.
If H2O cluster is deployed on YARN, it is required to start it on a YARN queue with YARN preemption disabled to ensure H2O nodes can't be killed by competing jobs.

There are two deployment strategies of external cluster: manual and automatic (YARN only). In manual mode, the user is responsible for starting H2O cluster and in automatic mode, the cluster is started automatically based on our configuration.
In both modes, regular H2O/H2O driver jar can't be used as main artifact for external H2O cluster. Instead a special, JAR file extended by classes required by Sparkling Water need to used.

For the released Sparkling Water versions, the extended H2O jar can be downloaded using the \texttt{./bin/get-extendend-h2o.sh} script. This script expects a single argument which specifies the Hadoop version for which the jar is to be obtained.

The following code downloads H2O extended JAR for the cdh5.8:

\begin{lstlisting}[style=Bash]
    ./bin/get-extended-h2o.sh cdh5.8
\end{lstlisting}
If we don't want to run on hadoop but you want to run H2O in standalone mode, we can get the corresponding extended H2O standalone jar as:

\begin{lstlisting}[style=Bash]
    ./bin/get-extended-h2o.sh standalone
\end{lstlisting}
If you want to see the list of supported Hadoop versions, just run the shell script without any arguments as:

\begin{lstlisting}[style=Bash]
    ./bin/get-extended-h2o.sh
\end{lstlisting}

The script downloads the jar to the current directory and prints the absolute path to the downloaded jar.

The following sections explain how to use external cluster in both modes. Let's assume for later sections that the path to the extended H2O/H2O driver jar file is available in \texttt{H2O\_EXTENDED\_JAR} environmental variable.

\subsubsection{Manual Mode of External Backend}

In this mode, we need to start H2O cluster before connecting to it manually. In general, H2O cluster can be started in two ways - using the multicast discovery of the other nodes and using the flatfile, where we manually specify the future locations of H2O nodes. We recommend to use flatfile to specify the location of nodes for production usage of Sparkling Water, but in simple environments where multicast is supported, the multicast discovery should work as well.
Let's have a look on how to start H2O cluster and connect to it from Sparkling Water in multicast environment. To start H2O cluster of 3 nodes, run the following line 3 times:

\begin{lstlisting}[style=Bash]
java -jar $H2O_EXTENDED_JAR -md5skip -name test
\end{lstlisting}

Don't forget the -md5skip argument, it's additional argument required for the external backend to work. After this step,
we should have H2O cluster of 3 nodes running and the nodes should have discovered each other using the multicast discovery.

Now, let's start Sparkling Water shell first as \texttt{./bin/sparkling-shell} and connect to the cluster:

\begin{lstlisting}[style=Scala]
import org.apache.spark.h2o._
val conf = new H2OConf(spark)
    .setExternalClusterMode()
    .useManualClusterStart()
    .setCloudName("test")
val hc = H2OContext.getOrCreate(spark, conf)
\end{lstlisting}

To connect to existing H2O cluster from Python, start PySparkling shell as \texttt{./bin/pysparkling} and do:

\begin{lstlisting}[style=Scala]
from pysparkling import *
conf = H2OConf(spark)
    .set_external_cluster_mode()
    .use_manual_cluster_start()
    .set_cloud_name("test")
hc = H2OContext.getOrCreate(spark, conf)
\end{lstlisting}

To start external H2O cluster where the nodes are discovered using the flatfile, you can run:

\begin{lstlisting}[style=bash]
java -jar $H2O_EXTENDED_JAR -md5skip -name test -flatfile path_to_flatfile
\end{lstlisting}

The flatfile should contain lines in format ip:port of nodes where H2O is supposed to run. To read more about flatfile and it's format, please see H2O's flatfile configuration property available at \url{https://github.com/h2oai/h2o-3/blob/master/h2o-docs/src/product/howto/H2O-DevCmdLine.md#flatfile}.
To connect to this external cluster, run the following commands in the corresponding shell ( Sparkling in case of Scala, PySparkling in case of Python):

Scala:
\begin{lstlisting}[style=Scala]
import org.apache.spark.h2o._
val conf = new H2OConf(spark)
    .setExternalClusterMode()
    .useManualClusterStart()
    .setH2OCluster("ip", port)
    .setCloudName("test")
val hc = H2OContext.getOrCreate(spark, conf)
\end{lstlisting}

Python:
\begin{lstlisting}[style=Python]
from pysparkling import *
conf = H2OConf(spark)
    .set_external_cluster_mode()
    .use_manual_cluster_start()
    .set_h2o_cluster("ip", port)
    .set_cloud_name("test")
hc = H2OContext.getOrCreate(spark, conf)
\end{lstlisting}

We can see that in this case we are using extra call \texttt{setH2OCluster} in Scala and \texttt{set\_h2o\_cluster} in Python. When the external cluster is started via the flatfile approach, we need to give Sparkling Water ip address and port of arbitrary node inside the H2O cloud in order to connect to the cluster. The ip and port of this node are passed as arguments to \texttt{setH2OCluster/set\_h2o\_cluster} method.

It's possible in both cases that node on which want to start Sparkling Watter shell is connected to more networks. In this case it can happen that H2O cloud decides to use addresses from network A, whilst Spark decides to use addresses for its executors and driver from network B. Later, when we start \texttt{H2OContext}, the special H2O client, running inside of the Spark Driver, can get the same IP address as the Spark driver and thus the rest of the H2O cloud can't see it. This shouldn't happen in environments where the nodes are connected to only one network, however we provide configuration how to deal with this case as well.

We can use method \texttt{setClientIp} in Scala and \texttt{set\_client\_ip} in Python available on \texttt{H2OConf} which expects IP address and sets this IP address for the H2O client running inside the Spark driver. The IP address passed to this method should be address of the node where Spark driver is about to run and should be from the same network as the rest of H2O cloud.

Let's say we have two H2O nodes on addresses 192.168.0.1 and 192.168.0.2 and also assume that Spark driver is available on 172.16.1.1 and the only executor is available on 172.16.1.2. The node with Spark driver is also connected to 192.168.0.x network with address 192.168.0.3.

In this case there is a chance that H2O client will use the address from 172.168.x.x network instead of the 192.168.0.x one, which can lead to the problem that H2O cloud and H2O client can't see each other.

We can force the client to use the correct address using the following configuration:

Scala:
\begin{lstlisting}[style=Scala]
import org.apache.spark.h2o._
val conf = new H2OConf(spark)
    .setExternalClusterMode()
    .useManualClusterStart()
    .setH2OCluster("ip", port)
    .setClientIp("192.168.0.3")
    .setCloudName("test")
val hc = H2OContext.getOrCreate(spark, conf)
\end{lstlisting}

Python:
\begin{lstlisting}[style=Python]
from pysparkling import *
conf = H2OConf(spark)
    .set_external_cluster_mode()
    .use_manual_cluster_start()
    .set_h2o_cluster("ip", port)
    .set_client_ip("192.168.0.3")
    .set_cloud_name("test")
hc = H2OContext.getOrCreate(spark, conf)
\end{lstlisting}

There is also a less strict configuration \texttt{setClientNetworkMask} in Scala and \texttt{set\_client\_network\_mask} in Python. Instead of its IP address equivalent, using this method we can force the H2O client to use just a specific network and leave up to the client which IP address from this network to use.

The same configuration can be applied when the H2O cluster has been started via multicast discovery.

\subsubsection{Automatic Mode of External Backend}

In automatic mode, H2O cluster is started automatically. The cluster can be started automatically only in YARN environment at the moment. We recommend this approach as it is easier to deploy external cluster in this mode ans it is also more suitable for production environments. When H2O cluster is start on YARN, it is started as map reduce job and it always use the flatfile approach for nodes to cloud up.

For this case to work, we need to extend H2O driver for the desired hadoop version as mentioned above. Let's assume the path to this extended H2O driver is stored in \texttt{H2O\_EXTENDED\_JAR} environmental property.

To start H2O cluster and connect to it from Spark application in Scala:
\begin{lstlisting}[style=Scala]
import org.apache.spark.h2o._
val conf = new H2OConf(spark)
    .setExternalClusterMode()
    .useAutoClusterStart()
    .setH2ODriverPath("path_to_extended_driver")
    .setNumOfExternalH2ONodes(1)
    .setMapperXmx("2G")
    .setYARNQueue("h2o_yarn_queue")
val hc = H2OContext.getOrCreate(spark, conf)
\end{lstlisting}


and in Python:
\begin{lstlisting}[style=Python]
from pysparkling import *
conf = H2OConf(spark)
    .set_external_cluster_mode()
    .use_auto_cluster_start()
    .set_h2o_driver_path("path_to_extended_driver")
    .set_num_of_external\_h2o\_nodes(1)
    .set_mapper_xmx("2G‚Äù)
    .set_yarn_queue("h2o_yarn_queue")
hc = H2OContext.getOrCreate(spark, conf)
\end{lstlisting}


In both cases we can see various configuration methods. We explain only the Scala ones since the python equivalents are doing exactly the same.

\begin{itemize}
    \item \texttt{setH2ODriverPath} method is used to tell Sparkling Water where it can find the extended H2O driver jar. This jar is passed to hadoop and used to start H2O cluster on YARN.
    \item \texttt{setNumOfExternalH2ONodes} method specifies how many H2O nodes we want to start.
    \item \texttt{setMapperXmx} method specifies how much memory each H2O node should have available.
    \item \texttt{setYarnQueue} method specifies YARN queue on which H2O cluster will be started. We highly recommend that this queue should have YARN preemption off in order to have stable H2O cluster.
\end{itemize}

When using \texttt{useAutoClusterStart} we do not need to call \texttt{setH2ODriverPath} explicitly in case when \texttt{H2O\_EXTENDED\_JAR} environmental property is set and pointing to that file. In this case Sparkling Water will fetch the path from this variable automatically. Also when \texttt{setCloudName} is not called, the name is set automatically and H2O cluster with that name is started.

It can also happen that we might need to use \texttt{setClientIp/set\_client\_ip} method as mentioned in the chapter above for the same reasons. The usage of this method in automatic mode is exactly the as in the manual mode.