{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H2O GBM Tuning Tutorial for Python\n",
    "\n",
    "### Arno Candel, PhD, Chief Architect, H2O.ai\n",
    "### Ported to Python by Navdeep Gill, M.S., Hacker/Data Scientist, H2O.ai\n",
    "\n",
    "In this tutorial, we show how to build a well-tuned H2O GBM model for a supervised classification task. We specifically don't focus on feature engineering and use a small dataset to allow you to reproduce these results in a few minutes on a laptop. This script can be directly transferred to datasets that are hundreds of GBs large and H2O clusters with dozens of compute nodes.\n",
    "\n",
    "You can download the source [from H2O's github repository](https://github.com/h2oai/h2o-3/blob/master/h2o-docs/src/product/tutorials/gbm/gbmTuning.ipynb).\n",
    "\n",
    "Ports to [R Markdown](https://github.com/h2oai/h2o-3/blob/master/h2o-docs/src/product/tutorials/gbm/gbmTuning.Rmd) and [Flow UI](https://raw.githubusercontent.com/h2oai/h2o-3/master/h2o-docs/src/product/flow/packs/examples/GBM_TuningGuide.flow) (now part of Example Flows) are available as well.\n",
    "\n",
    "## Installation of the H2O Python Package\n",
    "Either download H2O from [H2O.ai's website](http://h2o.ai/download) or install the latest version of H2O into Python with the following set of commands:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Install dependencies from command line (prepending with `sudo` if needed):\n",
    "\n",
    "```\n",
    "[sudo] pip install -U requests\n",
    "[sudo] pip install -U tabulate\n",
    "[sudo] pip install -U future\n",
    "[sudo] pip install -U six\n",
    "```\n",
    "\n",
    "The following command removes the H2O module for Python.\n",
    "```\n",
    "[sudo] pip uninstall h2o\n",
    "```\n",
    "\n",
    "Next, use pip to install this version of the H2O Python module.\n",
    "```\n",
    "[sudo] pip install http://h2o-release.s3.amazonaws.com/h2o/rel-turchin/8/Python/h2o-3.8.2.8-py2.py3-none-any.whl\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch an H2O cluster on localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "No instance found at ip and port: localhost:54321. Trying to start local jar...\n",
      "\n",
      "\n",
      "JVM stdout: /var/folders/fm/9q0qk_xs0wd07zr2v6d5mfv40000gn/T/tmpm9dEgv/h2o_arno_started_from_python.out\n",
      "JVM stderr: /var/folders/fm/9q0qk_xs0wd07zr2v6d5mfv40000gn/T/tmp_ublea/h2o_arno_started_from_python.err\n",
      "Using ice_root: /var/folders/fm/9q0qk_xs0wd07zr2v6d5mfv40000gn/T/tmpMuv0aE\n",
      "\n",
      "\n",
      "Java Version: java version \"1.8.0_60\"\n",
      "Java(TM) SE Runtime Environment (build 1.8.0_60-b27)\n",
      "Java HotSpot(TM) 64-Bit Server VM (build 25.60-b23, mixed mode)\n",
      "\n",
      "\n",
      "Starting H2O JVM and connecting: .............. Connection successful!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/IPython/core/formatters.py:92: DeprecationWarning: DisplayFormatter._ipython_display_formatter_default is deprecated: use @default decorator instead.\n",
      "  def _ipython_display_formatter_default(self):\n",
      "/usr/local/lib/python2.7/site-packages/IPython/core/formatters.py:98: DeprecationWarning: DisplayFormatter._formatters_default is deprecated: use @default decorator instead.\n",
      "  def _formatters_default(self):\n",
      "/usr/local/lib/python2.7/site-packages/IPython/core/formatters.py:677: DeprecationWarning: PlainTextFormatter._deferred_printers_default is deprecated: use @default decorator instead.\n",
      "  def _deferred_printers_default(self):\n",
      "/usr/local/lib/python2.7/site-packages/IPython/core/formatters.py:669: DeprecationWarning: PlainTextFormatter._singleton_printers_default is deprecated: use @default decorator instead.\n",
      "  def _singleton_printers_default(self):\n",
      "/usr/local/lib/python2.7/site-packages/IPython/core/formatters.py:672: DeprecationWarning: PlainTextFormatter._type_printers_default is deprecated: use @default decorator instead.\n",
      "  def _type_printers_default(self):\n",
      "/usr/local/lib/python2.7/site-packages/IPython/core/formatters.py:669: DeprecationWarning: PlainTextFormatter._singleton_printers_default is deprecated: use @default decorator instead.\n",
      "  def _singleton_printers_default(self):\n",
      "/usr/local/lib/python2.7/site-packages/IPython/core/formatters.py:672: DeprecationWarning: PlainTextFormatter._type_printers_default is deprecated: use @default decorator instead.\n",
      "  def _type_printers_default(self):\n",
      "/usr/local/lib/python2.7/site-packages/IPython/core/formatters.py:677: DeprecationWarning: PlainTextFormatter._deferred_printers_default is deprecated: use @default decorator instead.\n",
      "  def _deferred_printers_default(self):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime: </td>\n",
       "<td>1 seconds 514 milliseconds </td></tr>\n",
       "<tr><td>H2O cluster version: </td>\n",
       "<td>3.8.3.3</td></tr>\n",
       "<tr><td>H2O cluster name: </td>\n",
       "<td>H2O_started_from_python_arno_hfo986</td></tr>\n",
       "<tr><td>H2O cluster total nodes: </td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster total free memory: </td>\n",
       "<td>3.56 GB</td></tr>\n",
       "<tr><td>H2O cluster total cores: </td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster allowed cores: </td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster healthy: </td>\n",
       "<td>True</td></tr>\n",
       "<tr><td>H2O Connection ip: </td>\n",
       "<td>127.0.0.1</td></tr>\n",
       "<tr><td>H2O Connection port: </td>\n",
       "<td>54321</td></tr>\n",
       "<tr><td>H2O Connection proxy: </td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>Python Version: </td>\n",
       "<td>2.7.11</td></tr></table></div>"
      ],
      "text/plain": [
       "------------------------------  -----------------------------------\n",
       "H2O cluster uptime:             1 seconds 514 milliseconds\n",
       "H2O cluster version:            3.8.3.3\n",
       "H2O cluster name:               H2O_started_from_python_arno_hfo986\n",
       "H2O cluster total nodes:        1\n",
       "H2O cluster total free memory:  3.56 GB\n",
       "H2O cluster total cores:        8\n",
       "H2O cluster allowed cores:      8\n",
       "H2O cluster healthy:            True\n",
       "H2O Connection ip:              127.0.0.1\n",
       "H2O Connection port:            54321\n",
       "H2O Connection proxy:\n",
       "Python Version:                 2.7.11\n",
       "------------------------------  -----------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import h2o\n",
    "import numpy as np\n",
    "import math\n",
    "h2o.init(nthreads=-1, strict_version_check=True)\n",
    "## optional: connect to a running H2O cluster\n",
    "#h2o.init(ip=\"mycluster\", port=55555)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Import the data into H2O \n",
    "Everything is scalable and distributed from now on. All processing is done on the fully multi-threaded and distributed H2O Java-based backend and can be scaled to large datasets on large compute clusters.\n",
    "Here, we use a small public dataset ([Titanic](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/Titanic.html)), but you can use datasets that are hundreds of GBs large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parse Progress: [##################################################] 100%\n",
      "[1309, 14]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th style=\"text-align: right;\">  pclass</th><th style=\"text-align: right;\">  survived</th><th>name                                           </th><th>sex   </th><th style=\"text-align: right;\">    age</th><th style=\"text-align: right;\">  sibsp</th><th style=\"text-align: right;\">  parch</th><th style=\"text-align: right;\">  ticket</th><th style=\"text-align: right;\">    fare</th><th>cabin  </th><th>embarked  </th><th style=\"text-align: right;\">  boat</th><th style=\"text-align: right;\">  body</th><th>home.dest                      </th></tr>\n",
       "<tr><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">         1</td><td>Allen  Miss. Elisabeth Walton                  </td><td>female</td><td style=\"text-align: right;\">29     </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">   24160</td><td style=\"text-align: right;\">211.338 </td><td>B5     </td><td>S         </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">   nan</td><td>St Louis  MO                   </td></tr>\n",
       "<tr><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">         1</td><td>Allison  Master. Hudson Trevor                 </td><td>male  </td><td style=\"text-align: right;\"> 0.9167</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">  113781</td><td style=\"text-align: right;\">151.55  </td><td>C22 C26</td><td>S         </td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">   nan</td><td>Montreal  PQ / Chesterville  ON</td></tr>\n",
       "<tr><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">         0</td><td>Allison  Miss. Helen Loraine                   </td><td>female</td><td style=\"text-align: right;\"> 2     </td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">  113781</td><td style=\"text-align: right;\">151.55  </td><td>C22 C26</td><td>S         </td><td style=\"text-align: right;\">   nan</td><td style=\"text-align: right;\">   nan</td><td>Montreal  PQ / Chesterville  ON</td></tr>\n",
       "<tr><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">         0</td><td>Allison  Mr. Hudson Joshua Creighton           </td><td>male  </td><td style=\"text-align: right;\">30     </td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">  113781</td><td style=\"text-align: right;\">151.55  </td><td>C22 C26</td><td>S         </td><td style=\"text-align: right;\">   nan</td><td style=\"text-align: right;\">   135</td><td>Montreal  PQ / Chesterville  ON</td></tr>\n",
       "<tr><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">         0</td><td>Allison  Mrs. Hudson J C (Bessie Waldo Daniels)</td><td>female</td><td style=\"text-align: right;\">25     </td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">  113781</td><td style=\"text-align: right;\">151.55  </td><td>C22 C26</td><td>S         </td><td style=\"text-align: right;\">   nan</td><td style=\"text-align: right;\">   nan</td><td>Montreal  PQ / Chesterville  ON</td></tr>\n",
       "<tr><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">         1</td><td>Anderson  Mr. Harry                            </td><td>male  </td><td style=\"text-align: right;\">48     </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">   19952</td><td style=\"text-align: right;\"> 26.55  </td><td>E12    </td><td>S         </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">   nan</td><td>New York  NY                   </td></tr>\n",
       "<tr><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">         1</td><td>Andrews  Miss. Kornelia Theodosia              </td><td>female</td><td style=\"text-align: right;\">63     </td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">   13502</td><td style=\"text-align: right;\"> 77.9583</td><td>D7     </td><td>S         </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">   nan</td><td>Hudson  NY                     </td></tr>\n",
       "<tr><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">         0</td><td>Andrews  Mr. Thomas Jr                         </td><td>male  </td><td style=\"text-align: right;\">39     </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">  112050</td><td style=\"text-align: right;\">  0     </td><td>A36    </td><td>S         </td><td style=\"text-align: right;\">   nan</td><td style=\"text-align: right;\">   nan</td><td>Belfast  NI                    </td></tr>\n",
       "<tr><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">         1</td><td>Appleton  Mrs. Edward Dale (Charlotte Lamson)  </td><td>female</td><td style=\"text-align: right;\">53     </td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">   11769</td><td style=\"text-align: right;\"> 51.4792</td><td>C101   </td><td>S         </td><td style=\"text-align: right;\">   nan</td><td style=\"text-align: right;\">   nan</td><td>Bayside  Queens  NY            </td></tr>\n",
       "<tr><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">         0</td><td>Artagaveytia  Mr. Ramon                        </td><td>male  </td><td style=\"text-align: right;\">71     </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\"> 49.5042</td><td>       </td><td>C         </td><td style=\"text-align: right;\">   nan</td><td style=\"text-align: right;\">    22</td><td>Montevideo  Uruguay            </td></tr>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method H2OFrame.head of >\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th style=\"text-align: right;\">  pclass</th><th style=\"text-align: right;\">  survived</th><th>name                                           </th><th>sex   </th><th style=\"text-align: right;\">    age</th><th style=\"text-align: right;\">  sibsp</th><th style=\"text-align: right;\">  parch</th><th style=\"text-align: right;\">  ticket</th><th style=\"text-align: right;\">    fare</th><th>cabin  </th><th>embarked  </th><th style=\"text-align: right;\">  boat</th><th style=\"text-align: right;\">  body</th><th>home.dest                      </th></tr>\n",
       "<tr><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">         1</td><td>Allen  Miss. Elisabeth Walton                  </td><td>female</td><td style=\"text-align: right;\">29     </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">   24160</td><td style=\"text-align: right;\">211.338 </td><td>B5     </td><td>S         </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">   nan</td><td>St Louis  MO                   </td></tr>\n",
       "<tr><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">         1</td><td>Allison  Master. Hudson Trevor                 </td><td>male  </td><td style=\"text-align: right;\"> 0.9167</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">  113781</td><td style=\"text-align: right;\">151.55  </td><td>C22 C26</td><td>S         </td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">   nan</td><td>Montreal  PQ / Chesterville  ON</td></tr>\n",
       "<tr><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">         0</td><td>Allison  Miss. Helen Loraine                   </td><td>female</td><td style=\"text-align: right;\"> 2     </td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">  113781</td><td style=\"text-align: right;\">151.55  </td><td>C22 C26</td><td>S         </td><td style=\"text-align: right;\">   nan</td><td style=\"text-align: right;\">   nan</td><td>Montreal  PQ / Chesterville  ON</td></tr>\n",
       "<tr><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">         0</td><td>Allison  Mr. Hudson Joshua Creighton           </td><td>male  </td><td style=\"text-align: right;\">30     </td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">  113781</td><td style=\"text-align: right;\">151.55  </td><td>C22 C26</td><td>S         </td><td style=\"text-align: right;\">   nan</td><td style=\"text-align: right;\">   135</td><td>Montreal  PQ / Chesterville  ON</td></tr>\n",
       "<tr><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">         0</td><td>Allison  Mrs. Hudson J C (Bessie Waldo Daniels)</td><td>female</td><td style=\"text-align: right;\">25     </td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">  113781</td><td style=\"text-align: right;\">151.55  </td><td>C22 C26</td><td>S         </td><td style=\"text-align: right;\">   nan</td><td style=\"text-align: right;\">   nan</td><td>Montreal  PQ / Chesterville  ON</td></tr>\n",
       "<tr><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">         1</td><td>Anderson  Mr. Harry                            </td><td>male  </td><td style=\"text-align: right;\">48     </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">   19952</td><td style=\"text-align: right;\"> 26.55  </td><td>E12    </td><td>S         </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">   nan</td><td>New York  NY                   </td></tr>\n",
       "<tr><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">         1</td><td>Andrews  Miss. Kornelia Theodosia              </td><td>female</td><td style=\"text-align: right;\">63     </td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">   13502</td><td style=\"text-align: right;\"> 77.9583</td><td>D7     </td><td>S         </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">   nan</td><td>Hudson  NY                     </td></tr>\n",
       "<tr><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">         0</td><td>Andrews  Mr. Thomas Jr                         </td><td>male  </td><td style=\"text-align: right;\">39     </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">  112050</td><td style=\"text-align: right;\">  0     </td><td>A36    </td><td>S         </td><td style=\"text-align: right;\">   nan</td><td style=\"text-align: right;\">   nan</td><td>Belfast  NI                    </td></tr>\n",
       "<tr><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">         1</td><td>Appleton  Mrs. Edward Dale (Charlotte Lamson)  </td><td>female</td><td style=\"text-align: right;\">53     </td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">   11769</td><td style=\"text-align: right;\"> 51.4792</td><td>C101   </td><td>S         </td><td style=\"text-align: right;\">   nan</td><td style=\"text-align: right;\">   nan</td><td>Bayside  Queens  NY            </td></tr>\n",
       "<tr><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">         0</td><td>Artagaveytia  Mr. Ramon                        </td><td>male  </td><td style=\"text-align: right;\">71     </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\"> 49.5042</td><td>       </td><td>C         </td><td style=\"text-align: right;\">   nan</td><td style=\"text-align: right;\">    22</td><td>Montevideo  Uruguay            </td></tr>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method H2OFrame.tail of >\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th style=\"text-align: right;\">  pclass</th><th style=\"text-align: right;\">  survived</th><th>name                                           </th><th>sex   </th><th style=\"text-align: right;\">    age</th><th style=\"text-align: right;\">  sibsp</th><th style=\"text-align: right;\">  parch</th><th style=\"text-align: right;\">  ticket</th><th style=\"text-align: right;\">    fare</th><th>cabin  </th><th>embarked  </th><th style=\"text-align: right;\">  boat</th><th style=\"text-align: right;\">  body</th><th>home.dest                      </th></tr>\n",
       "<tr><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">         1</td><td>Allen  Miss. Elisabeth Walton                  </td><td>female</td><td style=\"text-align: right;\">29     </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">   24160</td><td style=\"text-align: right;\">211.338 </td><td>B5     </td><td>S         </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">   nan</td><td>St Louis  MO                   </td></tr>\n",
       "<tr><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">         1</td><td>Allison  Master. Hudson Trevor                 </td><td>male  </td><td style=\"text-align: right;\"> 0.9167</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">  113781</td><td style=\"text-align: right;\">151.55  </td><td>C22 C26</td><td>S         </td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">   nan</td><td>Montreal  PQ / Chesterville  ON</td></tr>\n",
       "<tr><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">         0</td><td>Allison  Miss. Helen Loraine                   </td><td>female</td><td style=\"text-align: right;\"> 2     </td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">  113781</td><td style=\"text-align: right;\">151.55  </td><td>C22 C26</td><td>S         </td><td style=\"text-align: right;\">   nan</td><td style=\"text-align: right;\">   nan</td><td>Montreal  PQ / Chesterville  ON</td></tr>\n",
       "<tr><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">         0</td><td>Allison  Mr. Hudson Joshua Creighton           </td><td>male  </td><td style=\"text-align: right;\">30     </td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">  113781</td><td style=\"text-align: right;\">151.55  </td><td>C22 C26</td><td>S         </td><td style=\"text-align: right;\">   nan</td><td style=\"text-align: right;\">   135</td><td>Montreal  PQ / Chesterville  ON</td></tr>\n",
       "<tr><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">         0</td><td>Allison  Mrs. Hudson J C (Bessie Waldo Daniels)</td><td>female</td><td style=\"text-align: right;\">25     </td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">  113781</td><td style=\"text-align: right;\">151.55  </td><td>C22 C26</td><td>S         </td><td style=\"text-align: right;\">   nan</td><td style=\"text-align: right;\">   nan</td><td>Montreal  PQ / Chesterville  ON</td></tr>\n",
       "<tr><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">         1</td><td>Anderson  Mr. Harry                            </td><td>male  </td><td style=\"text-align: right;\">48     </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">   19952</td><td style=\"text-align: right;\"> 26.55  </td><td>E12    </td><td>S         </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">   nan</td><td>New York  NY                   </td></tr>\n",
       "<tr><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">         1</td><td>Andrews  Miss. Kornelia Theodosia              </td><td>female</td><td style=\"text-align: right;\">63     </td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">   13502</td><td style=\"text-align: right;\"> 77.9583</td><td>D7     </td><td>S         </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">   nan</td><td>Hudson  NY                     </td></tr>\n",
       "<tr><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">         0</td><td>Andrews  Mr. Thomas Jr                         </td><td>male  </td><td style=\"text-align: right;\">39     </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">  112050</td><td style=\"text-align: right;\">  0     </td><td>A36    </td><td>S         </td><td style=\"text-align: right;\">   nan</td><td style=\"text-align: right;\">   nan</td><td>Belfast  NI                    </td></tr>\n",
       "<tr><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">         1</td><td>Appleton  Mrs. Edward Dale (Charlotte Lamson)  </td><td>female</td><td style=\"text-align: right;\">53     </td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">   11769</td><td style=\"text-align: right;\"> 51.4792</td><td>C101   </td><td>S         </td><td style=\"text-align: right;\">   nan</td><td style=\"text-align: right;\">   nan</td><td>Bayside  Queens  NY            </td></tr>\n",
       "<tr><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">         0</td><td>Artagaveytia  Mr. Ramon                        </td><td>male  </td><td style=\"text-align: right;\">71     </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\"> 49.5042</td><td>       </td><td>C         </td><td style=\"text-align: right;\">   nan</td><td style=\"text-align: right;\">    22</td><td>Montevideo  Uruguay            </td></tr>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method H2OFrame.describe of >\n",
      "[u'pclass', u'sex', u'age', u'sibsp', u'parch', u'ticket', u'fare', u'cabin', u'embarked', u'boat', u'body', u'home.dest']\n"
     ]
    }
   ],
   "source": [
    "## 'path' can point to a local file, hdfs, s3, nfs, Hive, directories, etc.\n",
    "df = h2o.import_file(path = \"http://s3.amazonaws.com/h2o-public-test-data/smalldata/gbm_test/titanic.csv\")\n",
    "print df.dim\n",
    "print df.head\n",
    "print df.tail\n",
    "print df.describe\n",
    "\n",
    "## pick a response for the supervised problem\n",
    "response = \"survived\"\n",
    "\n",
    "## the response variable is an integer, we will turn it into a categorical/factor for binary classification\n",
    "df[response] = df[response].asfactor()           \n",
    "\n",
    "## use all other columns (except for the name & the response column (\"survived\")) as predictors\n",
    "predictors = df.columns\n",
    "del predictors[1:3]\n",
    "print predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on, everything is generic and directly applies to most datasets. We assume that all feature engineering is done at this stage and focus on model tuning. For multi-class problems, you can use `h2o.logloss()` or `h2o.confusion_matrix()` instead of `h2o.auc()` and for regression problems, you can use `h2o.mean_residual_deviance()` or `h2o.mse()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data for Machine Learning\n",
    "We split the data into three pieces: 60% for training, 20% for validation, 20% for final testing. \n",
    "Here, we use random splitting, but this assumes i.i.d. data. If this is not the case (e.g., when events span across multiple rows or data has a time structure), you'll have to sample your data non-randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, valid, test = df.split_frame(\n",
    "    ratios=[0.6,0.2], \n",
    "    seed=1234, \n",
    "    destination_frames=['train.hex','valid.hex','test.hex']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish baseline performance\n",
    "As the first step, we'll build some default models to see what accuracy we can expect. Let's use the [AUC metric](http://mlwiki.org/index.php/ROC_Analysis) for this demo, but you can use `h2o.logloss()` and `stopping_metric=\"logloss\"` as well. It ranges from 0.5 for random models to 1 for perfect models.\n",
    "\n",
    "\n",
    "The first model is a default GBM, trained on the 60% training split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "gbm Model Build Progress: [##################################################] 100%\n",
      "Model Details\n",
      "=============\n",
      "H2OGradientBoostingEstimator :  Gradient Boosting Method\n",
      "Model Key:  GBM_model_python_1468807556132_1\n",
      "\n",
      "Model Summary: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>number_of_trees</b></td>\n",
       "<td><b>number_of_internal_trees</b></td>\n",
       "<td><b>model_size_in_bytes</b></td>\n",
       "<td><b>min_depth</b></td>\n",
       "<td><b>max_depth</b></td>\n",
       "<td><b>mean_depth</b></td>\n",
       "<td><b>min_leaves</b></td>\n",
       "<td><b>max_leaves</b></td>\n",
       "<td><b>mean_leaves</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>50.0</td>\n",
       "<td>50.0</td>\n",
       "<td>17181.0</td>\n",
       "<td>2.0</td>\n",
       "<td>5.0</td>\n",
       "<td>4.92</td>\n",
       "<td>3.0</td>\n",
       "<td>21.0</td>\n",
       "<td>9.14</td></tr></table></div>"
      ],
      "text/plain": [
       "    number_of_trees    number_of_internal_trees    model_size_in_bytes    min_depth    max_depth    mean_depth    min_leaves    max_leaves    mean_leaves\n",
       "--  -----------------  --------------------------  ---------------------  -----------  -----------  ------------  ------------  ------------  -------------\n",
       "    50                 50                          17181                  2            5            4.92          3             21            9.14"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomial: gbm\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.0406797359799\n",
      "R^2: 0.828341496541\n",
      "LogLoss: 0.168135896237\n",
      "Mean Per-Class Error: 0.0438517398512\n",
      "AUC: 0.988222972832\n",
      "Gini: 0.976445945665\n",
      "\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.40045091311: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>472.0</td>\n",
       "<td>7.0</td>\n",
       "<td>0.0146</td>\n",
       "<td> (7.0/479.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>22.0</td>\n",
       "<td>279.0</td>\n",
       "<td>0.0731</td>\n",
       "<td> (22.0/301.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>494.0</td>\n",
       "<td>286.0</td>\n",
       "<td>0.0372</td>\n",
       "<td> (29.0/780.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0    1    Error    Rate\n",
       "-----  ---  ---  -------  ------------\n",
       "0      472  7    0.0146   (7.0/479.0)\n",
       "1      22   279  0.0731   (22.0/301.0)\n",
       "Total  494  286  0.0372   (29.0/780.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.4004509</td>\n",
       "<td>0.9505963</td>\n",
       "<td>192.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1995825</td>\n",
       "<td>0.9455959</td>\n",
       "<td>205.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.4571007</td>\n",
       "<td>0.9687722</td>\n",
       "<td>189.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.4339709</td>\n",
       "<td>0.9628205</td>\n",
       "<td>191.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9799591</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0829058</td>\n",
       "<td>1.0</td>\n",
       "<td>251.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9799591</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_MCC</td>\n",
       "<td>0.4339709</td>\n",
       "<td>0.9217198</td>\n",
       "<td>191.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.2099932</td>\n",
       "<td>0.9394572</td>\n",
       "<td>203.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.4004509</td>\n",
       "<td>0.9561483</td>\n",
       "<td>192.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.400451     0.950596  192\n",
       "max f2                       0.199583     0.945596  205\n",
       "max f0point5                 0.457101     0.968772  189\n",
       "max accuracy                 0.433971     0.962821  191\n",
       "max precision                0.979959     1         0\n",
       "max recall                   0.0829058    1         251\n",
       "max specificity              0.979959     1         0\n",
       "max absolute_MCC             0.433971     0.92172   191\n",
       "max min_per_class_accuracy   0.209993     0.939457  203\n",
       "max mean_per_class_accuracy  0.400451     0.956148  192"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gains/Lift Table: Avg response rate: 38.59 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0102564</td>\n",
       "<td>0.9630355</td>\n",
       "<td>2.5913621</td>\n",
       "<td>2.5913621</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0265781</td>\n",
       "<td>0.0265781</td>\n",
       "<td>159.1362126</td>\n",
       "<td>159.1362126</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0217949</td>\n",
       "<td>0.9568772</td>\n",
       "<td>2.5913621</td>\n",
       "<td>2.5913621</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0299003</td>\n",
       "<td>0.0564784</td>\n",
       "<td>159.1362126</td>\n",
       "<td>159.1362126</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0307692</td>\n",
       "<td>0.9543191</td>\n",
       "<td>2.5913621</td>\n",
       "<td>2.5913621</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0232558</td>\n",
       "<td>0.0797342</td>\n",
       "<td>159.1362126</td>\n",
       "<td>159.1362126</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0410256</td>\n",
       "<td>0.9519030</td>\n",
       "<td>2.5913621</td>\n",
       "<td>2.5913621</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0265781</td>\n",
       "<td>0.1063123</td>\n",
       "<td>159.1362126</td>\n",
       "<td>159.1362126</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.05</td>\n",
       "<td>0.9511906</td>\n",
       "<td>2.5913621</td>\n",
       "<td>2.5913621</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0232558</td>\n",
       "<td>0.1295681</td>\n",
       "<td>159.1362126</td>\n",
       "<td>159.1362126</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1076923</td>\n",
       "<td>0.9477912</td>\n",
       "<td>2.5913621</td>\n",
       "<td>2.5913621</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.1495017</td>\n",
       "<td>0.2790698</td>\n",
       "<td>159.1362126</td>\n",
       "<td>159.1362126</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.15</td>\n",
       "<td>0.9460051</td>\n",
       "<td>2.5913621</td>\n",
       "<td>2.5913621</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.1096346</td>\n",
       "<td>0.3887043</td>\n",
       "<td>159.1362126</td>\n",
       "<td>159.1362126</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2</td>\n",
       "<td>0.9431959</td>\n",
       "<td>2.5913621</td>\n",
       "<td>2.5913621</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.1295681</td>\n",
       "<td>0.5182724</td>\n",
       "<td>159.1362126</td>\n",
       "<td>159.1362126</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3012821</td>\n",
       "<td>0.8373551</td>\n",
       "<td>2.5913621</td>\n",
       "<td>2.5913621</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.2624585</td>\n",
       "<td>0.7807309</td>\n",
       "<td>159.1362126</td>\n",
       "<td>159.1362126</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4</td>\n",
       "<td>0.2041091</td>\n",
       "<td>1.6153946</td>\n",
       "<td>2.3504983</td>\n",
       "<td>0.6233766</td>\n",
       "<td>0.9070513</td>\n",
       "<td>0.1594684</td>\n",
       "<td>0.9401993</td>\n",
       "<td>61.5394572</td>\n",
       "<td>135.0498339</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5012821</td>\n",
       "<td>0.1280484</td>\n",
       "<td>0.3936246</td>\n",
       "<td>1.9551198</td>\n",
       "<td>0.1518987</td>\n",
       "<td>0.7544757</td>\n",
       "<td>0.0398671</td>\n",
       "<td>0.9800664</td>\n",
       "<td>-60.6375373</td>\n",
       "<td>95.5119763</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6012821</td>\n",
       "<td>0.0844031</td>\n",
       "<td>0.0996678</td>\n",
       "<td>1.6465371</td>\n",
       "<td>0.0384615</td>\n",
       "<td>0.6353945</td>\n",
       "<td>0.0099668</td>\n",
       "<td>0.9900332</td>\n",
       "<td>-90.0332226</td>\n",
       "<td>64.6537129</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.7</td>\n",
       "<td>0.0779819</td>\n",
       "<td>0.1009622</td>\n",
       "<td>1.4285714</td>\n",
       "<td>0.0389610</td>\n",
       "<td>0.5512821</td>\n",
       "<td>0.0099668</td>\n",
       "<td>1.0</td>\n",
       "<td>-89.9037839</td>\n",
       "<td>42.8571429</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8</td>\n",
       "<td>0.0647129</td>\n",
       "<td>0.0</td>\n",
       "<td>1.25</td>\n",
       "<td>0.0</td>\n",
       "<td>0.4823718</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>25.0</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.9782051</td>\n",
       "<td>0.0520278</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0222805</td>\n",
       "<td>0.0</td>\n",
       "<td>0.3944954</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>2.2280472</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0490107</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.3858974</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  ---------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0102564                   0.963035           2.59136    2.59136            1                1                           0.0265781       0.0265781                  159.136   159.136\n",
       "    2        0.0217949                   0.956877           2.59136    2.59136            1                1                           0.0299003       0.0564784                  159.136   159.136\n",
       "    3        0.0307692                   0.954319           2.59136    2.59136            1                1                           0.0232558       0.0797342                  159.136   159.136\n",
       "    4        0.0410256                   0.951903           2.59136    2.59136            1                1                           0.0265781       0.106312                   159.136   159.136\n",
       "    5        0.05                        0.951191           2.59136    2.59136            1                1                           0.0232558       0.129568                   159.136   159.136\n",
       "    6        0.107692                    0.947791           2.59136    2.59136            1                1                           0.149502        0.27907                    159.136   159.136\n",
       "    7        0.15                        0.946005           2.59136    2.59136            1                1                           0.109635        0.388704                   159.136   159.136\n",
       "    8        0.2                         0.943196           2.59136    2.59136            1                1                           0.129568        0.518272                   159.136   159.136\n",
       "    9        0.301282                    0.837355           2.59136    2.59136            1                1                           0.262458        0.780731                   159.136   159.136\n",
       "    10       0.4                         0.204109           1.61539    2.3505             0.623377         0.907051                    0.159468        0.940199                   61.5395   135.05\n",
       "    11       0.501282                    0.128048           0.393625   1.95512            0.151899         0.754476                    0.0398671       0.980066                   -60.6375  95.512\n",
       "    12       0.601282                    0.0844031          0.0996678  1.64654            0.0384615        0.635394                    0.00996678      0.990033                   -90.0332  64.6537\n",
       "    13       0.7                         0.0779819          0.100962   1.42857            0.038961         0.551282                    0.00996678      1                          -89.9038  42.8571\n",
       "    14       0.8                         0.0647129          0          1.25               0                0.482372                    0               1                          -100      25\n",
       "    15       0.978205                    0.0520278          0          1.02228            0                0.394495                    0               1                          -100      2.22805\n",
       "    16       1                           0.0490107          0          1                  0                0.385897                    0               1                          -100      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Scoring History: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>timestamp</b></td>\n",
       "<td><b>duration</b></td>\n",
       "<td><b>number_of_trees</b></td>\n",
       "<td><b>training_MSE</b></td>\n",
       "<td><b>training_logloss</b></td>\n",
       "<td><b>training_AUC</b></td>\n",
       "<td><b>training_lift</b></td>\n",
       "<td><b>training_classification_error</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>2016-07-17 19:06:34</td>\n",
       "<td> 0.020 sec</td>\n",
       "<td>0.0</td>\n",
       "<td>0.2369806</td>\n",
       "<td>0.6668775</td>\n",
       "<td>0.5</td>\n",
       "<td>1.0</td>\n",
       "<td>0.6141026</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2016-07-17 19:06:34</td>\n",
       "<td> 0.133 sec</td>\n",
       "<td>1.0</td>\n",
       "<td>0.2064855</td>\n",
       "<td>0.6033605</td>\n",
       "<td>0.8851116</td>\n",
       "<td>2.5913621</td>\n",
       "<td>0.0897436</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2016-07-17 19:06:34</td>\n",
       "<td> 0.171 sec</td>\n",
       "<td>2.0</td>\n",
       "<td>0.1821387</td>\n",
       "<td>0.5530978</td>\n",
       "<td>0.8851428</td>\n",
       "<td>2.5913621</td>\n",
       "<td>0.0884615</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2016-07-17 19:06:34</td>\n",
       "<td> 0.202 sec</td>\n",
       "<td>3.0</td>\n",
       "<td>0.1625714</td>\n",
       "<td>0.5122612</td>\n",
       "<td>0.8851428</td>\n",
       "<td>2.5913621</td>\n",
       "<td>0.0884615</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2016-07-17 19:06:34</td>\n",
       "<td> 0.228 sec</td>\n",
       "<td>4.0</td>\n",
       "<td>0.1467807</td>\n",
       "<td>0.4785030</td>\n",
       "<td>0.8851428</td>\n",
       "<td>2.5913621</td>\n",
       "<td>0.0884615</td></tr>\n",
       "<tr><td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2016-07-17 19:06:35</td>\n",
       "<td> 1.030 sec</td>\n",
       "<td>46.0</td>\n",
       "<td>0.0406763</td>\n",
       "<td>0.1681376</td>\n",
       "<td>0.9882230</td>\n",
       "<td>2.5913621</td>\n",
       "<td>0.0371795</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2016-07-17 19:06:35</td>\n",
       "<td> 1.049 sec</td>\n",
       "<td>47.0</td>\n",
       "<td>0.0406773</td>\n",
       "<td>0.1681370</td>\n",
       "<td>0.9882230</td>\n",
       "<td>2.5913621</td>\n",
       "<td>0.0371795</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2016-07-17 19:06:35</td>\n",
       "<td> 1.068 sec</td>\n",
       "<td>48.0</td>\n",
       "<td>0.0406782</td>\n",
       "<td>0.1681366</td>\n",
       "<td>0.9882230</td>\n",
       "<td>2.5913621</td>\n",
       "<td>0.0371795</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2016-07-17 19:06:35</td>\n",
       "<td> 1.085 sec</td>\n",
       "<td>49.0</td>\n",
       "<td>0.0406790</td>\n",
       "<td>0.1681362</td>\n",
       "<td>0.9882230</td>\n",
       "<td>2.5913621</td>\n",
       "<td>0.0371795</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2016-07-17 19:06:35</td>\n",
       "<td> 1.103 sec</td>\n",
       "<td>50.0</td>\n",
       "<td>0.0406797</td>\n",
       "<td>0.1681359</td>\n",
       "<td>0.9882230</td>\n",
       "<td>2.5913621</td>\n",
       "<td>0.0371795</td></tr></table></div>"
      ],
      "text/plain": [
       "     timestamp            duration    number_of_trees    training_MSE     training_logloss    training_AUC    training_lift    training_classification_error\n",
       "---  -------------------  ----------  -----------------  ---------------  ------------------  --------------  ---------------  -------------------------------\n",
       "     2016-07-17 19:06:34  0.020 sec   0.0                0.236980604865   0.66687754039       0.5             1.0              0.614102564103\n",
       "     2016-07-17 19:06:34  0.133 sec   1.0                0.206485545277   0.603360501805      0.88511156271   2.59136212625    0.0897435897436\n",
       "     2016-07-17 19:06:34  0.171 sec   2.0                0.182138733238   0.553097780707      0.885142773913  2.59136212625    0.0884615384615\n",
       "     2016-07-17 19:06:34  0.202 sec   3.0                0.162571389022   0.512261184327      0.885142773913  2.59136212625    0.0884615384615\n",
       "     2016-07-17 19:06:34  0.228 sec   4.0                0.146780719947   0.478503019475      0.885142773913  2.59136212625    0.0884615384615\n",
       "---  ---                  ---         ---                ---              ---                 ---             ---              ---\n",
       "     2016-07-17 19:06:35  1.030 sec   46.0               0.0406763088862  0.168137585046      0.988222972832  2.59136212625    0.0371794871795\n",
       "     2016-07-17 19:06:35  1.049 sec   47.0               0.0406772863879  0.168137022774      0.988222972832  2.59136212625    0.0371794871795\n",
       "     2016-07-17 19:06:35  1.068 sec   48.0               0.0406781794184  0.168136566792      0.988222972832  2.59136212625    0.0371794871795\n",
       "     2016-07-17 19:06:35  1.085 sec   49.0               0.0406789941191  0.168136197404      0.988222972832  2.59136212625    0.0371794871795\n",
       "     2016-07-17 19:06:35  1.103 sec   50.0               0.0406797359799  0.168135896237      0.988222972832  2.59136212625    0.0371794871795"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "See the whole table with table.as_data_frame()\n",
      "\n",
      "Variable Importances: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>variable</b></td>\n",
       "<td><b>relative_importance</b></td>\n",
       "<td><b>scaled_importance</b></td>\n",
       "<td><b>percentage</b></td></tr>\n",
       "<tr><td>boat</td>\n",
       "<td>628.0942993</td>\n",
       "<td>1.0</td>\n",
       "<td>0.6422850</td></tr>\n",
       "<tr><td>home.dest</td>\n",
       "<td>147.9191895</td>\n",
       "<td>0.2355047</td>\n",
       "<td>0.1512612</td></tr>\n",
       "<tr><td>cabin</td>\n",
       "<td>129.4477234</td>\n",
       "<td>0.2060960</td>\n",
       "<td>0.1323724</td></tr>\n",
       "<tr><td>sex</td>\n",
       "<td>50.2480087</td>\n",
       "<td>0.0800007</td>\n",
       "<td>0.0513833</td></tr>\n",
       "<tr><td>ticket</td>\n",
       "<td>11.9593115</td>\n",
       "<td>0.0190406</td>\n",
       "<td>0.0122295</td></tr>\n",
       "<tr><td>age</td>\n",
       "<td>5.8832302</td>\n",
       "<td>0.0093668</td>\n",
       "<td>0.0060162</td></tr>\n",
       "<tr><td>body</td>\n",
       "<td>1.7419853</td>\n",
       "<td>0.0027734</td>\n",
       "<td>0.0017813</td></tr>\n",
       "<tr><td>embarked</td>\n",
       "<td>1.0638593</td>\n",
       "<td>0.0016938</td>\n",
       "<td>0.0010879</td></tr>\n",
       "<tr><td>parch</td>\n",
       "<td>0.8865532</td>\n",
       "<td>0.0014115</td>\n",
       "<td>0.0009066</td></tr>\n",
       "<tr><td>fare</td>\n",
       "<td>0.5306281</td>\n",
       "<td>0.0008448</td>\n",
       "<td>0.0005426</td></tr>\n",
       "<tr><td>sibsp</td>\n",
       "<td>0.1311035</td>\n",
       "<td>0.0002087</td>\n",
       "<td>0.0001341</td></tr>\n",
       "<tr><td>pclass</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "variable    relative_importance    scaled_importance    percentage\n",
       "----------  ---------------------  -------------------  ------------\n",
       "boat        628.094                1                    0.642285\n",
       "home.dest   147.919                0.235505             0.151261\n",
       "cabin       129.448                0.206096             0.132372\n",
       "sex         50.248                 0.0800007            0.0513833\n",
       "ticket      11.9593                0.0190406            0.0122295\n",
       "age         5.88323                0.00936679           0.00601615\n",
       "body        1.74199                0.00277345           0.00178134\n",
       "embarked    1.06386                0.00169379           0.0010879\n",
       "parch       0.886553               0.0014115            0.000906583\n",
       "fare        0.530628               0.000844822          0.000542617\n",
       "sibsp       0.131104               0.000208732          0.000134066\n",
       "pclass      0                      0                    0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#We only provide the required parameters, everything else is default\n",
    "gbm = h2o.H2OGradientBoostingEstimator()\n",
    "gbm.train(x=predictors, y=response, training_frame=train)\n",
    "\n",
    "## Show a detailed model summary\n",
    "print gbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.948013524937\n"
     ]
    }
   ],
   "source": [
    "## Get the AUC on the validation set\n",
    "perf = gbm.model_performance(valid)\n",
    "print perf.auc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC is over 94%, so this model is highly predictive!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second model is another default GBM, but trained on 80% of the data (here, we combine the training and validation splits to get more training data), and cross-validated using 4 folds.\n",
    "Note that cross-validation takes longer and is not usually done for really large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "gbm Model Build Progress: [##################################################] 100%\n"
     ]
    }
   ],
   "source": [
    "## rbind() makes a copy here, so it's better to use split_frame with `ratios = c(0.8)` instead above\n",
    "cv_gbm = h2o.H2OGradientBoostingEstimator(nfolds = 4, seed = 0xDECAF)\n",
    "cv_gbm.train(x = predictors, y = response, training_frame = train.rbind(valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the cross-validated performance is similar to the validation set performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.948595146871\n"
     ]
    }
   ],
   "source": [
    "## Show a detailed summary of the cross validation metrics\n",
    "## This gives you an idea of the variance between the folds\n",
    "cv_summary = cv_gbm.cross_validation_metrics_summary().as_data_frame()\n",
    "#print(cv_summary) ## Full summary of all metrics\n",
    "#print(cv_summary.iloc[4]) ## get the row with just the AUCs\n",
    "\n",
    "## Get the cross-validated AUC by scoring the combined holdout predictions.\n",
    "## (Instead of taking the average of the metrics across the folds)\n",
    "perf_cv = cv_gbm.model_performance(xval=True)\n",
    "print perf_cv.auc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train a GBM with \"I feel lucky\" parameters.\n",
    "We'll use early stopping to automatically tune the number of trees using the validation AUC. \n",
    "We'll use a lower learning rate (lower is always better, just takes more trees to converge).\n",
    "We'll also use stochastic sampling of rows and columns to (hopefully) improve generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "gbm Model Build Progress: [##################################################] 100%\n"
     ]
    }
   ],
   "source": [
    "gbm_lucky = h2o.H2OGradientBoostingEstimator(\n",
    "  ## more trees is better if the learning rate is small enough \n",
    "  ## here, use \"more than enough\" trees - we have early stopping\n",
    "  ntrees = 10000,                                                            \n",
    "\n",
    "  ## smaller learning rate is better (this is a good value for most datasets, but see below for annealing)\n",
    "  learn_rate = 0.01,                                                         \n",
    "\n",
    "  ## early stopping once the validation AUC doesn't improve by at least 0.01% for 5 consecutive scoring events\n",
    "  stopping_rounds = 5, stopping_tolerance = 1e-4, stopping_metric = \"AUC\", \n",
    "\n",
    "  ## sample 80% of rows per tree\n",
    "  sample_rate = 0.8,                                                       \n",
    "\n",
    "  ## sample 80% of columns per split\n",
    "  col_sample_rate = 0.8,                                                   \n",
    "\n",
    "  ## fix a random number generator seed for reproducibility\n",
    "  seed = 1234,                                                             \n",
    "\n",
    "  ## score every 10 trees to make early stopping reproducible (it depends on the scoring interval)\n",
    "  score_tree_interval = 10)\n",
    "\n",
    "gbm_lucky.train(x=predictors, y=response, training_frame=train, validation_frame=valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model doesn't seem to be better than the previous models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.943195266272\n"
     ]
    }
   ],
   "source": [
    "perf_lucky = gbm_lucky.model_performance(valid)\n",
    "print perf_lucky.auc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this small dataset, dropping 20% of observations per tree seems too aggressive in terms of adding regularization. For larger datasets, this is usually not a bad idea. But we'll let this parameter tune freshly below, so no worries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Hyper-Parameter Search\n",
    "\n",
    "Next, we'll do real hyper-parameter optimization to see if we can beat the best AUC so far (around 94%).\n",
    "\n",
    "The key here is to start tuning some key parameters first (i.e., those that we expect to have the biggest impact on the results). From experience with gradient boosted trees across many datasets, we can state the following \"rules\":\n",
    "\n",
    "1. Build as many trees (`ntrees`) as it takes until the validation set error starts increasing.\n",
    "2. A lower learning rate (`learn_rate`) is generally better, but will require more trees. Using `learn_rate=0.02 `and `learn_rate_annealing=0.995` (reduction of learning rate with each additional tree) can help speed up convergence without sacrificing accuracy too much, and is great to hyper-parameter searches. For faster scans, use values of 0.05 and 0.99 instead.\n",
    "3. The optimum maximum allowed depth for the trees (`max_depth`) is data dependent, deeper trees take longer to train, especially at depths greater than 10.\n",
    "4. Row and column sampling (`sample_rate` and `col_sample_rate`) can improve generalization and lead to lower validation and test set errors. Good general values for large datasets are around 0.7 to 0.8 (sampling 70-80 percent of the data) for both parameters. Column sampling per tree (`col_sample_rate_per_tree`) can also be tuned. Note that it is multiplicative with `col_sample_rate`, so setting both parameters to 0.8 results in 64% of columns being considered at any given node to split.\n",
    "5. For highly imbalanced classification datasets (e.g., fewer buyers than non-buyers), stratified row sampling based on response class membership can help improve predictive accuracy.  It is configured with `sample_rate_per_class` (array of ratios, one per response class in lexicographic order).\n",
    "6. Most other options only have a small impact on the model performance, but are worth tuning with a Random hyper-parameter search nonetheless, if highest performance is critical.\n",
    "\n",
    "First we want to know what value of `max_depth` to use because it has a big impact on the model training time and optimal values depend strongly on the dataset.\n",
    "We'll do a quick Cartesian grid search to get a rough idea of good candidate `max_depth` values. Each model in the grid search will use early stopping to tune the number of trees using the validation set AUC, as before.\n",
    "We'll use learning rate annealing to speed up convergence without sacrificing too much accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "gbm Grid Build Progress: [##################################################] 100%\n"
     ]
    }
   ],
   "source": [
    "## Depth 10 is usually plenty of depth for most datasets, but you never know\n",
    "hyper_params = {'max_depth' : range(1,30,2)}\n",
    "#hyper_params = {max_depth = [4,6,8,12,16,20]} ##faster for larger datasets\n",
    "\n",
    "#Build initial GBM Model\n",
    "gbm_grid = h2o.H2OGradientBoostingEstimator(\n",
    "        ## more trees is better if the learning rate is small enough \n",
    "        ## here, use \"more than enough\" trees - we have early stopping\n",
    "        ntrees=10000,\n",
    "        ## smaller learning rate is better\n",
    "        ## since we have learning_rate_annealing, we can afford to start with a \n",
    "        #bigger learning rate\n",
    "        learn_rate=0.05,\n",
    "        ## learning rate annealing: learning_rate shrinks by 1% after every tree \n",
    "        ## (use 1.00 to disable, but then lower the learning_rate)\n",
    "        learn_rate_annealing = 0.99,\n",
    "        ## sample 80% of rows per tree\n",
    "        sample_rate = 0.8,\n",
    "        ## sample 80% of columns per split\n",
    "        col_sample_rate = 0.8,\n",
    "        ## fix a random number generator seed for reproducibility\n",
    "        seed = 1234,\n",
    "        ## score every 10 trees to make early stopping reproducible \n",
    "        #(it depends on the scoring interval)\n",
    "        score_tree_interval = 10, \n",
    "        ## early stopping once the validation AUC doesn't improve by at least 0.01% for \n",
    "        #5 consecutive scoring events\n",
    "        stopping_rounds = 5,\n",
    "        stopping_metric = \"AUC\",\n",
    "        stopping_tolerance = 1e-4)\n",
    "\n",
    "#Build grid search with previously made GBM and hyper parameters\n",
    "grid = h2o.H2OGridSearch(gbm_grid,hyper_params,\n",
    "                         grid_id = 'depth_grid',\n",
    "                         search_criteria = {'strategy': \"Cartesian\"})\n",
    "\n",
    "\n",
    "#Train grid search\n",
    "grid.train(x=predictors, \n",
    "           y=response,\n",
    "           training_frame = train,\n",
    "           validation_frame = valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     max_depth            model_ids              logloss\n",
      "0           17   depth_grid_model_8  0.20544094075930078\n",
      "1           19   depth_grid_model_9  0.20584402503242194\n",
      "2           27  depth_grid_model_13  0.20627418156921704\n",
      "3           11   depth_grid_model_5   0.2069364255413584\n",
      "4           13   depth_grid_model_6   0.2078569528636169\n",
      "5           25  depth_grid_model_12  0.20834760530631993\n",
      "6            9   depth_grid_model_4  0.20842232867415922\n",
      "7           29  depth_grid_model_14  0.20904163538087436\n",
      "8           15   depth_grid_model_7  0.20991531457742935\n",
      "9           23  depth_grid_model_11   0.2104361858121492\n",
      "10          21  depth_grid_model_10  0.21069590143686837\n",
      "11           7   depth_grid_model_3  0.21127939637392396\n",
      "12           5   depth_grid_model_2  0.21509420086032935\n",
      "13           3   depth_grid_model_1  0.21854010261642962\n",
      "14           1   depth_grid_model_0  0.23892331983893703\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## by default, display the grid search results sorted by increasing logloss (since this is a classification task)\n",
    "print grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     max_depth            model_ids                 auc\n",
      "0           13   depth_grid_model_6  0.9552831783601015\n",
      "1           27  depth_grid_model_13  0.9547196393350239\n",
      "2           17   depth_grid_model_8  0.9543251620174698\n",
      "3           11   depth_grid_model_5  0.9538743307974078\n",
      "4            9   depth_grid_model_4  0.9534798534798535\n",
      "5           19   depth_grid_model_9  0.9534234995773457\n",
      "6           25  depth_grid_model_12  0.9529726683572838\n",
      "7           29  depth_grid_model_14  0.9528036066497605\n",
      "8           21  depth_grid_model_10  0.9526908988447449\n",
      "9           15   depth_grid_model_7  0.9526345449422373\n",
      "10           7   depth_grid_model_3   0.951789236404621\n",
      "11          23  depth_grid_model_11  0.9505494505494505\n",
      "12           3   depth_grid_model_1   0.949084249084249\n",
      "13           5   depth_grid_model_2  0.9484361792054099\n",
      "14           1   depth_grid_model_0  0.9478162862778248\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## sort the grid models by decreasing AUC\n",
    "sorted_grid = grid.get_grid(sort_by='auc',decreasing=True)\n",
    "print(sorted_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that `max_depth` values of 9 to 27 are best suited for this dataset, which is unusally deep!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxDepth 27\n",
      "MinDepth 9\n"
     ]
    }
   ],
   "source": [
    "max_depths = sorted_grid.sorted_metric_table()['max_depth'][0:5]\n",
    "new_max = int(max(max_depths, key=int))\n",
    "new_min = int(min(max_depths, key=int))\n",
    "\n",
    "print \"MaxDepth\", new_max\n",
    "print \"MinDepth\", new_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know a good range for max_depth, we can tune all other parameters in more detail. Since we don't know what combinations of hyper-parameters will result in the best model, we'll use random hyper-parameter search to \"let the machine get luckier than a best guess of any human\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create hyperameter and search criteria lists (ranges are inclusive..exclusive))\n",
    "hyper_params_tune = {'max_depth' : list(range(new_min,new_max+1,1)),\n",
    "                'sample_rate': [x/100. for x in range(20,101)],\n",
    "                'col_sample_rate' : [x/100. for x in range(20,101)],\n",
    "                'col_sample_rate_per_tree': [x/100. for x in range(20,101)],\n",
    "                'col_sample_rate_change_per_level': [x/100. for x in range(90,111)],\n",
    "                'min_rows': [2**x for x in range(0,int(math.log(train.nrow,2)-1)+1)],\n",
    "                'nbins': [2**x for x in range(4,11)],\n",
    "                'nbins_cats': [2**x for x in range(4,13)],\n",
    "                'min_split_improvement': [0,1e-8,1e-6,1e-4],\n",
    "                'histogram_type': [\"UniformAdaptive\",\"QuantilesGlobal\",\"RoundRobin\"]}\n",
    "search_criteria_tune = {'strategy': \"RandomDiscrete\",\n",
    "                   'max_runtime_secs': 3600,  ## limit the runtime to 60 minutes\n",
    "                   'max_models': 100,  ## build no more than 100 models\n",
    "                   'seed' : 1234,\n",
    "                   'stopping_rounds' : 5,\n",
    "                   'stopping_metric' : \"AUC\",\n",
    "                   'stopping_tolerance': 1e-3\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "gbm Grid Build Progress: [##################################################] 100%\n",
      "      col_sample_rate col_sample_rate_change_per_level  \\\n",
      "0                0.49                             1.04   \n",
      "1                0.35                             1.09   \n",
      "2                0.73                              0.9   \n",
      "3                0.97                             0.96   \n",
      "4                 0.5                             1.02   \n",
      "5                 0.6                              1.0   \n",
      "6                 0.5                             0.94   \n",
      "7                0.81                             0.94   \n",
      "8                0.63                              1.0   \n",
      "9                 0.4                             1.01   \n",
      "10               0.65                             1.08   \n",
      "11                0.2                             0.96   \n",
      "12               0.61                             1.04   \n",
      "13               0.45                             1.03   \n",
      "14               0.85                             1.07   \n",
      "15               0.62                             0.96   \n",
      "16                0.7                             1.02   \n",
      "17               0.55                             1.05   \n",
      "18               0.96                             1.04   \n",
      "19               0.76                             0.97   \n",
      "20                0.9                              1.1   \n",
      "21                0.9                             1.01   \n",
      "22               0.92                             0.93   \n",
      "23               0.47                             0.98   \n",
      "24               0.42                             1.08   \n",
      "25               0.75                             0.99   \n",
      "26               0.42                             0.98   \n",
      "27               0.92                             1.06   \n",
      "28               0.96                              1.1   \n",
      "29               0.92                             1.04   \n",
      ".. ..             ...                              ...   \n",
      "70               0.88                             1.05   \n",
      "71                0.2                             0.94   \n",
      "72               0.48                              1.0   \n",
      "73               0.52                              1.0   \n",
      "74               0.25                             1.04   \n",
      "75               0.74                             1.07   \n",
      "76               0.99                              1.0   \n",
      "77               0.44                             1.03   \n",
      "78               0.33                             1.06   \n",
      "79               0.45                             1.06   \n",
      "80               0.25                             1.06   \n",
      "81               0.37                             0.94   \n",
      "82               0.67                             1.04   \n",
      "83               0.46                             0.94   \n",
      "84               0.31                             0.94   \n",
      "85               0.66                             1.01   \n",
      "86               0.22                             1.06   \n",
      "87               0.57                              1.0   \n",
      "88               0.22                             1.09   \n",
      "89               0.76                             0.94   \n",
      "90               0.27                             1.07   \n",
      "91                0.2                             0.94   \n",
      "92               0.57                              1.1   \n",
      "93               0.92                             1.06   \n",
      "94                0.7                             1.08   \n",
      "95               0.96                             0.94   \n",
      "96               0.61                             0.97   \n",
      "97                0.5                             1.03   \n",
      "98               0.87                              1.0   \n",
      "99               0.24                             1.08   \n",
      "\n",
      "   col_sample_rate_per_tree   histogram_type max_depth min_rows  \\\n",
      "0                      0.94  QuantilesGlobal        27      2.0   \n",
      "1                      0.83  QuantilesGlobal        14      4.0   \n",
      "2                       0.6  QuantilesGlobal        12      1.0   \n",
      "3                      0.96  QuantilesGlobal         9      1.0   \n",
      "4                      0.65       RoundRobin        13      2.0   \n",
      "5                      0.89  UniformAdaptive        20      1.0   \n",
      "6                      0.92       RoundRobin         9      2.0   \n",
      "7                      0.89  QuantilesGlobal         9     16.0   \n",
      "8                      0.85       RoundRobin        12      2.0   \n",
      "9                      0.55  QuantilesGlobal        19      2.0   \n",
      "10                     0.95  UniformAdaptive        11      4.0   \n",
      "11                     0.96       RoundRobin        12      8.0   \n",
      "12                     0.61  UniformAdaptive        23      1.0   \n",
      "13                     0.79       RoundRobin        11     32.0   \n",
      "14                     0.95  UniformAdaptive        17      8.0   \n",
      "15                     0.68       RoundRobin        19     64.0   \n",
      "16                     0.56  UniformAdaptive        25      2.0   \n",
      "17                      1.0  UniformAdaptive        13      8.0   \n",
      "18                     0.99  UniformAdaptive        22      1.0   \n",
      "19                     0.91  UniformAdaptive        24      8.0   \n",
      "20                     0.59       RoundRobin        26     32.0   \n",
      "21                     0.71  QuantilesGlobal        25      4.0   \n",
      "22                     0.56  QuantilesGlobal        13      4.0   \n",
      "23                     0.68  UniformAdaptive        23      8.0   \n",
      "24                     0.45       RoundRobin        13      1.0   \n",
      "25                      0.8  UniformAdaptive        14     32.0   \n",
      "26                     0.53  UniformAdaptive        11      1.0   \n",
      "27                     0.73  UniformAdaptive         9      8.0   \n",
      "28                     0.43       RoundRobin        27      2.0   \n",
      "29                      1.0  QuantilesGlobal        26     64.0   \n",
      "..                      ...              ...       ...      ...   \n",
      "70                     0.31  UniformAdaptive        22    128.0   \n",
      "71                     0.45  UniformAdaptive        19      1.0   \n",
      "72                     0.35  QuantilesGlobal        23     64.0   \n",
      "73                     0.42  UniformAdaptive        26     64.0   \n",
      "74                     0.87  QuantilesGlobal        10    128.0   \n",
      "75                      0.4       RoundRobin         9    128.0   \n",
      "76                     0.22       RoundRobin        14     64.0   \n",
      "77                     0.38  QuantilesGlobal        22    128.0   \n",
      "78                     0.69       RoundRobin        21    128.0   \n",
      "79                     0.28  UniformAdaptive         9     32.0   \n",
      "80                     0.22  QuantilesGlobal        20      8.0   \n",
      "81                     0.23  QuantilesGlobal        16      2.0   \n",
      "82                     0.21  UniformAdaptive        11     16.0   \n",
      "83                     0.24  UniformAdaptive        24      1.0   \n",
      "84                      0.3  UniformAdaptive        15     64.0   \n",
      "85                     0.22       RoundRobin        13      4.0   \n",
      "86                     0.55       RoundRobin        20     64.0   \n",
      "87                     0.23       RoundRobin        26     64.0   \n",
      "88                     0.22  UniformAdaptive        20    128.0   \n",
      "89                      0.2       RoundRobin         9    128.0   \n",
      "90                     0.39       RoundRobin        10    128.0   \n",
      "91                     0.49  QuantilesGlobal        27    128.0   \n",
      "92                     0.68       RoundRobin        11    256.0   \n",
      "93                     0.64       RoundRobin        13    256.0   \n",
      "94                     0.99  QuantilesGlobal        26    256.0   \n",
      "95                     0.62  QuantilesGlobal        11    256.0   \n",
      "96                     0.36  QuantilesGlobal        24    256.0   \n",
      "97                     0.45       RoundRobin        25    256.0   \n",
      "98                      0.2       RoundRobin        17    256.0   \n",
      "99                      0.3  UniformAdaptive        15    256.0   \n",
      "\n",
      "   min_split_improvement nbins nbins_cats sample_rate            model_ids  \\\n",
      "0                    0.0    32        256        0.86  final_grid_model_68   \n",
      "1                 1.0E-8    64        128        0.69  final_grid_model_38   \n",
      "2                 1.0E-4  1024        256        0.29  final_grid_model_45   \n",
      "3                 1.0E-4  1024         64        0.32  final_grid_model_75   \n",
      "4                 1.0E-8   512        512        0.64   final_grid_model_0   \n",
      "5                    0.0    16        128        0.64   final_grid_model_6   \n",
      "6                    0.0   128       2048        0.61  final_grid_model_14   \n",
      "7                 1.0E-8  1024         32        0.71  final_grid_model_69   \n",
      "8                    0.0   128       4096        0.37   final_grid_model_5   \n",
      "9                    0.0    64         32        0.42  final_grid_model_88   \n",
      "10                1.0E-8    64        512         0.4  final_grid_model_33   \n",
      "11                1.0E-8    64        256        0.93  final_grid_model_87   \n",
      "12                1.0E-4    64         16        0.69  final_grid_model_81   \n",
      "13                1.0E-4    64         16        0.55  final_grid_model_54   \n",
      "14                1.0E-6    16        512        0.55  final_grid_model_97   \n",
      "15                   0.0   256        128        0.96  final_grid_model_76   \n",
      "16                1.0E-8    32        256        0.34  final_grid_model_22   \n",
      "17                1.0E-4  1024       2048        0.86  final_grid_model_17   \n",
      "18                1.0E-6  1024       2048        0.29  final_grid_model_39   \n",
      "19                1.0E-4  1024       2048        0.43  final_grid_model_79   \n",
      "20                1.0E-8   128        256        0.62  final_grid_model_91   \n",
      "21                   0.0    32       4096        0.46  final_grid_model_46   \n",
      "22                   0.0   128        128        0.93  final_grid_model_96   \n",
      "23                1.0E-8   128        128        0.87   final_grid_model_2   \n",
      "24                1.0E-6    16        256        0.35  final_grid_model_16   \n",
      "25                1.0E-6  1024        512         0.5   final_grid_model_7   \n",
      "26                1.0E-4    64         16        0.69  final_grid_model_55   \n",
      "27                1.0E-8    32        128        0.28  final_grid_model_60   \n",
      "28                1.0E-8   512       2048        0.48  final_grid_model_64   \n",
      "29                   0.0   256         16        0.57  final_grid_model_48   \n",
      "..                   ...   ...        ...         ...                  ...   \n",
      "70                1.0E-6  1024       4096        0.37  final_grid_model_31   \n",
      "71                1.0E-6    16       4096        0.39  final_grid_model_71   \n",
      "72                1.0E-4    32        512         0.4  final_grid_model_56   \n",
      "73                   0.0  1024         16        0.67  final_grid_model_59   \n",
      "74                1.0E-8   256       4096        0.95  final_grid_model_72   \n",
      "75                1.0E-4    16        128         0.3  final_grid_model_89   \n",
      "76                   0.0   256        128         0.5  final_grid_model_57   \n",
      "77                1.0E-6   256        512        0.55   final_grid_model_9   \n",
      "78                1.0E-4    64        256        0.35  final_grid_model_18   \n",
      "79                1.0E-8   256         64        0.28  final_grid_model_92   \n",
      "80                   0.0   512       2048        0.23  final_grid_model_36   \n",
      "81                1.0E-6   512       4096        0.47  final_grid_model_63   \n",
      "82                   0.0    32         64         0.6  final_grid_model_10   \n",
      "83                   0.0   128        512        0.65  final_grid_model_42   \n",
      "84                1.0E-8   128        128         0.4  final_grid_model_47   \n",
      "85                1.0E-6    16       4096        0.59  final_grid_model_19   \n",
      "86                1.0E-4    32         16        0.28  final_grid_model_73   \n",
      "87                1.0E-6    64         64        0.25  final_grid_model_35   \n",
      "88                1.0E-8  1024       2048        0.59  final_grid_model_61   \n",
      "89                   0.0   256       1024        0.62  final_grid_model_67   \n",
      "90                1.0E-6    16         64        0.57   final_grid_model_1   \n",
      "91                   0.0    16         32         0.4  final_grid_model_28   \n",
      "92                   0.0    16       4096        0.58   final_grid_model_8   \n",
      "93                   0.0    16         32        0.78  final_grid_model_70   \n",
      "94                1.0E-4    32         16        0.49  final_grid_model_86   \n",
      "95                1.0E-6    64       4096        0.57  final_grid_model_95   \n",
      "96                1.0E-6   128       1024        0.65  final_grid_model_98   \n",
      "97                1.0E-8   512         16        0.28  final_grid_model_58   \n",
      "98                1.0E-6   512       1024        0.97  final_grid_model_51   \n",
      "99                1.0E-4    32         64        0.97  final_grid_model_44   \n",
      "\n",
      "                logloss  \n",
      "0    0.1724195177377109  \n",
      "1    0.1785402233722598  \n",
      "2   0.18361800905283707  \n",
      "3    0.1871222263287576  \n",
      "4   0.18959285673409573  \n",
      "5   0.19152584981364526  \n",
      "6   0.19274618621918435  \n",
      "7    0.1947930984585484  \n",
      "8     0.196485575764478  \n",
      "9    0.1968403959647829  \n",
      "10   0.1999688912894301  \n",
      "11  0.20201657250742558  \n",
      "12  0.20266301087084315  \n",
      "13  0.20516449122756517  \n",
      "14   0.2067531116433615  \n",
      "15   0.2087686550469798  \n",
      "16  0.20877820607021177  \n",
      "17  0.20900838036852132  \n",
      "18  0.21087515188016223  \n",
      "19   0.2115041324592003  \n",
      "20  0.21208885921765247  \n",
      "21  0.21216588823615523  \n",
      "22  0.21265074880389523  \n",
      "23  0.21299994256302826  \n",
      "24  0.21362387535587823  \n",
      "25   0.2149390445444144  \n",
      "26  0.21510488500476962  \n",
      "27  0.21564848516181673  \n",
      "28  0.21625308206132732  \n",
      "29  0.21802015810369804  \n",
      "..                  ...  \n",
      "70   0.2774771827112313  \n",
      "71   0.2819876568815201  \n",
      "72    0.286371071676637  \n",
      "73  0.29041035952972577  \n",
      "74  0.29823678827120337  \n",
      "75   0.3016387593981173  \n",
      "76   0.3118405801095551  \n",
      "77  0.32686853593555837  \n",
      "78  0.32716497253228505  \n",
      "79   0.3285607327288318  \n",
      "80  0.33296974353472003  \n",
      "81   0.3350584463863175  \n",
      "82   0.3377299690705004  \n",
      "83  0.34081574296481054  \n",
      "84   0.3556018858084106  \n",
      "85  0.38840541246389937  \n",
      "86   0.4125497858974529  \n",
      "87  0.41411961317447754  \n",
      "88  0.42004941940980145  \n",
      "89   0.4205701620399925  \n",
      "90   0.4416592685663139  \n",
      "91   0.4534058856469468  \n",
      "92   0.5093556450277551  \n",
      "93   0.5118825801170095  \n",
      "94   0.5142960257124096  \n",
      "95   0.5245975877044032  \n",
      "96   0.5403552362246625  \n",
      "97   0.5440442492091082  \n",
      "98   0.5487016151393015  \n",
      "99   0.5827120934746949  \n",
      "\n",
      "[100 rows x 13 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gbm_final_grid = h2o.H2OGradientBoostingEstimator(distribution='bernoulli',\n",
    "                    ## more trees is better if the learning rate is small enough \n",
    "                    ## here, use \"more than enough\" trees - we have early stopping\n",
    "                    ntrees=10000,\n",
    "                    ## smaller learning rate is better\n",
    "                    ## since we have learning_rate_annealing, we can afford to start with a \n",
    "                    #bigger learning rate\n",
    "                    learn_rate=0.05,\n",
    "                    ## learning rate annealing: learning_rate shrinks by 1% after every tree \n",
    "                    ## (use 1.00 to disable, but then lower the learning_rate)\n",
    "                    learn_rate_annealing = 0.99,\n",
    "                    ## score every 10 trees to make early stopping reproducible \n",
    "                    #(it depends on the scoring interval)\n",
    "                    score_tree_interval = 10,\n",
    "                    ## fix a random number generator seed for reproducibility\n",
    "                    seed = 1234,\n",
    "                    ## early stopping once the validation AUC doesn't improve by at least 0.01% for \n",
    "                    #5 consecutive scoring events\n",
    "                    stopping_rounds = 5,\n",
    "                    stopping_metric = \"AUC\",\n",
    "                    stopping_tolerance = 1e-4)\n",
    "            \n",
    "#Build grid search with previously made GBM and hyper parameters\n",
    "final_grid = h2o.H2OGridSearch(gbm_final_grid, hyper_params = hyper_params_tune,\n",
    "                                    grid_id = 'final_grid',\n",
    "                                    search_criteria = search_criteria_tune)\n",
    "#Train grid search\n",
    "final_grid.train(x=predictors, \n",
    "           y=response,\n",
    "           ## early stopping based on timeout (no model should take more than 1 hour - modify as needed)\n",
    "           max_runtime_secs = 3600, \n",
    "           training_frame = train,\n",
    "           validation_frame = valid)\n",
    "\n",
    "print final_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the best models have even better validation AUCs than our previous best models, so the random grid search was successful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      col_sample_rate col_sample_rate_change_per_level  \\\n",
      "0                0.73                              0.9   \n",
      "1                0.49                             1.04   \n",
      "2                0.92                             0.93   \n",
      "3                 0.5                             1.02   \n",
      "4                0.35                             1.09   \n",
      "5                 0.7                             1.02   \n",
      "6                0.81                             0.94   \n",
      "7                0.61                             1.04   \n",
      "8                 0.4                             1.01   \n",
      "9                0.97                             0.96   \n",
      "10               0.91                             0.96   \n",
      "11                0.5                             0.94   \n",
      "12               0.51                              1.1   \n",
      "13               0.66                             0.96   \n",
      "14               0.96                              1.1   \n",
      "15               0.71                              0.9   \n",
      "16               0.66                             1.03   \n",
      "17                0.2                             0.96   \n",
      "18               0.65                             1.08   \n",
      "19               0.72                             1.08   \n",
      "20               0.47                             0.98   \n",
      "21               0.75                             0.94   \n",
      "22               0.42                             0.98   \n",
      "23               0.45                             1.03   \n",
      "24               0.63                              1.0   \n",
      "25                0.6                              1.0   \n",
      "26               0.92                             1.06   \n",
      "27               0.81                             1.06   \n",
      "28               0.29                             1.02   \n",
      "29               0.52                              1.1   \n",
      ".. ..             ...                              ...   \n",
      "70               0.25                             1.06   \n",
      "71               0.54                             1.07   \n",
      "72               0.91                             1.02   \n",
      "73                0.8                              0.9   \n",
      "74               0.31                             0.94   \n",
      "75               0.33                             1.06   \n",
      "76               0.88                             1.05   \n",
      "77               0.32                             1.02   \n",
      "78                0.5                             0.99   \n",
      "79               0.25                             1.04   \n",
      "80               0.44                             1.03   \n",
      "81               0.66                             0.99   \n",
      "82               0.69                             0.97   \n",
      "83               0.37                             0.94   \n",
      "84               0.46                             0.94   \n",
      "85               0.66                             1.01   \n",
      "86               0.57                              1.0   \n",
      "87               0.22                             1.06   \n",
      "88               0.76                             0.94   \n",
      "89               0.22                             1.09   \n",
      "90                0.2                             0.94   \n",
      "91               0.27                             1.07   \n",
      "92               0.61                             0.97   \n",
      "93               0.57                              1.1   \n",
      "94               0.92                             1.06   \n",
      "95                0.7                             1.08   \n",
      "96                0.5                             1.03   \n",
      "97               0.87                              1.0   \n",
      "98               0.24                             1.08   \n",
      "99               0.96                             0.94   \n",
      "\n",
      "   col_sample_rate_per_tree   histogram_type max_depth min_rows  \\\n",
      "0                       0.6  QuantilesGlobal        12      1.0   \n",
      "1                      0.94  QuantilesGlobal        27      2.0   \n",
      "2                      0.56  QuantilesGlobal        13      4.0   \n",
      "3                      0.65       RoundRobin        13      2.0   \n",
      "4                      0.83  QuantilesGlobal        14      4.0   \n",
      "5                      0.56  UniformAdaptive        25      2.0   \n",
      "6                      0.89  QuantilesGlobal         9     16.0   \n",
      "7                      0.61  UniformAdaptive        23      1.0   \n",
      "8                      0.55  QuantilesGlobal        19      2.0   \n",
      "9                      0.96  QuantilesGlobal         9      1.0   \n",
      "10                      0.4       RoundRobin        15      4.0   \n",
      "11                     0.92       RoundRobin         9      2.0   \n",
      "12                      0.5  UniformAdaptive        14      4.0   \n",
      "13                     0.38  UniformAdaptive        23      8.0   \n",
      "14                     0.43       RoundRobin        27      2.0   \n",
      "15                     0.37  UniformAdaptive        14     16.0   \n",
      "16                     0.42  QuantilesGlobal        12     16.0   \n",
      "17                     0.96       RoundRobin        12      8.0   \n",
      "18                     0.95  UniformAdaptive        11      4.0   \n",
      "19                     0.32  UniformAdaptive        23      4.0   \n",
      "20                     0.68  UniformAdaptive        23      8.0   \n",
      "21                      0.6  QuantilesGlobal        12      1.0   \n",
      "22                     0.53  UniformAdaptive        11      1.0   \n",
      "23                     0.79       RoundRobin        11     32.0   \n",
      "24                     0.85       RoundRobin        12      2.0   \n",
      "25                     0.89  UniformAdaptive        20      1.0   \n",
      "26                     0.73  UniformAdaptive         9      8.0   \n",
      "27                     0.68  QuantilesGlobal         9      1.0   \n",
      "28                     0.85  UniformAdaptive        21      1.0   \n",
      "29                     0.33  QuantilesGlobal        25     32.0   \n",
      "..                      ...              ...       ...      ...   \n",
      "70                     0.22  QuantilesGlobal        20      8.0   \n",
      "71                     0.58  UniformAdaptive        23    128.0   \n",
      "72                     0.97  UniformAdaptive        18      2.0   \n",
      "73                     0.57  UniformAdaptive        18    128.0   \n",
      "74                      0.3  UniformAdaptive        15     64.0   \n",
      "75                     0.69       RoundRobin        21    128.0   \n",
      "76                     0.31  UniformAdaptive        22    128.0   \n",
      "77                     0.88  UniformAdaptive        21     64.0   \n",
      "78                     0.81  QuantilesGlobal        13    128.0   \n",
      "79                     0.87  QuantilesGlobal        10    128.0   \n",
      "80                     0.38  QuantilesGlobal        22    128.0   \n",
      "81                     0.68  UniformAdaptive        14     64.0   \n",
      "82                     0.98  UniformAdaptive        26    128.0   \n",
      "83                     0.23  QuantilesGlobal        16      2.0   \n",
      "84                     0.24  UniformAdaptive        24      1.0   \n",
      "85                     0.22       RoundRobin        13      4.0   \n",
      "86                     0.23       RoundRobin        26     64.0   \n",
      "87                     0.55       RoundRobin        20     64.0   \n",
      "88                      0.2       RoundRobin         9    128.0   \n",
      "89                     0.22  UniformAdaptive        20    128.0   \n",
      "90                     0.49  QuantilesGlobal        27    128.0   \n",
      "91                     0.39       RoundRobin        10    128.0   \n",
      "92                     0.36  QuantilesGlobal        24    256.0   \n",
      "93                     0.68       RoundRobin        11    256.0   \n",
      "94                     0.64       RoundRobin        13    256.0   \n",
      "95                     0.99  QuantilesGlobal        26    256.0   \n",
      "96                     0.45       RoundRobin        25    256.0   \n",
      "97                      0.2       RoundRobin        17    256.0   \n",
      "98                      0.3  UniformAdaptive        15    256.0   \n",
      "99                     0.62  QuantilesGlobal        11    256.0   \n",
      "\n",
      "   min_split_improvement nbins nbins_cats sample_rate            model_ids  \\\n",
      "0                 1.0E-4  1024        256        0.29  final_grid_model_45   \n",
      "1                    0.0    32        256        0.86  final_grid_model_68   \n",
      "2                    0.0   128        128        0.93  final_grid_model_96   \n",
      "3                 1.0E-8   512        512        0.64   final_grid_model_0   \n",
      "4                 1.0E-8    64        128        0.69  final_grid_model_38   \n",
      "5                 1.0E-8    32        256        0.34  final_grid_model_22   \n",
      "6                 1.0E-8  1024         32        0.71  final_grid_model_69   \n",
      "7                 1.0E-4    64         16        0.69  final_grid_model_81   \n",
      "8                    0.0    64         32        0.42  final_grid_model_88   \n",
      "9                 1.0E-4  1024         64        0.32  final_grid_model_75   \n",
      "10                1.0E-4    32       1024        0.67  final_grid_model_41   \n",
      "11                   0.0   128       2048        0.61  final_grid_model_14   \n",
      "12                   0.0   128       1024        0.46  final_grid_model_25   \n",
      "13                1.0E-8   256         32        0.92  final_grid_model_50   \n",
      "14                1.0E-8   512       2048        0.48  final_grid_model_64   \n",
      "15                1.0E-4  1024         32        0.92  final_grid_model_37   \n",
      "16                1.0E-4  1024        512        0.59  final_grid_model_21   \n",
      "17                1.0E-8    64        256        0.93  final_grid_model_87   \n",
      "18                1.0E-8    64        512         0.4  final_grid_model_33   \n",
      "19                   0.0   512         32        0.54  final_grid_model_77   \n",
      "20                1.0E-8   128        128        0.87   final_grid_model_2   \n",
      "21                1.0E-4    16        512         0.5  final_grid_model_11   \n",
      "22                1.0E-4    64         16        0.69  final_grid_model_55   \n",
      "23                1.0E-4    64         16        0.55  final_grid_model_54   \n",
      "24                   0.0   128       4096        0.37   final_grid_model_5   \n",
      "25                   0.0    16        128        0.64   final_grid_model_6   \n",
      "26                1.0E-8    32        128        0.28  final_grid_model_60   \n",
      "27                   0.0   128       2048        0.77  final_grid_model_29   \n",
      "28                1.0E-6    64       4096        0.75  final_grid_model_40   \n",
      "29                   0.0    64         64        0.65  final_grid_model_20   \n",
      "..                   ...   ...        ...         ...                  ...   \n",
      "70                   0.0   512       2048        0.23  final_grid_model_36   \n",
      "71                1.0E-4   128       2048        0.72  final_grid_model_85   \n",
      "72                1.0E-6    64       2048        0.26   final_grid_model_4   \n",
      "73                1.0E-6  1024       4096        0.37  final_grid_model_43   \n",
      "74                1.0E-8   128        128         0.4  final_grid_model_47   \n",
      "75                1.0E-4    64        256        0.35  final_grid_model_18   \n",
      "76                1.0E-6  1024       4096        0.37  final_grid_model_31   \n",
      "77                1.0E-8   512       1024        0.46  final_grid_model_13   \n",
      "78                1.0E-6   256       4096        0.76   final_grid_model_3   \n",
      "79                1.0E-8   256       4096        0.95  final_grid_model_72   \n",
      "80                1.0E-6   256        512        0.55   final_grid_model_9   \n",
      "81                1.0E-4  1024       1024        0.23  final_grid_model_27   \n",
      "82                   0.0   256       4096        0.82  final_grid_model_93   \n",
      "83                1.0E-6   512       4096        0.47  final_grid_model_63   \n",
      "84                   0.0   128        512        0.65  final_grid_model_42   \n",
      "85                1.0E-6    16       4096        0.59  final_grid_model_19   \n",
      "86                1.0E-6    64         64        0.25  final_grid_model_35   \n",
      "87                1.0E-4    32         16        0.28  final_grid_model_73   \n",
      "88                   0.0   256       1024        0.62  final_grid_model_67   \n",
      "89                1.0E-8  1024       2048        0.59  final_grid_model_61   \n",
      "90                   0.0    16         32         0.4  final_grid_model_28   \n",
      "91                1.0E-6    16         64        0.57   final_grid_model_1   \n",
      "92                1.0E-6   128       1024        0.65  final_grid_model_98   \n",
      "93                   0.0    16       4096        0.58   final_grid_model_8   \n",
      "94                   0.0    16         32        0.78  final_grid_model_70   \n",
      "95                1.0E-4    32         16        0.49  final_grid_model_86   \n",
      "96                1.0E-8   512         16        0.28  final_grid_model_58   \n",
      "97                1.0E-6   512       1024        0.97  final_grid_model_51   \n",
      "98                1.0E-4    32         64        0.97  final_grid_model_44   \n",
      "99                1.0E-6    64       4096        0.57  final_grid_model_95   \n",
      "\n",
      "                   auc  \n",
      "0   0.9723584108199492  \n",
      "1   0.9714003944773175  \n",
      "2   0.9711186249647789  \n",
      "3   0.9710059171597633  \n",
      "4   0.9707805015497324  \n",
      "5   0.9699351930121162  \n",
      "6   0.9684136376444069  \n",
      "7   0.9681318681318682  \n",
      "8   0.9680191603268526  \n",
      "9   0.9653141730064807  \n",
      "10  0.9649760495914341  \n",
      "11  0.9644125105663568  \n",
      "12  0.9642998027613412  \n",
      "13  0.9638489715412792  \n",
      "14  0.9636235559312482  \n",
      "15  0.9636235559312482  \n",
      "16  0.9635672020287406  \n",
      "17  0.9628909551986475  \n",
      "18  0.9627218934911242  \n",
      "19  0.9627218934911242  \n",
      "20  0.9626655395886166  \n",
      "21  0.9625528317836011  \n",
      "22  0.9624964778810933  \n",
      "23  0.9623837700760778  \n",
      "24  0.9623837700760778  \n",
      "25  0.9622710622710622  \n",
      "26  0.9621020005635391  \n",
      "27  0.9617638771484927  \n",
      "28  0.9617075232459849  \n",
      "29  0.9607495069033531  \n",
      "..                 ...  \n",
      "70  0.9487461256692026  \n",
      "71  0.9483516483516483  \n",
      "72  0.9482389405466328  \n",
      "73  0.9481544096928712  \n",
      "74  0.9469709777402086  \n",
      "75  0.9465201465201465  \n",
      "76  0.9464919695688926  \n",
      "77  0.9461538461538462  \n",
      "78  0.9454212454212454  \n",
      "79  0.9451676528599605  \n",
      "80  0.9443223443223443  \n",
      "81  0.9435333896872359  \n",
      "82    0.94305438151592  \n",
      "83  0.9426599041983657  \n",
      "84   0.937644406875176  \n",
      "85  0.9364609749225133  \n",
      "86  0.9340095801634263  \n",
      "87  0.9326852634544942  \n",
      "88  0.9298957452803607  \n",
      "89  0.9296703296703297  \n",
      "90  0.9256410256410257  \n",
      "91   0.913468582699352  \n",
      "92  0.8084249084249083  \n",
      "93  0.8080022541561004  \n",
      "94  0.8048182586644125  \n",
      "95  0.8014370245139476  \n",
      "96  0.7997464074387151  \n",
      "97  0.7940264863341787  \n",
      "98  0.7854888701042547  \n",
      "99  0.7838827838827839  \n",
      "\n",
      "[100 rows x 13 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Sort the grid models by AUC\n",
    "sorted_final_grid = final_grid.get_grid(sort_by='auc',decreasing=True)\n",
    "\n",
    "print sorted_final_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also see the results of the grid search in [Flow](http://localhost:54321/):\n",
    "![alt text](./final_grid.png \"Final Grid Search Results in Flow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Inspection and Final Test Set Scoring\n",
    "\n",
    "Let's see how well the best model of the grid search (as judged by validation set AUC) does on the held out test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97878948064\n"
     ]
    }
   ],
   "source": [
    "#Get the best model from the list (the model name listed at the top of the table)\n",
    "best_model = h2o.get_model(sorted_final_grid.sorted_metric_table()['model_ids'][0])\n",
    "performance_best_model = best_model.model_performance(test)\n",
    "print performance_best_model.auc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good news. It does as well on the test set as on the validation set, so it looks like our best GBM model generalizes well to the unseen test set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the winning model's parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['learn_rate = 0.05',\n",
       " 'fold_column = None',\n",
       " 'col_sample_rate_per_tree = 0.6',\n",
       " 'learn_rate_annealing = 0.99',\n",
       " 'score_tree_interval = 10',\n",
       " 'sample_rate_per_class = None',\n",
       " 'seed = 1234',\n",
       " 'keep_cross_validation_predictions = False',\n",
       " \"model_id = {u'URL': u'/3/Models/final_grid_model_45', u'_exclude_fields': u'', u'type': u'Key<Model>', u'name': u'final_grid_model_45', u'__meta': {u'schema_name': u'ModelKeyV3', u'schema_version': 3, u'schema_type': u'Key<Model>'}}\",\n",
       " 'nfolds = 0',\n",
       " 'max_abs_leafnode_pred = 1.79769313486e+308',\n",
       " 'offset_column = None',\n",
       " 'quantile_alpha = 0.5',\n",
       " 'stopping_tolerance = 0.0001',\n",
       " 'fold_assignment = AUTO',\n",
       " \"training_frame = {u'URL': u'/3/Frames/train.hex', u'_exclude_fields': u'', u'type': u'Key<Frame>', u'name': u'train.hex', u'__meta': {u'schema_name': u'FrameKeyV3', u'schema_version': 3, u'schema_type': u'Key<Frame>'}}\",\n",
       " 'max_runtime_secs = 3512.106',\n",
       " 'checkpoint = None',\n",
       " 'balance_classes = False',\n",
       " 'r2_stopping = 0.999999',\n",
       " \"validation_frame = {u'URL': u'/3/Frames/valid.hex', u'_exclude_fields': u'', u'type': u'Key<Frame>', u'name': u'valid.hex', u'__meta': {u'schema_name': u'FrameKeyV3', u'schema_version': 3, u'schema_type': u'Key<Frame>'}}\",\n",
       " 'max_depth = 12',\n",
       " \"response_column = {u'is_member_of_frames': None, u'_exclude_fields': u'', u'column_name': u'survived', u'__meta': {u'schema_name': u'ColSpecifierV3', u'schema_version': 3, u'schema_type': u'VecSpecifier'}}\",\n",
       " 'build_tree_one_node = False',\n",
       " 'ntrees = 10000',\n",
       " 'min_split_improvement = 0.0001',\n",
       " \"ignored_columns = [u'name']\",\n",
       " 'tweedie_power = 1.5',\n",
       " 'min_rows = 1.0',\n",
       " 'max_confusion_matrix_size = 20',\n",
       " 'score_each_iteration = False',\n",
       " 'nbins_top_level = 1024',\n",
       " 'max_after_balance_size = 5.0',\n",
       " 'nbins = 1024',\n",
       " 'histogram_type = QuantilesGlobal',\n",
       " 'col_sample_rate = 0.73',\n",
       " 'stopping_metric = AUC',\n",
       " 'weights_column = None',\n",
       " 'stopping_rounds = 5',\n",
       " 'col_sample_rate_change_per_level = 0.9',\n",
       " 'max_hit_ratio_k = 0',\n",
       " 'nbins_cats = 256',\n",
       " 'sample_rate = 0.29',\n",
       " 'distribution = bernoulli',\n",
       " 'class_sampling_factors = None',\n",
       " 'ignore_const_cols = True',\n",
       " 'keep_cross_validation_fold_assignment = False']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_list = []\n",
    "for key, value in best_model.params.iteritems():\n",
    "    params_list.append(str(key)+\" = \"+str(value['actual']))\n",
    "params_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can confirm that these parameters are generally sound, by building a GBM model on the whole dataset (instead of the 60%) and using internal 5-fold cross-validation (re-using all other parameters including the seed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gbm = h2o.get_model(sorted_final_grid.sorted_metric_table()['model_ids'][0])\n",
    "#get the parameters from the Random grid search model and modify them slightly\n",
    "params = gbm.params\n",
    "new_params = {\"nfolds\":5, \"model_id\":None}\n",
    "for key in new_params.keys():\n",
    "    params[key]['actual'] = new_params[key] \n",
    "gbm_best = h2o.H2OGradientBoostingEstimator()\n",
    "for key in params.keys():\n",
    "    if key in dir(gbm_best) and getattr(gbm_best,key) != params[key]['actual']:\n",
    "        setattr(gbm_best,key,params[key]['actual']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "gbm Model Build Progress: [##################################################] 100%\n"
     ]
    }
   ],
   "source": [
    "gbm_best.train(x=predictors, y=response, training_frame=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-Validation Metrics Summary: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>mean</b></td>\n",
       "<td><b>sd</b></td>\n",
       "<td><b>cv_1_valid</b></td>\n",
       "<td><b>cv_2_valid</b></td>\n",
       "<td><b>cv_3_valid</b></td>\n",
       "<td><b>cv_4_valid</b></td>\n",
       "<td><b>cv_5_valid</b></td></tr>\n",
       "<tr><td>accuracy</td>\n",
       "<td>0.9443999</td>\n",
       "<td>0.0064340</td>\n",
       "<td>0.9400749</td>\n",
       "<td>0.9335793</td>\n",
       "<td>0.9379845</td>\n",
       "<td>0.9566929</td>\n",
       "<td>0.9536679</td></tr>\n",
       "<tr><td>auc</td>\n",
       "<td>0.9704713</td>\n",
       "<td>0.0070059</td>\n",
       "<td>0.9639977</td>\n",
       "<td>0.9584524</td>\n",
       "<td>0.9655612</td>\n",
       "<td>0.9839788</td>\n",
       "<td>0.9803662</td></tr>\n",
       "<tr><td>err</td>\n",
       "<td>0.0556001</td>\n",
       "<td>0.0064340</td>\n",
       "<td>0.0599251</td>\n",
       "<td>0.0664207</td>\n",
       "<td>0.0620155</td>\n",
       "<td>0.0433071</td>\n",
       "<td>0.0463320</td></tr>\n",
       "<tr><td>err_count</td>\n",
       "<td>14.6</td>\n",
       "<td>1.8761663</td>\n",
       "<td>16.0</td>\n",
       "<td>18.0</td>\n",
       "<td>16.0</td>\n",
       "<td>11.0</td>\n",
       "<td>12.0</td></tr>\n",
       "<tr><td>f0point5</td>\n",
       "<td>0.9483252</td>\n",
       "<td>0.0102195</td>\n",
       "<td>0.9469697</td>\n",
       "<td>0.9210526</td>\n",
       "<td>0.9562212</td>\n",
       "<td>0.9619687</td>\n",
       "<td>0.955414</td></tr>\n",
       "<tr><td>f1</td>\n",
       "<td>0.9237261</td>\n",
       "<td>0.0100577</td>\n",
       "<td>0.9259259</td>\n",
       "<td>0.9032258</td>\n",
       "<td>0.9120879</td>\n",
       "<td>0.9398907</td>\n",
       "<td>0.9375</td></tr>\n",
       "<tr><td>f2</td>\n",
       "<td>0.9005541</td>\n",
       "<td>0.0133487</td>\n",
       "<td>0.9057971</td>\n",
       "<td>0.886076</td>\n",
       "<td>0.8718488</td>\n",
       "<td>0.9188034</td>\n",
       "<td>0.9202454</td></tr>\n",
       "<tr><td>lift_top_group</td>\n",
       "<td>2.6258688</td>\n",
       "<td>0.0998947</td>\n",
       "<td>2.3839285</td>\n",
       "<td>2.8229167</td>\n",
       "<td>2.632653</td>\n",
       "<td>2.6736841</td>\n",
       "<td>2.6161616</td></tr>\n",
       "<tr><td>logloss</td>\n",
       "<td>0.1869302</td>\n",
       "<td>0.0142185</td>\n",
       "<td>0.2034029</td>\n",
       "<td>0.2098996</td>\n",
       "<td>0.1933566</td>\n",
       "<td>0.1559504</td>\n",
       "<td>0.1720417</td></tr>\n",
       "<tr><td>max_per_class_error</td>\n",
       "<td>0.11417</td>\n",
       "<td>0.0161180</td>\n",
       "<td>0.1071429</td>\n",
       "<td>0.125</td>\n",
       "<td>0.1530612</td>\n",
       "<td>0.0947368</td>\n",
       "<td>0.0909091</td></tr>\n",
       "<tr><td>mcc</td>\n",
       "<td>0.8823097</td>\n",
       "<td>0.0141451</td>\n",
       "<td>0.8774228</td>\n",
       "<td>0.8537732</td>\n",
       "<td>0.8707511</td>\n",
       "<td>0.9077432</td>\n",
       "<td>0.9018585</td></tr>\n",
       "<tr><td>mean_per_class_accuracy</td>\n",
       "<td>0.9331479</td>\n",
       "<td>0.0080393</td>\n",
       "<td>0.9335253</td>\n",
       "<td>0.9203572</td>\n",
       "<td>0.9203444</td>\n",
       "<td>0.9463423</td>\n",
       "<td>0.9451705</td></tr>\n",
       "<tr><td>mean_per_class_error</td>\n",
       "<td>0.0668521</td>\n",
       "<td>0.0080393</td>\n",
       "<td>0.0664747</td>\n",
       "<td>0.0796429</td>\n",
       "<td>0.0796556</td>\n",
       "<td>0.0536577</td>\n",
       "<td>0.0548295</td></tr>\n",
       "<tr><td>mse</td>\n",
       "<td>0.0501457</td>\n",
       "<td>0.0042764</td>\n",
       "<td>0.0544056</td>\n",
       "<td>0.0572033</td>\n",
       "<td>0.0529171</td>\n",
       "<td>0.0411026</td>\n",
       "<td>0.0451000</td></tr>\n",
       "<tr><td>precision</td>\n",
       "<td>0.9655963</td>\n",
       "<td>0.0130557</td>\n",
       "<td>0.9615384</td>\n",
       "<td>0.9333333</td>\n",
       "<td>0.9880952</td>\n",
       "<td>0.9772728</td>\n",
       "<td>0.9677419</td></tr>\n",
       "<tr><td>r2</td>\n",
       "<td>0.7870655</td>\n",
       "<td>0.0187190</td>\n",
       "<td>0.7765828</td>\n",
       "<td>0.7499365</td>\n",
       "<td>0.7753588</td>\n",
       "<td>0.8244438</td>\n",
       "<td>0.8090054</td></tr>\n",
       "<tr><td>recall</td>\n",
       "<td>0.88583</td>\n",
       "<td>0.0161180</td>\n",
       "<td>0.8928571</td>\n",
       "<td>0.875</td>\n",
       "<td>0.8469388</td>\n",
       "<td>0.9052632</td>\n",
       "<td>0.9090909</td></tr>\n",
       "<tr><td>specificity</td>\n",
       "<td>0.9804658</td>\n",
       "<td>0.0069456</td>\n",
       "<td>0.9741936</td>\n",
       "<td>0.9657143</td>\n",
       "<td>0.99375</td>\n",
       "<td>0.9874214</td>\n",
       "<td>0.98125</td></tr></table></div>"
      ],
      "text/plain": [
       "                         mean       sd          cv_1_valid    cv_2_valid    cv_3_valid    cv_4_valid    cv_5_valid\n",
       "-----------------------  ---------  ----------  ------------  ------------  ------------  ------------  ------------\n",
       "accuracy                 0.9444     0.00643399  0.940075      0.933579      0.937984      0.956693      0.953668\n",
       "auc                      0.970471   0.00700594  0.963998      0.958452      0.965561      0.983979      0.980366\n",
       "err                      0.0556001  0.00643399  0.0599251     0.0664207     0.0620155     0.0433071     0.046332\n",
       "err_count                14.6       1.87617     16            18            16            11            12\n",
       "f0point5                 0.948325   0.0102195   0.94697       0.921053      0.956221      0.961969      0.955414\n",
       "f1                       0.923726   0.0100577   0.925926      0.903226      0.912088      0.939891      0.9375\n",
       "f2                       0.900554   0.0133487   0.905797      0.886076      0.871849      0.918803      0.920245\n",
       "lift_top_group           2.62587    0.0998947   2.38393       2.82292       2.63265       2.67368       2.61616\n",
       "logloss                  0.18693    0.0142185   0.203403      0.2099        0.193357      0.15595       0.172042\n",
       "max_per_class_error      0.11417    0.016118    0.107143      0.125         0.153061      0.0947368     0.0909091\n",
       "mcc                      0.88231    0.0141451   0.877423      0.853773      0.870751      0.907743      0.901859\n",
       "mean_per_class_accuracy  0.933148   0.00803933  0.933525      0.920357      0.920344      0.946342      0.94517\n",
       "mean_per_class_error     0.0668521  0.00803933  0.0664747     0.0796429     0.0796556     0.0536577     0.0548295\n",
       "mse                      0.0501457  0.00427637  0.0544056     0.0572033     0.0529171     0.0411026     0.0451\n",
       "precision                0.965596   0.0130557   0.961538      0.933333      0.988095      0.977273      0.967742\n",
       "r2                       0.787065   0.018719    0.776583      0.749936      0.775359      0.824444      0.809005\n",
       "recall                   0.88583    0.016118    0.892857      0.875         0.846939      0.905263      0.909091\n",
       "specificity              0.980466   0.0069456   0.974194      0.965714      0.99375       0.987421      0.98125"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print gbm_best.cross_validation_metrics_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the winning model performs slightly better on the validation and test sets than during cross-validation on the training set as the mean AUC on the 5 folds is estimated to be only 97.04%, but with a fairly large standard deviation of 0.7%. For small datasets, such a large variance is not unusual. To get a better estimate of model performance, the Random hyper-parameter search could have used `nfolds = 5` (or 10, or similar) in combination with 80% of the data for training (i.e., not holding out a validation set, but only the final test set). However, this would take more time, as `nfolds+1` models will be built for every set of parameters.\n",
    "\n",
    "Instead, to save time, let's just scan through the top 5 models and cross-validate their parameters with `nfolds=5` on the entire dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "gbm Model Build Progress: [##################################################] 100%\n",
      "final_grid_model_45\n",
      "                       auc\n",
      "mean            0.97047126\n",
      "sd            0.0070059407\n",
      "cv_1_valid       0.9639977\n",
      "cv_2_valid       0.9584524\n",
      "cv_3_valid       0.9655612\n",
      "cv_4_valid       0.9839788\n",
      "cv_5_valid       0.9803662\n",
      "Name: 1, dtype: object\n",
      "\n",
      "gbm Model Build Progress: [##################################################] 100%\n",
      "final_grid_model_68\n",
      "                      auc\n",
      "mean            0.9719337\n",
      "sd            0.006935956\n",
      "cv_1_valid      0.9698157\n",
      "cv_2_valid     0.95497024\n",
      "cv_3_valid      0.9716199\n",
      "cv_4_valid     0.98106587\n",
      "cv_5_valid       0.982197\n",
      "Name: 1, dtype: object\n",
      "\n",
      "gbm Model Build Progress: [##################################################] 100%\n",
      "final_grid_model_96\n",
      "                      auc\n",
      "mean            0.9722442\n",
      "sd            0.005797062\n",
      "cv_1_valid      0.9639401\n",
      "cv_2_valid     0.96068454\n",
      "cv_3_valid       0.978125\n",
      "cv_4_valid       0.978484\n",
      "cv_5_valid      0.9799874\n",
      "Name: 1, dtype: object\n",
      "\n",
      "gbm Model Build Progress: [##################################################] 100%\n",
      "final_grid_model_0\n",
      "                      auc\n",
      "mean           0.96713257\n",
      "sd            0.008364663\n",
      "cv_1_valid     0.96059906\n",
      "cv_2_valid     0.94699407\n",
      "cv_3_valid     0.97302294\n",
      "cv_4_valid     0.97815293\n",
      "cv_5_valid     0.97689396\n",
      "Name: 1, dtype: object\n",
      "\n",
      "gbm Model Build Progress: [##################################################] 100%\n",
      "final_grid_model_38\n",
      "                      auc\n",
      "mean           0.97262114\n",
      "sd            0.005390482\n",
      "cv_1_valid     0.96797234\n",
      "cv_2_valid      0.9598512\n",
      "cv_3_valid      0.9776148\n",
      "cv_4_valid     0.98020524\n",
      "cv_5_valid      0.9774621\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "for i in range(5): \n",
    "    gbm = h2o.get_model(sorted_final_grid.sorted_metric_table()['model_ids'][i])\n",
    "    #get the parameters from the Random grid search model and modify them slightly\n",
    "    params = gbm.params\n",
    "    new_params = {\"nfolds\":5, \"model_id\":None}\n",
    "    for key in new_params.keys():\n",
    "        params[key]['actual'] = new_params[key]\n",
    "    new_model = h2o.H2OGradientBoostingEstimator()\n",
    "    for key in params.keys():\n",
    "        if key in dir(new_model) and getattr(new_model,key) != params[key]['actual']:\n",
    "            setattr(new_model,key,params[key]['actual'])\n",
    "    new_model.train(x = predictors, y = response, training_frame = df)  \n",
    "    cv_summary = new_model.cross_validation_metrics_summary().as_data_frame()\n",
    "    print(gbm.model_id)\n",
    "    print(cv_summary.iloc[1]) ## AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The avid reader might have noticed that we just implicitly did further parameter tuning using the \"final\" test set (which is part of the entire dataset `df`), which is not good practice - one is not supposed to use the \"final\" test set more than once. Hence, we're not going to pick a different \"best\" model, but we're just learning about the variance in AUCs. It turns out, for this tiny dataset, that the variance is rather large, which is not surprising.\n",
    "\n",
    "Keeping the same \"best\" model, we can make test set predictions as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "gbm prediction Progress: [##################################################] 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th style=\"text-align: right;\">  predict</th><th style=\"text-align: right;\">       p0</th><th style=\"text-align: right;\">       p1</th></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.948394 </td><td style=\"text-align: right;\">0.0516064</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.940819 </td><td style=\"text-align: right;\">0.0591814</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.924897 </td><td style=\"text-align: right;\">0.0751026</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.0156249</td><td style=\"text-align: right;\">0.984375 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.0116196</td><td style=\"text-align: right;\">0.98838  </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.854808 </td><td style=\"text-align: right;\">0.145192 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.0440646</td><td style=\"text-align: right;\">0.955935 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.0115973</td><td style=\"text-align: right;\">0.988403 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.0544365</td><td style=\"text-align: right;\">0.945563 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.919649 </td><td style=\"text-align: right;\">0.0803507</td></tr>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = best_model.predict(test)\n",
    "preds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the label (survived or not) is predicted as well (in the first predict column), and it uses the threshold with the highest F1 score (here: 0.528098) to make labels from the probabilities for survival (`p1`). The probability for death (`p0`) is given for convenience, as it is just `1-p1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ModelMetricsBinomial: gbm\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 0.0490254194251\n",
      "R^2: 0.792582001197\n",
      "LogLoss: 0.183618009053\n",
      "Mean Per-Class Error: 0.0666666666667\n",
      "AUC: 0.97235841082\n",
      "Gini: 0.94471682164\n",
      "\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.492182327674: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>169.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td> (0.0/169.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>14.0</td>\n",
       "<td>91.0</td>\n",
       "<td>0.1333</td>\n",
       "<td> (14.0/105.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>183.0</td>\n",
       "<td>91.0</td>\n",
       "<td>0.0511</td>\n",
       "<td> (14.0/274.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0    1    Error    Rate\n",
       "-----  ---  ---  -------  ------------\n",
       "0      169  0    0        (0.0/169.0)\n",
       "1      14   91   0.1333   (14.0/105.0)\n",
       "Total  183  91   0.0511   (14.0/274.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.4921823</td>\n",
       "<td>0.9285714</td>\n",
       "<td>90.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1612587</td>\n",
       "<td>0.9074074</td>\n",
       "<td>119.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.4921823</td>\n",
       "<td>0.9701493</td>\n",
       "<td>90.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.4921823</td>\n",
       "<td>0.9489051</td>\n",
       "<td>90.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9885634</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0501451</td>\n",
       "<td>1.0</td>\n",
       "<td>199.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9885634</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_MCC</td>\n",
       "<td>0.4921823</td>\n",
       "<td>0.8946308</td>\n",
       "<td>90.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.2158661</td>\n",
       "<td>0.9053254</td>\n",
       "<td>111.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.4921823</td>\n",
       "<td>0.9333333</td>\n",
       "<td>90.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.492182     0.928571  90\n",
       "max f2                       0.161259     0.907407  119\n",
       "max f0point5                 0.492182     0.970149  90\n",
       "max accuracy                 0.492182     0.948905  90\n",
       "max precision                0.988563     1         0\n",
       "max recall                   0.0501451    1         199\n",
       "max specificity              0.988563     1         0\n",
       "max absolute_MCC             0.492182     0.894631  90\n",
       "max min_per_class_accuracy   0.215866     0.905325  111\n",
       "max mean_per_class_accuracy  0.492182     0.933333  90"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gains/Lift Table: Avg response rate: 38.32 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0109489</td>\n",
       "<td>0.9878848</td>\n",
       "<td>2.6095238</td>\n",
       "<td>2.6095238</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0285714</td>\n",
       "<td>0.0285714</td>\n",
       "<td>160.9523810</td>\n",
       "<td>160.9523810</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0218978</td>\n",
       "<td>0.9871058</td>\n",
       "<td>2.6095238</td>\n",
       "<td>2.6095238</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0285714</td>\n",
       "<td>0.0571429</td>\n",
       "<td>160.9523810</td>\n",
       "<td>160.9523810</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0328467</td>\n",
       "<td>0.9866763</td>\n",
       "<td>2.6095238</td>\n",
       "<td>2.6095238</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0285714</td>\n",
       "<td>0.0857143</td>\n",
       "<td>160.9523810</td>\n",
       "<td>160.9523810</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0401460</td>\n",
       "<td>0.9858998</td>\n",
       "<td>2.6095238</td>\n",
       "<td>2.6095238</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0190476</td>\n",
       "<td>0.1047619</td>\n",
       "<td>160.9523810</td>\n",
       "<td>160.9523810</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0510949</td>\n",
       "<td>0.9856960</td>\n",
       "<td>2.6095238</td>\n",
       "<td>2.6095238</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0285714</td>\n",
       "<td>0.1333333</td>\n",
       "<td>160.9523810</td>\n",
       "<td>160.9523810</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1021898</td>\n",
       "<td>0.9798586</td>\n",
       "<td>2.6095238</td>\n",
       "<td>2.6095238</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.1333333</td>\n",
       "<td>0.2666667</td>\n",
       "<td>160.9523810</td>\n",
       "<td>160.9523810</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1496350</td>\n",
       "<td>0.9731845</td>\n",
       "<td>2.6095238</td>\n",
       "<td>2.6095238</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.1238095</td>\n",
       "<td>0.3904762</td>\n",
       "<td>160.9523810</td>\n",
       "<td>160.9523810</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2007299</td>\n",
       "<td>0.9548071</td>\n",
       "<td>2.6095238</td>\n",
       "<td>2.6095238</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.1333333</td>\n",
       "<td>0.5238095</td>\n",
       "<td>160.9523810</td>\n",
       "<td>160.9523810</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.2992701</td>\n",
       "<td>0.8924580</td>\n",
       "<td>2.6095238</td>\n",
       "<td>2.6095238</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.2571429</td>\n",
       "<td>0.7809524</td>\n",
       "<td>160.9523810</td>\n",
       "<td>160.9523810</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4014599</td>\n",
       "<td>0.2327126</td>\n",
       "<td>1.2115646</td>\n",
       "<td>2.2536797</td>\n",
       "<td>0.4642857</td>\n",
       "<td>0.8636364</td>\n",
       "<td>0.1238095</td>\n",
       "<td>0.9047619</td>\n",
       "<td>21.1564626</td>\n",
       "<td>125.3679654</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5</td>\n",
       "<td>0.1163121</td>\n",
       "<td>0.4832451</td>\n",
       "<td>1.9047619</td>\n",
       "<td>0.1851852</td>\n",
       "<td>0.7299270</td>\n",
       "<td>0.0476190</td>\n",
       "<td>0.9523810</td>\n",
       "<td>-51.6754850</td>\n",
       "<td>90.4761905</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5985401</td>\n",
       "<td>0.0723806</td>\n",
       "<td>0.1932981</td>\n",
       "<td>1.6229965</td>\n",
       "<td>0.0740741</td>\n",
       "<td>0.6219512</td>\n",
       "<td>0.0190476</td>\n",
       "<td>0.9714286</td>\n",
       "<td>-80.6701940</td>\n",
       "<td>62.2996516</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.7007299</td>\n",
       "<td>0.0526387</td>\n",
       "<td>0.1863946</td>\n",
       "<td>1.4134921</td>\n",
       "<td>0.0714286</td>\n",
       "<td>0.5416667</td>\n",
       "<td>0.0190476</td>\n",
       "<td>0.9904762</td>\n",
       "<td>-81.3605442</td>\n",
       "<td>41.3492063</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7992701</td>\n",
       "<td>0.0440208</td>\n",
       "<td>0.0966490</td>\n",
       "<td>1.2511416</td>\n",
       "<td>0.0370370</td>\n",
       "<td>0.4794521</td>\n",
       "<td>0.0095238</td>\n",
       "<td>1.0</td>\n",
       "<td>-90.3350970</td>\n",
       "<td>25.1141553</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8978102</td>\n",
       "<td>0.0369130</td>\n",
       "<td>0.0</td>\n",
       "<td>1.1138211</td>\n",
       "<td>0.0</td>\n",
       "<td>0.4268293</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>11.3821138</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0165575</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.3832117</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0109489                   0.987885           2.60952   2.60952            1                1                           0.0285714       0.0285714                  160.952   160.952\n",
       "    2        0.0218978                   0.987106           2.60952   2.60952            1                1                           0.0285714       0.0571429                  160.952   160.952\n",
       "    3        0.0328467                   0.986676           2.60952   2.60952            1                1                           0.0285714       0.0857143                  160.952   160.952\n",
       "    4        0.040146                    0.9859             2.60952   2.60952            1                1                           0.0190476       0.104762                   160.952   160.952\n",
       "    5        0.0510949                   0.985696           2.60952   2.60952            1                1                           0.0285714       0.133333                   160.952   160.952\n",
       "    6        0.10219                     0.979859           2.60952   2.60952            1                1                           0.133333        0.266667                   160.952   160.952\n",
       "    7        0.149635                    0.973184           2.60952   2.60952            1                1                           0.12381         0.390476                   160.952   160.952\n",
       "    8        0.20073                     0.954807           2.60952   2.60952            1                1                           0.133333        0.52381                    160.952   160.952\n",
       "    9        0.29927                     0.892458           2.60952   2.60952            1                1                           0.257143        0.780952                   160.952   160.952\n",
       "    10       0.40146                     0.232713           1.21156   2.25368            0.464286         0.863636                    0.12381         0.904762                   21.1565   125.368\n",
       "    11       0.5                         0.116312           0.483245  1.90476            0.185185         0.729927                    0.047619        0.952381                   -51.6755  90.4762\n",
       "    12       0.59854                     0.0723806          0.193298  1.623              0.0740741        0.621951                    0.0190476       0.971429                   -80.6702  62.2997\n",
       "    13       0.70073                     0.0526387          0.186395  1.41349            0.0714286        0.541667                    0.0190476       0.990476                   -81.3605  41.3492\n",
       "    14       0.79927                     0.0440208          0.096649  1.25114            0.037037         0.479452                    0.00952381      1                          -90.3351  25.1142\n",
       "    15       0.89781                     0.036913           0         1.11382            0                0.426829                    0               1                          -100      11.3821\n",
       "    16       1                           0.0165575          0         1                  0                0.383212                    0               1                          -100      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.model_performance(valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also see the \"best\" model in more detail in [Flow](http://localhost:54321/):\n",
    "![alt text](./best_gbm1.png \"Best GBM from Grid Search in Flow\")\n",
    "![alt text](./best_gbm2.png \"Best GBM from Grid Search in Flow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model and the predictions can be saved to file as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Export File Progress: [##################################################] 100%\n"
     ]
    }
   ],
   "source": [
    "h2o.save_model(best_model, \"/tmp/bestModel.csv\", force=True)\n",
    "h2o.export_file(preds, \"/tmp/bestPreds.csv\", force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#h2o.download_pojo(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can also be exported as a plain old Java object (POJO) for H2O-independent (standalone/Storm/Kafka/UDF) scoring in any Java environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```\n",
    "/*\n",
    " Licensed under the Apache License, Version 2.0\n",
    "    http://www.apache.org/licenses/LICENSE-2.0.html\n",
    "\n",
    "  AUTOGENERATED BY H2O at 2016-07-17T18:38:50.337-07:00\n",
    "  3.8.3.3\n",
    "\n",
    "  Standalone prediction code with sample test data for GBMModel named final_grid_model_45\n",
    "\n",
    "  How to download, compile and execute:\n",
    "      mkdir tmpdir\n",
    "      cd tmpdir\n",
    "      curl http://127.0.0.1:54321/3/h2o-genmodel.jar > h2o-genmodel.jar\n",
    "      curl http://127.0.0.1:54321/3/Models.java/final_grid_model_45 > final_grid_model_45.java\n",
    "      javac -cp h2o-genmodel.jar -J-Xmx2g -J-XX:MaxPermSize=128m final_grid_model_45.java\n",
    "\n",
    "     (Note:  Try java argument -XX:+PrintCompilation to show runtime JIT compiler behavior.)\n",
    "*/\n",
    "import java.util.Map;\n",
    "import hex.genmodel.GenModel;\n",
    "import hex.genmodel.annotations.ModelPojo;\n",
    "\n",
    "...\n",
    "class final_grid_model_45_Tree_0_class_0 {\n",
    "  static final double score0(double[] data) {\n",
    "    double pred =      (Double.isNaN(data[1]) || !GenModel.bitSetContains(GRPSPLIT0, 0, data[1 /* sex */]) ? \n",
    "         (Double.isNaN(data[7]) || !GenModel.bitSetContains(GRPSPLIT1, 13, data[7 /* cabin */]) ? \n",
    "             (Double.isNaN(data[7]) || !GenModel.bitSetContains(GRPSPLIT2, 9, data[7 /* cabin */]) ? \n",
    "                 (Double.isNaN(data[7]) || !GenModel.bitSetContains(GRPSPLIT3, 9, data[7 /* cabin */]) ? \n",
    "                     (data[2 /* age */] <1.4174492f ? \n",
    "                        0.13087687f : \n",
    "                         (Double.isNaN(data[7]) || !GenModel.bitSetContains(GRPSPLIT4, 9, data[7 /* cabin */]) ? \n",
    "                             (Double.isNaN(data[3]) || data[3 /* sibsp */] <1.000313f ? \n",
    "                                 (data[6 /* fare */] <7.91251f ? \n",
    "                                     (Double.isNaN(data[5]) || data[5 /* ticket */] <368744.5f ? \n",
    "                                        -0.08224204f : \n",
    "                                         (Double.isNaN(data[2]) || data[2 /* age */] <13.0f ? \n",
    "                                            -0.028962314f : \n",
    "                                            -0.08224204f)) : \n",
    "                                     (Double.isNaN(data[7]) || !GenModel.bitSetContains(GRPSPLIT5, 9, data[7 /* cabin */]) ? \n",
    "                                         (data[6 /* fare */] <7.989957f ? \n",
    "                                             (Double.isNaN(data[3]) || data[3 /* sibsp */] <0.0017434144f ? \n",
    "                                                0.07759714f : \n",
    "                                                0.13087687f) : \n",
    "                                             (data[6 /* fare */] <12.546303f ? \n",
    "                                                -0.07371729f : \n",
    "                                                 (Double.isNaN(data[4]) || data[4 /* parch */] <1.0020853f ? \n",
    "                                                    -0.037374903f : \n",
    "                                                    -0.08224204f))) : \n",
    "                                        0.0f)) : \n",
    "                                -0.08224204f) : \n",
    "                            0.0f)) : \n",
    "                    0.0f) : \n",
    "                -0.08224204f) : \n",
    "            -0.08224204f) :  \n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling Techniques\n",
    "\n",
    "After learning above that the variance of the test set AUC of the top few models was rather large, we might be able to turn this into our advantage by using ensembling techniques. The simplest one is taking the average of the predictions (survival probabilities) of the top `k` grid search model predictions (here, we use `k=10`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "gbm prediction Progress: [##################################################] 100%\n",
      "\n",
      "gbm prediction Progress: [##################################################] 100%\n",
      "\n",
      "gbm prediction Progress: [##################################################] 100%\n",
      "\n",
      "gbm prediction Progress: [##################################################] 100%\n",
      "\n",
      "gbm prediction Progress: [##################################################] 100%\n",
      "\n",
      "gbm prediction Progress: [##################################################] 100%\n",
      "\n",
      "gbm prediction Progress: [##################################################] 100%\n",
      "\n",
      "gbm prediction Progress: [##################################################] 100%\n",
      "\n",
      "gbm prediction Progress: [##################################################] 100%\n",
      "\n",
      "gbm prediction Progress: [##################################################] 100%\n"
     ]
    }
   ],
   "source": [
    "prob = None\n",
    "k=10\n",
    "for i in range(0,k): \n",
    "    gbm = h2o.get_model(sorted_final_grid.sorted_metric_table()['model_ids'][i])\n",
    "    if (prob is None):\n",
    "        prob = gbm.predict(test)[\"p1\"]\n",
    "    else:\n",
    "        prob = prob + gbm.predict(test)[\"p1\"]\n",
    "prob = prob/k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a blended probability of survival for each person on the Titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th style=\"text-align: right;\">       p1</th></tr>\n",
       "<tr><td style=\"text-align: right;\">0.0596246</td></tr>\n",
       "<tr><td style=\"text-align: right;\">0.0511568</td></tr>\n",
       "<tr><td style=\"text-align: right;\">0.115645 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">0.977491 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">0.981366 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">0.210353 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">0.948326 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">0.976989 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">0.944669 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">0.107771 </td></tr>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can bring those ensemble predictions to our Python session's memory space and use other Python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98202722347033167"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "# convert prob and test[response] h2oframes to pandas' frames and then convert them each to numpy array\n",
    "np_array_prob = prob.as_data_frame().as_matrix()\n",
    "np_array_test = test[response].as_data_frame().as_matrix()\n",
    "probInPy = np_array_prob\n",
    "labeInPy = np_array_test\n",
    "# compare true scores (test[response]) to probability scores (prob)\n",
    "roc_auc_score(labeInPy, probInPy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple blended ensemble test set prediction has an even higher AUC than the best single model, but we need to do more validation studies, ideally using cross-validation. We leave this as an exercise for the reader - take the parameters of the top `10` models, retrain them with `nfolds=5` on the full dataset, set `keep_holdout_predictions=True` and sum up their predicted probabilities, then score that with sklearn's roc_auc_score as shown above.\n",
    "\n",
    "For more sophisticated ensembling approaches, such as stacking via a superlearner, we refer to the [H2O Ensemble](https://github.com/h2oai/h2o-3/tree/master/h2o-r/ensemble) github page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "We learned how to build H2O GBM models for a binary classification task on a small but realistic dataset with numerical and categorical variables, with the goal to maximize the AUC (ranges from 0.5 to 1). We first established a baseline with the default model, then carefully tuned the remaining hyper-parameters without \"too much\" human guess-work. We used both Cartesian and Random hyper-parameter searches to find good models. We were able to get the AUC on a holdout test set from the low 94% range with the default model to the mid 97% range after tuning, and to above 98% with some simple ensembling technique known as blending. We performed simple cross-validation variance analysis to learn that results were slightly \"lucky\" due to the specific train/valid/test set splits, and settled to expect 97% AUCs instead.\n",
    "\n",
    "Note that this script and the findings therein are directly transferrable to large datasets on distributed clusters including Spark/Hadoop environments.\n",
    "\n",
    "More information can be found here [http://www.h2o.ai/docs/](http://www.h2o.ai/docs/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
