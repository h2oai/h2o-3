diff --git a/build.gradle b/build.gradle
index 6db4590..ea98bc6 100644
--- a/build.gradle
+++ b/build.gradle
@@ -271,3 +271,11 @@ clean.dependsOn cleanTarget
 // Import project development profiles
 //
 apply from: "gradle/profiles.gradle"
+
+//
+// Import jacoco plugin if needed
+//
+if (project.hasProperty("jacocoCoverage")) {
+    apply from: "$rootDir/gradle/jacoco.gradle"
+}
+
diff --git a/gradle/jacoco.gradle b/gradle/jacoco.gradle
new file mode 100644
index 0000000..30cbe43
--- /dev/null
+++ b/gradle/jacoco.gradle
@@ -0,0 +1,69 @@
+// Gradle file for jacoco code coverage
+
+apply plugin: 'jacoco'
+
+//
+// When using v2.0+ of the Jenkins JaCoCo plugin, the
+// JaCoCo version needs to be v0.7.5+
+// 
+jacoco {
+    toolVersion = "0.7.5.201505241946"
+    reportsDir = file("${buildDir}/reports/jacoco/")
+}
+
+repositories {
+    mavenCentral()
+}
+
+String destination = "${buildDir}/reports/jacoco/report.exec"
+
+task jacocoMergeExecs(type: JacocoMerge) {
+    FileTree execs = fileTree(dir: "${rootDir}", include: "**/*.exec")
+    executionData = execs
+    destinationFile = new File(destination) 
+}
+
+//
+// Generate a test report for all tests
+//
+jacocoTestReport {
+    // String to help locate the directories of the source code
+    String sourceDirEnd = "/src/main/java/"
+
+    // Collect .exec file
+    FileTree execLocation = fileTree(destination)
+    executionData = execLocation
+
+    // Collect source files
+    FileTree sourceLocation = fileTree("$rootDir")
+    sourceLocation.include ("**" + sourceDirEnd + "**")
+    sourceLocation.each {File file ->
+    	String fileDir = file.getParent() + "/"
+	fileDir = fileDir.substring(0, (fileDir.lastIndexOf(sourceDirEnd) + sourceDirEnd.length()))
+    	if (sourceDirectories == null) {
+	   sourceDirectories = files(fileDir)
+	} else {
+           sourceDirectories = sourceDirectories + files(fileDir)
+	}
+    }
+
+    // Collect class files
+    FileTree classLocation = fileTree(dir: "$rootDir", include: "**/build/libs/**/*.jar", excludes: ["build/", "h2o-assembly/"])
+    classDirectories = classLocation
+    
+    reports {
+        xml.enabled true
+        csv.enabled false
+        html.enabled true
+    }
+}
+
+task cleanCoverageReports(type: Delete) {
+    delete files("${buildDir}/reports/jacoco")
+}
+
+task cleanCoverageData (type: Delete) {
+    delete file("${buildDir}/reports/jacoco/report.exec")
+}
+
+jacocoTestReport.dependsOn jacocoMergeExecs
\ No newline at end of file
diff --git a/gradle/multiNodeTesting.gradle b/gradle/multiNodeTesting.gradle
index 587714b..407a331 100644
--- a/gradle/multiNodeTesting.gradle
+++ b/gradle/multiNodeTesting.gradle
@@ -4,5 +4,9 @@ task testMultiNode(type: Exec) {
     if (project.hasProperty("java6Convert")) {
       environment "TEST_JAVA_HOME", System.getenv("JAVA_6_HOME")
     }
-    commandLine 'bash', './testMultiNode.sh'
+    def args = ['bash', './testMultiNode.sh']
+    if (project.hasProperty("jacocoCoverage")) {
+        args << 'jacoco'
+    }
+    commandLine args
 }
diff --git a/h2o-algos/src/main/java/hex/DataInfo.java b/h2o-algos/src/main/java/hex/DataInfo.java
index c88785b..8f9456a 100644
--- a/h2o-algos/src/main/java/hex/DataInfo.java
+++ b/h2o-algos/src/main/java/hex/DataInfo.java
@@ -86,7 +86,7 @@ public class DataInfo extends Keyed<DataInfo> {
   public int _nums;
   public int _cats;
   public int [] _catOffsets;
-  public int [] _catMissing;  // bucket for missing categoricals
+  public boolean [] _catMissing;  // bucket for missing categoricals
   public int [] _catModes;    // majority class of each categorical col
   public int [] _permutation; // permutation matrix mapping input col indices to adaptedFrame
   public double [] _normMul;
@@ -179,15 +179,16 @@ public class DataInfo extends Keyed<DataInfo> {
     // Compute the cardinality of each cat
     _catModes = new int[_cats];
     _catOffsets = MemoryManager.malloc4(ncats+1);
-    _catMissing = new int[ncats];
+    _catMissing = new boolean[ncats];
     int len = _catOffsets[0] = 0;
+
     for(int i = 0; i < ncats; ++i) {
       _catModes[i] = imputeCat(train.vec(cats[i]));
       _permutation[i] = cats[i];
       names[i]  =   train._names[cats[i]];
       Vec v = (tvecs2[i] = tvecs[cats[i]]);
-      _catMissing[i] = missingBucket ? 1 : 0; //needed for test time
-      _catOffsets[i+1] = (len += v.domain().length - (useAllFactorLevels?0:1) + (missingBucket ? 1 : 0)); //missing values turn into a new factor level
+      _catMissing[i] = missingBucket; //needed for test time
+      _catOffsets[i+1] = (len += v.domain().length - (useAllFactorLevels?0:1) + (missingBucket? 1 : 0)); //missing values turn into a new factor level
     }
     _numMeans = new double[_nums];
     for(int i = 0; i < _nums; ++i){
@@ -217,14 +218,14 @@ public class DataInfo extends Keyed<DataInfo> {
   }
 
   public DataInfo validDinfo(Frame valid) {
-    DataInfo res = new DataInfo(_adaptedFrame,null,1,_useAllFactorLevels,TransformType.NONE,TransformType.NONE,_skipMissing,_imputeMissing,false,_weights,_offset,_fold);
+    DataInfo res = new DataInfo(_adaptedFrame,null,1,_useAllFactorLevels,TransformType.NONE,TransformType.NONE,_skipMissing,_imputeMissing,!(_skipMissing || _imputeMissing),_weights,_offset,_fold);
     res._adaptedFrame = new Frame(_adaptedFrame.names(),valid.vecs(_adaptedFrame.names()));
     res._valid = true;
     return res;
   }
 
   public DataInfo scoringInfo(){
-    DataInfo res = new DataInfo(_adaptedFrame,null,1,_useAllFactorLevels,TransformType.NONE,TransformType.NONE,_skipMissing,_imputeMissing,false,_weights,_offset,_fold);
+    DataInfo res = new DataInfo(_adaptedFrame,null,1,_useAllFactorLevels,TransformType.NONE,TransformType.NONE,_skipMissing,_imputeMissing,!_skipMissing,_weights,_offset,_fold);
     res._adaptedFrame = null;
     res._weights = false;
     res._offset = false;
@@ -270,7 +271,8 @@ public class DataInfo extends Keyed<DataInfo> {
     _imputeMissing = imputeMissing;
     _adaptedFrame = fr;
     _catOffsets = MemoryManager.malloc4(catLevels.length + 1);
-    _catMissing = new int[catLevels.length];
+    _catMissing = new boolean[catLevels.length];
+    Arrays.fill(_catMissing,!(imputeMissing || skipMissing));
     int s = 0;
     for(int i = 0; i < catLevels.length; ++i){
       _catOffsets[i] = s;
@@ -374,10 +376,23 @@ public class DataInfo extends Keyed<DataInfo> {
     }
     if(_predictor_transform.isMeanAdjusted()) {
       if(mean.length != _normSub.length)
-        throw new IllegalArgumentException("Length of sigmas does not match number of scaled columns.");
+        throw new IllegalArgumentException("Length of means does not match number of scaled columns.");
       System.arraycopy(mean,0,_normSub,0,mean.length);
     }
   }
+  public void updateWeightedSigmaAndMeanForResponse(double [] sigmas, double [] mean) {
+    if(_response_transform.isSigmaScaled()) {
+      if(sigmas.length != _normRespMul.length)
+        throw new IllegalArgumentException("Length of sigmas does not match number of scaled columns.");
+      for(int i = 0; i < sigmas.length; ++i)
+        _normRespMul[i] = sigmas[i] != 0?1.0/sigmas[i]:1;
+    }
+    if(_response_transform.isMeanAdjusted()) {
+      if(mean.length != _normRespSub.length)
+        throw new IllegalArgumentException("Length of means does not match number of scaled columns.");
+      System.arraycopy(mean,0,_normRespSub,0,mean.length);
+    }
+  }
 
   private void setTransform(TransformType t, double [] normMul, double [] normSub, int vecStart, int n) {
     for (int i = 0; i < n; ++i) {
@@ -445,7 +460,7 @@ public class DataInfo extends Keyed<DataInfo> {
           continue;
         res[k++] = _adaptedFrame._names[i] + "." + vecs[i].domain()[j];
       }
-      if (_catMissing[i] > 0) res[k++] = _adaptedFrame._names[i] + ".missing(NA)";
+      if (_catMissing[i]) res[k++] = _adaptedFrame._names[i] + ".missing(NA)";
     }
     final int nums = n-k;
     System.arraycopy(_adaptedFrame._names, _cats, res, k, nums);
@@ -618,17 +633,15 @@ public class DataInfo extends Keyed<DataInfo> {
     int v = c + _catOffsets[cid];
     if(v >= _catOffsets[cid+1]) { // previously unseen level
       assert _valid:"categorical value out of bounds, got " + v + ", next cat starts at " + _catOffsets[cid+1];
-      return -2;
+      return _catMissing[cid]?_catOffsets[cid+1]-1:-2;// if we have NA bucket, treat previously unseen as NA.
     }
     return v;
   }
 
-  public final Row extractDenseRow(double [] vals, Row row, double w, double o) {
+  public final Row extractDenseRow(double [] vals, Row row) {
     row.bad = false;
     row.rid = 0;
     row.cid = 0;
-    row.offset = o;
-    row.weight = w;
     if(row.weight == 0) return row;
 
     if (_skipMissing)
@@ -644,7 +657,7 @@ public class DataInfo extends Keyed<DataInfo> {
           int c = getCategoricalId(i,_catModes[i]);
           if(c >= 0)
             row.binIds[nbins++] = c;
-        } else   // TODO: What if missingBucket = false?
+        } else if(_catMissing[i])  // TODO: What if missingBucket = false?
           row.binIds[nbins++] = _catOffsets[i + 1] - 1; // missing value turns into extra (last) factor
       } else {
         int c = getCategoricalId(i,(int)vals[i]);
@@ -697,8 +710,9 @@ public class DataInfo extends Keyed<DataInfo> {
             int c = getCategoricalId(i,_catModes[i]);
             if(c >= 0)
               row.binIds[nbins++] = c;
-          } else   // TODO: What if missingBucket = false?
+          } else if(_catMissing[i])  // TODO: What if missingBucket = false?
             row.binIds[nbins++] = _catOffsets[i + 1] - 1; // missing value turns into extra (last) factor
+          // else skip
       } else {
         int c = getCategoricalId(i,(int)chunks[i].at8(rid));
         if(c >= 0)
diff --git a/h2o-algos/src/main/java/hex/FrameTask.java b/h2o-algos/src/main/java/hex/FrameTask.java
index d0c60ed..5921a26 100644
--- a/h2o-algos/src/main/java/hex/FrameTask.java
+++ b/h2o-algos/src/main/java/hex/FrameTask.java
@@ -100,8 +100,10 @@ public abstract class FrameTask<T extends FrameTask<T>> extends MRTask<T>{
     final long offset = chunks[0].start();
     boolean doWork = chunkInit();
     if (!doWork) return;
-    final boolean obs_weights = _dinfo._weights && !_fr.vecs()[_dinfo.weightChunkId()].isConst();
-    final double global_weight_sum = obs_weights ? _fr.vecs()[_dinfo.weightChunkId()].mean() * _fr.numRows() : 0;
+    final boolean obs_weights = _dinfo._weights
+            && !_fr.vecs()[_dinfo.weightChunkId()].isConst() //if all constant weights (such as 1) -> doesn't count as obs weights
+            && !(_fr.vecs()[_dinfo.weightChunkId()].isBinary()); //special case for cross-val      -> doesn't count as obs weights
+    final double global_weight_sum = obs_weights ? Math.round(_fr.vecs()[_dinfo.weightChunkId()].mean() * _fr.numRows()) : 0;
 
     DataInfo.Row row = null;
     DataInfo.Row[] rows = null;
@@ -148,6 +150,7 @@ public abstract class FrameTask<T extends FrameTask<T>> extends MRTask<T>{
 
     final int miniBatchSize = getMiniBatchSize();
     long num_processed_rows = 0;
+    long num_skipped_rows = 0;
     int miniBatchCounter = 0;
     for(int rep = 0; rep < repeats; ++rep) {
       for(int row_idx = 0; row_idx < nrows; ++row_idx){
@@ -175,7 +178,10 @@ public abstract class FrameTask<T extends FrameTask<T>> extends MRTask<T>{
         assert(r >= 0 && r<=nrows);
 
         row = _sparse ? rows[r] : _dinfo.extractDenseRow(chunks, r, row);
-        if(!row.bad) {
+        if(row.bad || row.weight == 0) {
+          num_skipped_rows++;
+          continue;
+        } else {
           assert(row.weight > 0); //check that we never process a row that was held out via row.weight = 0
           long seed = offset + rep * nrows + r;
           miniBatchCounter++;
@@ -194,7 +200,7 @@ public abstract class FrameTask<T extends FrameTask<T>> extends MRTask<T>{
     if (miniBatchCounter>0)
       applyMiniBatchUpdate(miniBatchCounter); //finish up the last piece
 
-    assert(fraction != 1 || num_processed_rows == repeats * nrows);
+    assert(fraction != 1 || num_processed_rows + num_skipped_rows == repeats * nrows);
     chunkDone(num_processed_rows);
   }
 
diff --git a/h2o-algos/src/main/java/hex/coxph/CoxPH.java b/h2o-algos/src/main/java/hex/coxph/CoxPH.java
index 4dd25d9..425af31 100644
--- a/h2o-algos/src/main/java/hex/coxph/CoxPH.java
+++ b/h2o-algos/src/main/java/hex/coxph/CoxPH.java
@@ -20,7 +20,6 @@ public class CoxPH extends ModelBuilder<CoxPHModel,CoxPHModel.CoxPHParameters,Co
   @Override public BuilderVisibility builderVisibility() { return BuilderVisibility.Experimental; }
   public CoxPH( CoxPHModel.CoxPHParameters parms ) { super(parms); init(false); }
   @Override protected CoxPHDriver trainModelImpl() { return new CoxPHDriver(); }
-  @Override public long progressUnits() { return _parms.iter_max; }
 
   /** Initialize the ModelBuilder, validating all arguments and preparing the
    *  training frame.  This call is expected to be overridden in the subclasses
diff --git a/h2o-algos/src/main/java/hex/coxph/CoxPHModel.java b/h2o-algos/src/main/java/hex/coxph/CoxPHModel.java
index d6c2883..96c54a0 100644
--- a/h2o-algos/src/main/java/hex/coxph/CoxPHModel.java
+++ b/h2o-algos/src/main/java/hex/coxph/CoxPHModel.java
@@ -24,6 +24,7 @@ public class CoxPHModel extends Model<CoxPHModel,CoxPHParameters,CoxPHOutput> {
     public String algoName() { return "CoxPH"; }
     public String fullName() { return "Cox Proportional Hazards"; }
     public String javaName() { return CoxPHModel.class.getName(); }
+    @Override public long progressUnits() { return iter_max; }
     // get destination_key  from SupervisedModel.SupervisedParameters from Model.Parameters
     // get training_frame   from SupervisedModel.SupervisedParameters from Model.Parameters
     // get validation_frame from SupervisedModel.SupervisedParameters from Model.Parameters
diff --git a/h2o-algos/src/main/java/hex/deeplearning/DeepLearning.java b/h2o-algos/src/main/java/hex/deeplearning/DeepLearning.java
index 1215534..3daf313 100755
--- a/h2o-algos/src/main/java/hex/deeplearning/DeepLearning.java
+++ b/h2o-algos/src/main/java/hex/deeplearning/DeepLearning.java
@@ -2,6 +2,8 @@ package hex.deeplearning;
 
 import hex.*;
 import hex.deeplearning.DeepLearningModel.DeepLearningParameters;
+import hex.deeplearning.DeepLearningModel.DeepLearningParameters.MissingValuesHandling;
+import hex.glm.GLMTask;
 import water.*;
 import water.exceptions.H2OIllegalArgumentException;
 import water.exceptions.H2OModelBuilderIllegalArgumentException;
@@ -47,14 +49,6 @@ public class DeepLearning extends ModelBuilder<DeepLearningModel,DeepLearningMod
 
   @Override protected DeepLearningDriver trainModelImpl() { return new DeepLearningDriver(); }
 
-  @Override
-  public long progressUnits() {
-    long work = 1;
-    if (null != _train)
-      work = (long)(_parms._epochs * _train.numRows());
-    return Math.max(1, work);
-  }
-
   /** Initialize the ModelBuilder, validating all arguments and preparing the
    *  training frame.  This call is expected to be overridden in the subclasses
    *  and each subclass will start with "super.init();".  This call is made
@@ -73,12 +67,13 @@ public class DeepLearning extends ModelBuilder<DeepLearningModel,DeepLearningMod
    * @param train Training frame
    * @param valid Validation frame
    * @param parms Model parameters
+   * @param nClasses Number of response levels (1: regression, >=2: classification)
    * @return DataInfo
    */
-  static DataInfo makeDataInfo(Frame train, Frame valid, DeepLearningParameters parms) {
+  static DataInfo makeDataInfo(Frame train, Frame valid, DeepLearningParameters parms, int nClasses) {
     double x = 0.782347234;
-    boolean identityLink = new Distribution(parms._distribution, parms._tweedie_power).link(x) == x;
-    return new DataInfo(
+    boolean identityLink = new Distribution(parms).link(x) == x;
+    DataInfo dinfo = new DataInfo(
             train,
             valid,
             parms._autoencoder ? 0 : 1, //nResponses
@@ -91,7 +86,22 @@ public class DeepLearning extends ModelBuilder<DeepLearningModel,DeepLearningMod
             parms._weights_column != null, // observation weights
             parms._offset_column != null,
             parms._fold_column != null
-      );
+    );
+    // Checks and adjustments:
+    // 1) observation weights (adjust mean/sigmas for predictors and response)
+    // 2) NAs (check that there's enough rows left)
+    GLMTask.YMUTask ymt = new GLMTask.YMUTask(dinfo, nClasses, true, !parms._autoencoder && nClasses == 1, parms._missing_values_handling == MissingValuesHandling.Skip, !parms._autoencoder).doAll(dinfo._adaptedFrame);
+    if (ymt._wsum == 0 && parms._missing_values_handling == DeepLearningParameters.MissingValuesHandling.Skip)
+      throw new H2OIllegalArgumentException("No rows left in the dataset after filtering out rows with missing values. Ignore columns with many NAs or set missing_values_handling to 'MeanImputation'.");
+    if (parms._weights_column != null && parms._offset_column != null) {
+      Log.warn("Combination of offset and weights can lead to slight differences because Rollupstats aren't weighted - need to re-calculate weighted mean/sigma of the response including offset terms.");
+    }
+    if (parms._weights_column != null && parms._offset_column == null /*FIXME: offset not yet implemented*/) {
+      dinfo.updateWeightedSigmaAndMean(ymt._basicStats.sigma(), ymt._basicStats.mean());
+      if (nClasses == 1)
+        dinfo.updateWeightedSigmaAndMeanForResponse(ymt._basicStatsResponse.sigma(), ymt._basicStatsResponse.mean());
+    }
+    return dinfo;
   }
 
   @Override protected void checkMemoryFootPrint() {
@@ -237,8 +247,7 @@ public class DeepLearning extends ModelBuilder<DeepLearningModel,DeepLearningMod
           // This can add or remove dummy columns (can happen if the dataset is sparse and datasets have different non-const columns)
           for (String st : previous.adaptTestForTrain(_train,true,false)) Log.warn(st);
           for (String st : previous.adaptTestForTrain(_valid,true,false)) Log.warn(st);
-
-          dinfo = makeDataInfo(_train, _valid, _parms);
+          dinfo = makeDataInfo(_train, _valid, _parms, nclasses());
           DKV.put(dinfo);
           cp = new DeepLearningModel(dest(), _parms, previous, false, dinfo);
           cp.write_lock(_job);
@@ -343,6 +352,11 @@ public class DeepLearning extends ModelBuilder<DeepLearningModel,DeepLearningMod
           model._output._modelClassDist = _weights != null ? cd.doAll(l, w).rel_dist() : cd.doAll(l).rel_dist();
         }
         model.training_rows = train.numRows();
+        if (_weights != null && _weights.min()==0 && _weights.max()==1 && _weights.isInt()) {
+          model.training_rows = Math.round(train.numRows()*_weights.mean());
+          Log.warn("Not counting " + (train.numRows() - model.training_rows) + " rows with weight=0 towards an epoch.");
+        }
+        Log.info("One epoch corresponds to " + model.training_rows + " training data rows.");
         trainScoreFrame = sampleFrame(train, mp._score_training_samples, mp._seed); //training scoring dataset is always sampled uniformly from the training dataset
         if( trainScoreFrame != train ) Scope.track(trainScoreFrame);
 
@@ -363,14 +377,14 @@ public class DeepLearning extends ModelBuilder<DeepLearningModel,DeepLearningMod
         }
 
         // Set train_samples_per_iteration size (cannot be done earlier since this depends on whether stratified sampling is done)
-        model.actual_train_samples_per_iteration = computeTrainSamplesPerIteration(mp, train.numRows(), model);
+        model.actual_train_samples_per_iteration = computeTrainSamplesPerIteration(mp, model.training_rows, model);
         // Determine whether shuffling is enforced
-        if(mp._replicate_training_data && (model.actual_train_samples_per_iteration == train.numRows()*(mp._single_node_mode ?1:H2O.CLOUD.size())) && !mp._shuffle_training_data && H2O.CLOUD.size() > 1 && !mp._reproducible) {
+        if(mp._replicate_training_data && (model.actual_train_samples_per_iteration == model.training_rows*(mp._single_node_mode ?1:H2O.CLOUD.size())) && !mp._shuffle_training_data && H2O.CLOUD.size() > 1 && !mp._reproducible) {
           if (!mp._quiet_mode)
             Log.info("Enabling training data shuffling, because all nodes train on the full dataset (replicated training data).");
           mp._shuffle_training_data = true;
         }
-        if(!mp._shuffle_training_data && model.actual_train_samples_per_iteration == train.numRows() && train.anyVec().nChunks()==1) {
+        if(!mp._shuffle_training_data && model.actual_train_samples_per_iteration == model.training_rows && train.anyVec().nChunks()==1) {
           if (!mp._quiet_mode)
             Log.info("Enabling training data shuffling to avoid training rows in the same order over and over (no Hogwild since there's only 1 chunk).");
           mp._shuffle_training_data = true;
diff --git a/h2o-algos/src/main/java/hex/deeplearning/DeepLearningModel.java b/h2o-algos/src/main/java/hex/deeplearning/DeepLearningModel.java
index 5f1341f..36cc971 100755
--- a/h2o-algos/src/main/java/hex/deeplearning/DeepLearningModel.java
+++ b/h2o-algos/src/main/java/hex/deeplearning/DeepLearningModel.java
@@ -80,7 +80,7 @@ public class DeepLearningModel extends Model<DeepLearningModel,DeepLearningModel
   public double deviance(double w, double y, double f) {
     // Note: Must use sanitized parameters via get_params() as this._params can still have defaults AUTO, etc.)
     assert(get_params()._distribution != Distribution.Family.AUTO);
-    return new Distribution(get_params()._distribution, get_params()._tweedie_power).deviance(w,y,f);
+    return new Distribution(get_params()).deviance(w,y,f);
   }
 
   // Default publicly visible Schema is V2
@@ -437,7 +437,7 @@ public class DeepLearningModel extends Model<DeepLearningModel,DeepLearningModel
    */
   public DeepLearningModel(final Key destKey, final DeepLearningParameters parms, final DeepLearningModelOutput output, Frame train, Frame valid, int nClasses) {
     super(destKey, parms, output);
-    final DataInfo dinfo = makeDataInfo(train, valid, _parms);
+    final DataInfo dinfo = makeDataInfo(train, valid, _parms, nClasses);
     _output._names  = train._names   ; // Since changed by DataInfo, need to be reflected in the Model output as well
     _output._domains= train.domains();
     _output._names = dinfo._adaptedFrame.names();
@@ -788,7 +788,7 @@ public class DeepLearningModel extends Model<DeepLearningModel,DeepLearningModel
     double loss = 0;
     Neurons[] neurons = DeepLearningTask.makeNeuronsForTraining(model_info());
     //for absolute error, gradient -1/1 matches the derivative of abs(x) without correction term
-    final double prefactor = _parms._distribution == Distribution.Family.laplace ? 1 : 0.5;
+    final double prefactor = _parms._distribution == Distribution.Family.laplace || _parms._distribution == Distribution.Family.quantile ? 1 : 0.5;
     for (DataInfo.Row myRow : myRows) {
       if (myRow == null) continue;
       long seed = -1; //ignored
@@ -826,7 +826,7 @@ public class DeepLearningModel extends Model<DeepLearningModel,DeepLearningModel
 //        pred = (pred / di._normRespMul[0] + di._normRespSub[0]);
 //        actual = (actual / di._normRespMul[0] + di._normRespSub[0]);
 //      }
-        Distribution dist = new Distribution(model_info.get_params()._distribution, model_info.get_params()._tweedie_power);
+        Distribution dist = new Distribution(model_info.get_params());
         pred = dist.linkInv(pred);
         loss += prefactor * dist.deviance(1 /*weight*/, actual, pred);
       }
@@ -879,8 +879,9 @@ public class DeepLearningModel extends Model<DeepLearningModel,DeepLearningModel
       else
         preds[0] = out[0];
       // transform prediction to response space
-      preds[0] = new Distribution(model_info.get_params()._distribution, model_info.get_params()._tweedie_power).linkInv(preds[0]);
-      if (Double.isNaN(preds[0])) throw new RuntimeException("Predicted regression target NaN!");
+      preds[0] = new Distribution(model_info.get_params()).linkInv(preds[0]);
+      if (Double.isNaN(preds[0]))
+        throw new RuntimeException("Predicted regression target NaN!");
     }
     return preds;
   }
@@ -1396,7 +1397,7 @@ public class DeepLearningModel extends Model<DeepLearningModel,DeepLearningModel
       else {
         bodySb.i(2).p("preds[1] = ACTIVATION[i][0];").nl();
       }
-      bodySb.i(2).p("preds[1] = " + new Distribution(model_info.get_params()._distribution, model_info.get_params()._tweedie_power).linkInvString("preds[1]")+";").nl();
+      bodySb.i(2).p("preds[1] = " + new Distribution(model_info.get_params()).linkInvString("preds[1]")+";").nl();
       bodySb.i(2).p("if (Double.isNaN(preds[1])) throw new RuntimeException(\"Predicted regression target NaN!\");").nl();
       bodySb.i(1).p("}").nl();
       bodySb.i().p("}").nl();
@@ -1455,7 +1456,11 @@ public class DeepLearningModel extends Model<DeepLearningModel,DeepLearningModel
       super();
       _stopping_rounds = 5;
     }
-  
+    @Override
+    public long progressUnits() {
+      if (train()==null) return 1;
+      return (long)Math.ceil(_epochs*train().numRows());
+    }
     @Override
     public double missingColumnsType() {
       return _sparse ? 0 : Double.NaN;
@@ -1860,7 +1865,7 @@ public class DeepLearningModel extends Model<DeepLearningModel,DeepLearningModel
      * Absolute, Quadratic, Huber or CrossEntropy for classification
      */
     public enum Loss {
-      Automatic, Quadratic, CrossEntropy, Huber, Absolute
+      Automatic, Quadratic, CrossEntropy, Huber, Absolute, Quantile
     }
   
     /**
@@ -1966,6 +1971,7 @@ public class DeepLearningModel extends Model<DeepLearningModel,DeepLearningModel
           case gaussian:
           case huber:
           case laplace:
+          case quantile:
           case tweedie:
           case gamma:
           case poisson:
@@ -1994,6 +2000,10 @@ public class DeepLearningModel extends Model<DeepLearningModel,DeepLearningModel
             if (_loss != Loss.Absolute && _loss != Loss.Automatic)
               dl.error("_distribution", "Only Automatic or Absolute loss is allowed for " + _distribution + " distribution.");
             break;
+          case quantile:
+            if (_loss != Loss.Quantile && _loss != Loss.Automatic)
+              dl.error("_distribution", "Only Automatic or Quantile loss is allowed for " + _distribution + " distribution.");
+            break;
           case huber:
             if (_loss != Loss.Huber && _loss != Loss.Automatic)
               dl.error("_distribution", "Only Automatic or Huber loss is allowed for " + _distribution + " distribution.");
@@ -2136,6 +2146,7 @@ public class DeepLearningModel extends Model<DeepLearningModel,DeepLearningModel
               "_max_categorical_features",
               "_nfolds",
               "_distribution",
+              "_quantile_alpha",
               "_tweedie_power"
       };
   
@@ -2147,6 +2158,7 @@ public class DeepLearningModel extends Model<DeepLearningModel,DeepLearningModel
                   ) {
             if (f.getName().equals("_hidden")) continue;
             if (f.getName().equals("_ignored_columns")) continue;
+	    if (f.getName().equals("$jacocoData")) continue; // If code coverage is enabled
             throw H2O.unimpl("Please add " + f.getName() + " to either cp_modifiable or cp_not_modifiable");
           }
       }
@@ -2319,6 +2331,9 @@ public class DeepLearningModel extends Model<DeepLearningModel,DeepLearningModel
               case Absolute:
                 toParms._distribution = Distribution.Family.laplace;
                 break;
+              case Quantile:
+                toParms._distribution = Distribution.Family.quantile;
+                break;
               case Huber:
                 toParms._distribution = Distribution.Family.huber;
                 break;
@@ -2333,6 +2348,9 @@ public class DeepLearningModel extends Model<DeepLearningModel,DeepLearningModel
             case gaussian:
               toParms._loss = Loss.Quadratic;
               break;
+            case quantile:
+              toParms._loss = Loss.Quantile;
+              break;
             case laplace:
               toParms._loss = Loss.Absolute;
               break;
diff --git a/h2o-algos/src/main/java/hex/deeplearning/DeepLearningModelInfo.java b/h2o-algos/src/main/java/hex/deeplearning/DeepLearningModelInfo.java
index e160495..4bcb765 100644
--- a/h2o-algos/src/main/java/hex/deeplearning/DeepLearningModelInfo.java
+++ b/h2o-algos/src/main/java/hex/deeplearning/DeepLearningModelInfo.java
@@ -60,7 +60,7 @@ final public class DeepLearningModelInfo extends Iced {
     if (cats == null) return;
     if (_saw_missing_cats == null) return;
     for (int i=0; i<cats.length; ++i) {
-      assert(data_info._catMissing[i] == 1); //have a missing bucket for each categorical
+      assert(data_info._catMissing[i]); //have a missing bucket for each categorical
       if (_saw_missing_cats[i]) continue;
       _saw_missing_cats[i] = (cats[i] == data_info._catOffsets[i+1]-1);
     }
@@ -589,7 +589,7 @@ final public class DeepLearningModelInfo extends Iced {
     // zero out missing categorical variables if they were never seen
     if (_saw_missing_cats != null) {
       for (int i = 0; i < _saw_missing_cats.length; ++i) {
-        assert (data_info._catMissing[i] == 1); //have a missing bucket for each categorical
+        assert (data_info._catMissing[i]); //have a missing bucket for each categorical
         if (!_saw_missing_cats[i]) vi[data_info._catOffsets[i + 1] - 1] = 0;
       }
     }
diff --git a/h2o-algos/src/main/java/hex/deeplearning/Neurons.java b/h2o-algos/src/main/java/hex/deeplearning/Neurons.java
index abda351..2a72037 100644
--- a/h2o-algos/src/main/java/hex/deeplearning/Neurons.java
+++ b/h2o-algos/src/main/java/hex/deeplearning/Neurons.java
@@ -134,7 +134,7 @@ public abstract class Neurons {
     params._hidden_dropout_ratios = minfo.get_params()._hidden_dropout_ratios;
     params._rate *= Math.pow(params._rate_decay, index-1);
     params._distribution = minfo.get_params()._distribution;
-    _dist = new Distribution(params._distribution, params._tweedie_power);
+    _dist = new Distribution(params);
     _a = new Storage.DenseVector(units);
     if (!(this instanceof Input)) {
       _e = new Storage.DenseVector(units);
@@ -481,7 +481,7 @@ public abstract class Neurons {
       int    [] cats = MemoryManager.malloc4(_dinfo._cats); // a bit wasteful - reallocated each time
       int i = 0, ncats = 0;
       for(; i < _dinfo._cats; ++i){
-        assert(_dinfo._catMissing[i] != 0); //we now *always* have a categorical level for NAs, just in case.
+        assert(_dinfo._catMissing[i]); //we now *always* have a categorical level for NAs, just in case.
         if (Double.isNaN(data[i])) {
           cats[ncats] = (_dinfo._catOffsets[i+1]-1); //use the extra level for NAs made during training
         } else {
@@ -499,8 +499,7 @@ public abstract class Neurons {
         }
         ncats++;
       }
-      final int n = data.length - (_dinfo._weights ? 1 : 0) - (_dinfo._offset ? 1 : 0);
-      for(;i < n;++i){
+      for(;i < data.length;++i){
         double d = data[i];
         if(_dinfo._normMul != null) d = (d - _dinfo._normSub[i-_dinfo._cats])*_dinfo._normMul[i-_dinfo._cats];
         nums[i-_dinfo._cats] = d; //can be NaN for missing numerical data
diff --git a/h2o-algos/src/main/java/hex/example/Example.java b/h2o-algos/src/main/java/hex/example/Example.java
index baa8762..57d0801 100644
--- a/h2o-algos/src/main/java/hex/example/Example.java
+++ b/h2o-algos/src/main/java/hex/example/Example.java
@@ -20,7 +20,6 @@ public class Example extends ModelBuilder<ExampleModel,ExampleParameters,Example
   // Called from Nano thread; start the Example Job on a F/J thread
   public Example( ExampleModel.ExampleParameters parms ) { super(parms); init(false); }
   @Override protected ExampleDriver trainModelImpl() { return new ExampleDriver(); }
-  @Override public long progressUnits() { return _parms._max_iterations; }
 
   /** Initialize the ModelBuilder, validating all arguments and preparing the
    *  training frame.  This call is expected to be overridden in the subclasses
diff --git a/h2o-algos/src/main/java/hex/example/ExampleModel.java b/h2o-algos/src/main/java/hex/example/ExampleModel.java
index f066ef4..b1d40e3 100644
--- a/h2o-algos/src/main/java/hex/example/ExampleModel.java
+++ b/h2o-algos/src/main/java/hex/example/ExampleModel.java
@@ -12,6 +12,7 @@ public class ExampleModel extends Model<ExampleModel,ExampleModel.ExampleParamet
     public String algoName() { return "Example"; }
     public String fullName() { return "Example"; }
     public String javaName() { return ExampleModel.class.getName(); }
+    @Override public long progressUnits() { return _max_iterations; }
     public int _max_iterations = 1000; // Max iterations
   }
 
diff --git a/h2o-algos/src/main/java/hex/glm/ComputationState.java b/h2o-algos/src/main/java/hex/glm/ComputationState.java
index 34a49bd..49cfac1 100644
--- a/h2o-algos/src/main/java/hex/glm/ComputationState.java
+++ b/h2o-algos/src/main/java/hex/glm/ComputationState.java
@@ -13,6 +13,7 @@ import water.H2O;
 import water.Key;
 import water.MemoryManager;
 import water.util.ArrayUtils;
+import water.util.Log;
 import water.util.MathUtils;
 
 import java.text.DecimalFormat;
@@ -119,8 +120,8 @@ public final class ComputationState {
     int selected = 0;
     _activeBC = _bc;
     _activeData = _activeData != null?_activeData:_dinfo;
-    if (!_allIn && _alpha > 0) {
-      final double rhs = _alpha * (2 * _lambda - _previousLambda);
+    if (!_allIn) {
+      final double rhs = Math.abs(_alpha * (2 * _lambda - _previousLambda));
       int [] cols = MemoryManager.malloc4(P);
       int j = 0;
       int[] oldActiveCols = _activeData._activeCols == null ? new int[0] : _activeData.activeCols();
@@ -128,25 +129,27 @@ public final class ComputationState {
         if (j < oldActiveCols.length && i == oldActiveCols[j]) {
           cols[selected++] = i;
           ++j;
-        } else if (_ginfo._gradient[i] > rhs || _ginfo._gradient[i] < -rhs) {
+        } else if (_ginfo._gradient[i] > rhs || -_ginfo._gradient[i] > rhs) {
           cols[selected++] = i;
         }
       }
-      _allIn = _alpha == 0 || selected == P;
+      _allIn = selected == P;
       if(!_allIn) {
-        if (_intercept) cols[selected++] = P;
+        cols[selected++] = P; // intercept is always selected, even if it is false (it's gonna be dropped later, it is needed for other stuff too)
         cols = Arrays.copyOf(cols, selected);
         double [] b = ArrayUtils.select(_beta, cols);
         assert Arrays.equals(_beta,ArrayUtils.expandAndScatter(b,_dinfo.fullN()+1,cols));
         _beta = b;
-        _ginfo = new GLMGradientInfo(_ginfo._likelihood, _ginfo._objVal, ArrayUtils.select(_ginfo._gradient, cols));
         _activeData = _dinfo.filterExpandedColumns(Arrays.copyOf(cols, selected));
+        _ginfo = new GLMGradientInfo(_ginfo._likelihood, _ginfo._objVal, ArrayUtils.select(_ginfo._gradient, cols));
         _activeBC = _bc.filterExpandedColumns(_activeData.activeCols());
         _gslvr = new GLMGradientSolver(_jobKey,_parms,_activeData,(1-_alpha)*_lambda,_bc);
         assert _beta.length == selected;
-      } else _activeData = _dinfo;
+        return selected;
+      }
     }
-    return selected;
+    _activeData = _dinfo;
+    return _dinfo.fullN();
   }
 
   private DataInfo [] _activeDataMultinomial;
@@ -222,7 +225,7 @@ public final class ComputationState {
     int selected = 0;
     _activeBC = _bc;
     _activeData = _dinfo;
-    if (!_allIn && _alpha > 0) {
+    if (!_allIn) {
       if(_activeDataMultinomial == null)
         _activeDataMultinomial = new DataInfo[_nclasses];
       final double rhs = _alpha * (2 * _lambda - _previousLambda);
@@ -245,7 +248,7 @@ public final class ComputationState {
         for(int i = start; i < selected; ++i)
           cols[i] += c*N;
       }
-      _allIn = _alpha == 0 || selected == cols.length;
+      _allIn = selected == cols.length;
     }
     return selected;
   }
@@ -260,7 +263,7 @@ public final class ComputationState {
       return checkKKTsMultinomial();
     double [] beta = _beta;
     if(_activeData._activeCols != null)
-      beta = ArrayUtils.expandAndScatter(beta,_dinfo.fullN() + (_intercept?1:0),_activeData._activeCols);
+      beta = ArrayUtils.expandAndScatter(beta,_dinfo.fullN() + 1,_activeData._activeCols);
     int [] activeCols = _activeData.activeCols();
     _gslvr = new GLMGradientSolver(_jobKey,_parms,_dinfo,(1-_alpha)*_lambda,_bc);
     GLMGradientInfo ginfo = _gslvr.getGradient(beta);
@@ -274,7 +277,7 @@ public final class ComputationState {
     _beta = beta;
     _ginfo = ginfo;
     _activeBC = null;
-    if(!_allIn && _lambda*_alpha > 0) {
+    if(!_allIn) {
       int[] failedCols = new int[64];
       int fcnt = 0;
       for (int i = 0; i < grad.length - 1; ++i) {
@@ -286,6 +289,7 @@ public final class ComputationState {
         }
       }
       if (fcnt > 0) {
+        Log.info(fcnt + " variables failed KKT conditions, adding them to the model and recomputing.");
         final int n = activeCols.length;
         int[] newCols = Arrays.copyOf(activeCols, activeCols.length + fcnt);
         for (int i = 0; i < fcnt; ++i)
@@ -302,12 +306,13 @@ public final class ComputationState {
     return true;
   }
   public int []  removeCols(int [] cols) {
-    int [] activeCols = ArrayUtils.removeSorted(_activeData.activeCols(),cols);
+    int [] activeCols = ArrayUtils.removeIds(_activeData.activeCols(),cols);
     if(_beta != null)
-      _beta = ArrayUtils.select(_beta,activeCols);
+      _beta = ArrayUtils.removeIds(_beta,cols);
     if(_ginfo != null)
-      _ginfo._gradient = ArrayUtils.select(_ginfo._gradient,activeCols);
-    _activeData = _activeData.filterExpandedColumns(activeCols);
+      _ginfo._gradient = ArrayUtils.removeIds(_ginfo._gradient,cols);
+    _activeData = _dinfo.filterExpandedColumns(activeCols);
+    _activeBC = _bc.filterExpandedColumns(activeCols);
     _gslvr = new GLMGradientSolver(_jobKey, _parms, _activeData, (1 - _alpha) * _lambda, _activeBC);
     return activeCols;
   }
diff --git a/h2o-algos/src/main/java/hex/glm/GLM.java b/h2o-algos/src/main/java/hex/glm/GLM.java
index dd358b6..80b5ebb 100644
--- a/h2o-algos/src/main/java/hex/glm/GLM.java
+++ b/h2o-algos/src/main/java/hex/glm/GLM.java
@@ -302,7 +302,7 @@ public class GLM extends ModelBuilder<GLMModel,GLMParameters,GLMOutput> {
         _parms._max_active_predictors = _parms._solver == Solver.IRLSM ? 7000 : 100000000;
       if (_parms._link == Link.family_default)
         _parms._link = _parms._family.defaultLink;
-      _dinfo = new DataInfo(_train.clone(), _valid, 1, _parms._use_all_factor_levels || _parms._lambda_search, _parms._standardize ? DataInfo.TransformType.STANDARDIZE : DataInfo.TransformType.NONE, DataInfo.TransformType.NONE, _parms._missing_values_handling == MissingValuesHandling.Skip, _parms._missing_values_handling == MissingValuesHandling.MeanImputation, false, hasWeightCol(), hasOffsetCol(), hasFoldCol());
+      _dinfo = new DataInfo(_train.clone(), _valid, 1, _parms._use_all_factor_levels || _parms._lambda_search, _parms._standardize ? DataInfo.TransformType.STANDARDIZE : DataInfo.TransformType.NONE, DataInfo.TransformType.NONE, _parms._missing_values_handling == MissingValuesHandling.Skip, false ,_parms._missing_values_handling == MissingValuesHandling.MeanImputation, hasWeightCol(), hasOffsetCol(), hasFoldCol());
       checkMemoryFootPrint(_dinfo);
       if (_parms._max_iterations == -1) { // fill in default max iterations
         int numclasses = _parms._family == Family.multinomial?nclasses():1;
@@ -315,6 +315,9 @@ public class GLM extends ModelBuilder<GLMModel,GLMParameters,GLMOutput> {
         }
       }
       BetaConstraint bc = (_parms._beta_constraints != null)?new BetaConstraint(_parms._beta_constraints.get()):new BetaConstraint();
+      if((bc.hasBounds() || bc.hasProximalPenalty()) && _parms._compute_p_values)
+        error("_compute_p_values","P-values can not be computed for constrained problems");
+
       if (_valid != null)
         _validDinfo = _dinfo.validDinfo(_valid);
       _state = new ComputationState(_job._key, _parms, _dinfo, bc, nclasses());
@@ -326,7 +329,7 @@ public class GLM extends ModelBuilder<GLMModel,GLMParameters,GLMOutput> {
           Vec wc = _weights == null ? _dinfo._adaptedFrame.anyVec().makeCon(1) : _weights.makeCopy();
           _dinfo.setWeights(_generatedWeights = "__glm_gen_weights", wc);
         }
-        YMUTask ymt = new YMUTask(_dinfo, _parms._family == Family.multinomial?nclasses():1, !_parms._stdOverride, setWeights, skippingRows).doAll(_dinfo._adaptedFrame);
+        YMUTask ymt = new YMUTask(_dinfo, _parms._family == Family.multinomial?nclasses():1, !_parms._stdOverride, setWeights, skippingRows,true).doAll(_dinfo._adaptedFrame);
         if (ymt._wsum == 0)
           throw new IllegalArgumentException("No rows left in the dataset after filtering out rows with missing values. Ignore columns with many NAs or impute your missing values prior to calling glm.");
         Log.info(LogMsg("using " + ymt._nobs + " nobs out of " + _dinfo._adaptedFrame.numRows() + " total"));
@@ -396,12 +399,10 @@ public class GLM extends ModelBuilder<GLMModel,GLMParameters,GLMOutput> {
     }
   }
 
-  private static final long WORK_TOTAL = 1000000;
+  protected static final long WORK_TOTAL = 1000000;
 
   @Override protected GLMDriver trainModelImpl() { return new GLMDriver(); }
 
-  @Override public long progressUnits() { return WORK_TOTAL; }
-
   private final double lmax(double[] grad) {
     return Math.max(ArrayUtils.maxValue(grad), -ArrayUtils.minValue(grad)) / Math.max(1e-2, _parms._alpha[0]);
   }
@@ -435,11 +436,20 @@ public class GLM extends ModelBuilder<GLMModel,GLMParameters,GLMOutput> {
         gram.dropIntercept();
         xy = Arrays.copyOf(xy, xy.length - 1);
       }
+      int [] zeros = gram.dropZeroCols();
+
+      assert zeros.length == 0:"zero column(s) in gram matrix";
       gram.mul(_parms._obj_reg);
       ArrayUtils.mult(xy, _parms._obj_reg);
       if(_parms._remove_collinear_columns || _parms._compute_p_values) {
         ArrayList<Integer> ignoredCols = new ArrayList<>();
-        Cholesky chol = ((_state._iter == 0 && _parms._remove_collinear_columns)?gram.qrCholesky(ignoredCols):gram.cholesky(null));
+        Cholesky chol = ((_state._iter == 0)?gram.qrCholesky(ignoredCols):gram.cholesky(null));
+        if(!ignoredCols.isEmpty() && !_parms._remove_collinear_columns) {
+          int [] collinear_cols = new int[ignoredCols.size()];
+          for(int i = 0; i < collinear_cols.length; ++i)
+            collinear_cols[i] = ignoredCols.get(i);
+          throw new NonSPDMatrixException("Found collinear columns in the dataset. Can not compute compute p-values without removing them, set remove_collinear_columns flag to true. Found collinear columns " + Arrays.toString(ArrayUtils.select(_dinfo.coefNames(),collinear_cols)));
+        }
         if(!chol.isSPD()) throw new NonSPDMatrixException();
         _chol = chol;
         if(!ignoredCols.isEmpty()) { // got some redundant cols
@@ -450,7 +460,8 @@ public class GLM extends ModelBuilder<GLMModel,GLMParameters,GLMOutput> {
           // need to drop the cols from everywhere
           _model.addWarning("Removed collinear columns " + Arrays.toString(collinear_col_names));
           Log.warn("Removed collinear columns " + Arrays.toString(collinear_col_names));
-          xy = ArrayUtils.select(xy,_state.removeCols(collinear_cols));
+          _state.removeCols(collinear_cols);
+          xy = ArrayUtils.removeIds(xy,collinear_cols);
         }
         chol.solve(xy);
       } else { // todo add switch between COD and ADMM
@@ -835,6 +846,9 @@ public class GLM extends ModelBuilder<GLMModel,GLMParameters,GLMOutput> {
       _keys2Keep.put("dest",dest());
       _parms.read_lock_frames(_job);
       init(true);
+      if (error_count() > 0) {
+        throw H2OModelBuilderIllegalArgumentException.makeFromBuilder(GLM.this);
+      }
       double nullDevTrain = Double.NaN;
       double nullDevTest = Double.NaN;
       if(_parms._lambda_search) {
@@ -848,9 +862,7 @@ public class GLM extends ModelBuilder<GLMModel,GLMParameters,GLMOutput> {
         _workPerIteration = WORK_TOTAL/_parms._nlambdas;
       } else
         _workPerIteration = 1 + (WORK_TOTAL/_parms._max_iterations);
-      if (error_count() > 0) {
-        throw H2OModelBuilderIllegalArgumentException.makeFromBuilder(GLM.this);
-      }
+
       if(_parms._family == Family.multinomial && _parms._solver == Solver.IRLSM) {
         double [] nb = getNullBeta();
         double maxRow = ArrayUtils.maxValue(nb);
@@ -866,12 +878,14 @@ public class GLM extends ModelBuilder<GLMModel,GLMParameters,GLMOutput> {
       for (int i = 0; i < _parms._lambda.length; ++i) { // lambda search
         _model.addSubmodel(_state.beta(),_parms._lambda[i],_state._iter);
         _state.setLambda(_parms._lambda[i]);
-        if(_parms._family == Family.multinomial)
-          for(int c = 0; c < _nclass; ++c)
-            Log.info(LogMsg("Class " + c + " got " + _state.activeDataMultinomial(c).fullN() + " active columns out of " + _state._dinfo.fullN() + " total"));
-        else
-          Log.info(LogMsg("Got " + _state.activeData().fullN() + " active columns out of " + _state._dinfo.fullN() + " total"));
-        do { fitModel(); } while(!_state.checkKKTs());
+        do {
+          if(_parms._family == Family.multinomial)
+            for(int c = 0; c < _nclass; ++c)
+              Log.info(LogMsg("Class " + c + " got " + _state.activeDataMultinomial(c).fullN() + " active columns out of " + _state._dinfo.fullN() + " total"));
+          else
+            Log.info(LogMsg("Got " + _state.activeData().fullN() + " active columns out of " + _state._dinfo.fullN() + " total"));
+          fitModel();
+        } while(!_state.checkKKTs());
         Log.info(LogMsg("solution has " + ArrayUtils.countNonzeros(_state.beta()) + " nonzeros"));
         if(_parms._lambda_search) {  // compute train and test dev
           double trainDev = _parms._family == Family.multinomial
@@ -1552,6 +1566,10 @@ public class GLM extends ModelBuilder<GLMModel,GLMParameters,GLMOutput> {
       return false;
     }
 
+    public boolean hasProximalPenalty() {
+      return _betaGiven != null && _rho != null && ArrayUtils.countNonzeros(_rho) > 0;
+    }
+
     public void adjustGradient(double[] beta, double[] grad) {
       if (_betaGiven != null && _rho != null) {
         for (int i = 0; i < _betaGiven.length; ++i) {
diff --git a/h2o-algos/src/main/java/hex/glm/GLMModel.java b/h2o-algos/src/main/java/hex/glm/GLMModel.java
index a82fb23..8b9578a 100755
--- a/h2o-algos/src/main/java/hex/glm/GLMModel.java
+++ b/h2o-algos/src/main/java/hex/glm/GLMModel.java
@@ -13,7 +13,6 @@ import hex.glm.GLMModel.GLMParameters.Link;
 import org.apache.commons.math3.distribution.NormalDistribution;
 import org.apache.commons.math3.distribution.RealDistribution;
 import org.apache.commons.math3.distribution.TDistribution;
-import water.DKV;
 import water.H2O;
 import water.Iced;
 import water.Key;
@@ -111,6 +110,7 @@ public class GLMModel extends Model<GLMModel,GLMModel.GLMParameters,GLMModel.GLM
     public String algoName() { return "GLM"; }
     public String fullName() { return "Generalized Linear Modeling"; }
     public String javaName() { return GLMModel.class.getName(); }
+    @Override public long progressUnits() { return GLM.WORK_TOTAL; }
     // public int _response; // TODO: the standard is now _response_column in SupervisedModel.SupervisedParameters
     public boolean _standardize = true;
     public Family _family;
@@ -164,7 +164,7 @@ public class GLMModel extends Model<GLMModel,GLMModel.GLMParameters,GLMModel.GLM
       if(_obj_reg != -1 && _obj_reg <= 0)
         glm.error("obj_reg","Must be positive or -1 for default");
       if(_prior != -1 && _prior <= 0 || _prior >= 1)
-        glm.error("_prior","Prior must be in (exlusive) range (0,1)");
+        glm.error("_prior","Prior must be in (exclusive) range (0,1)");
       if(_family != Family.tweedie) {
         glm.hide("_tweedie_variance_power","Only applicable with Tweedie family");
         glm.hide("_tweedie_link_power","Only applicable with Tweedie family");
@@ -945,125 +945,56 @@ public class GLMModel extends Model<GLMModel,GLMModel.GLMParameters,GLMModel.GLM
     return super.checksum_impl();
   }
 
-  private double [] scoreMultinomial(Chunk[] chks, int row_in_chunk, double[] tmp, double[] preds) {
-    double[] eta = MemoryManager.malloc8d(_output.nclasses());
-    final double[][] b = _output._global_beta_multinomial;
-    final int P = b[0].length;
-    int[] catOffs = dinfo()._catOffsets;
-    for (int i = 0; i < catOffs.length - 1; ++i) {
-      if (chks[i].isNA(row_in_chunk)) {
-        Arrays.fill(eta, Double.NaN);
-        break;
+  private double [] scoreRow(Row r, double o, double [] preds) {
+    if(_parms._family == Family.multinomial) {
+      if(_eta == null) _eta = new ThreadLocal<>();
+      double[] eta = _eta.get();
+      if(eta == null) _eta.set(eta = MemoryManager.malloc8d(_output.nclasses()));
+      final double[][] bm = _output._global_beta_multinomial;
+      double sumExp = 0;
+      double maxRow = 0;
+      for (int c = 0; c < bm.length; ++c) {
+        eta[c] = r.innerProduct(bm[c]) + o;
+        if(eta[c] > maxRow)
+          maxRow = eta[c];
       }
-      long lval = chks[i].at8(row_in_chunk);
-      int ival = (int) lval;
-      if (ival != lval) throw new IllegalArgumentException("categorical value out of range");
-      if (!_parms._use_all_factor_levels) --ival;
-      int from = catOffs[i];
-      int to = catOffs[i + 1];
-      // can get values out of bounds for cat levels not seen in training
-      if (ival >= 0 && (ival + from) < catOffs[i + 1])
-        for (int j = 0; j < _output.nclasses(); ++j)
-          eta[j] += b[j][ival + from];
-    }
-    final int noff = dinfo().numStart() - dinfo()._cats;
-    for (int i = dinfo()._cats; i < b.length - 1 - noff; ++i) {
-      double d = chks[i].atd(row_in_chunk);
-      for (int j = 0; j < _output.nclasses(); ++j)
-        eta[j] += b[j][noff + i] * d;
-    }
-    double sumExp = 0;
-    double max_row = 0;
-    for (int j = 0; j < _output.nclasses(); ++j) {
-      eta[j] += b[j][P - 1];
-      if(eta[j] > max_row)
-        max_row = eta[j];
+      for (int c = 0; c < bm.length; ++c)
+        sumExp += eta[c] = Math.exp(eta[c]-maxRow); // intercept
+      sumExp = 1.0 / sumExp;
+      for (int c = 0; c < bm.length; ++c)
+        preds[c + 1] = eta[c] * sumExp;
+      preds[0] = ArrayUtils.maxIndex(eta);
+    } else {
+      double mu = _parms.linkInv(r.innerProduct(beta()) + o);
+      if (_parms._family == Family.binomial) { // threshold for prediction
+        preds[0] = mu >= defaultThreshold()?1:0;
+        preds[1] = 1.0 - mu; // class 0
+        preds[2] = mu; // class 1
+      } else
+        preds[0] = mu;
     }
-    for (int j = 0; j < _output.nclasses(); ++j)
-      sumExp += eta[j] = Math.exp(eta[j]-max_row); // intercept
-    sumExp = 1.0 / sumExp;
-    for (int i = 0; i < eta.length; ++i)
-      preds[i + 1] = eta[i] * sumExp;
-    preds[0] = ArrayUtils.maxIndex(eta);
     return preds;
   }
 
   private transient ThreadLocal<Row> _row;
+  private transient ThreadLocal<double[]> _eta;
 
-  @Override
-  // public double[] score0( Chunk chks[], double weight, double offset, int row_in_chunk, double[] tmp, double[] preds )
-  public double[] score0(Chunk[] chks, double weight, double offset, int row_in_chunk, double[] tmp, double[] preds) {
-    if(_parms._family == Family.multinomial)
-      return scoreMultinomial(chks,row_in_chunk,tmp,preds);
+  private final Row getRow(){
     if(_row == null) _row = new ThreadLocal<>();
     Row r = _row.get();
     if(r == null) _row.set(r = _output._scoringDinfo.newDenseRow());
-    _output._scoringDinfo.extractDenseRow(chks,row_in_chunk,r);
-    double mu = preds[0] = _parms.linkInv(r.innerProduct(beta()) + offset);
-    if( _parms._family == Family.binomial ) { // threshold for prediction
-      preds[1] = 1.0 - mu; // class 0
-      preds[2] =       mu; // class 1
-    }
-    return preds;
+    return r;
   }
 
-  @Override protected double[] score0(double[] data, double[] preds){return score0(data,preds,1,0);}
 
-  private double [] scoreMultinomial(double[] data, double[] preds, double w, double o) {
-    double [] eta = MemoryManager.malloc8d(_output.nclasses());
-    final double [][] b = _output._global_beta_multinomial;
-    final int P = b[0].length;
-    final DataInfo dinfo = _output._dinfo;
-    for(int i = 0; i < dinfo._cats; ++i) {
-      if(Double.isNaN(data[i])) {
-        Arrays.fill(eta,Double.NaN);
-        break;
-      }
-      int ival = (int) data[i];
-      if (ival != data[i]) throw new IllegalArgumentException("categorical value out of range");
-      ival += dinfo._catOffsets[i];
-      if (!_parms._use_all_factor_levels)
-        --ival;
-      // can get values out of bounds for cat levels not seen in training
-      if (ival >= dinfo._catOffsets[i] && ival < dinfo._catOffsets[i + 1])
-        for(int j = 0; j < eta.length; ++j)
-          eta[j] += b[j][ival];
-    }
-    int noff = dinfo.numStart();
-    for(int i = 0; i < dinfo._nums; ++i) {
-      double d = data[dinfo._cats + i];
-      for (int j = 0; j < eta.length; ++j)
-        eta[j] += b[j][noff + i] * d;
-    }
-    double sumExp = 0;
-    double max_row = 0;
-    for (int j = 0; j < eta.length; ++j) {
-      eta[j] += b[j][P - 1];
-      if(eta[j] > max_row)
-        max_row = eta[j];
-    }
-    for (int j = 0; j < eta.length; ++j)
-      sumExp += (eta[j] = Math.exp(eta[j]-max_row));
-    sumExp = 1.0/sumExp;
-    preds[0] = ArrayUtils.maxIndex(eta);
-    for(int i = 0; i < eta.length; ++i)
-      preds[1+i] = eta[i]*sumExp;
-    return preds;
+  @Override
+  // public double[] score0( Chunk chks[], double weight, double offset, int row_in_chunk, double[] tmp, double[] preds )
+  public double[] score0(Chunk[] chks, double weight, double offset, int row_in_chunk, double[] tmp, double[] preds) {
+    return scoreRow(_output._scoringDinfo.extractDenseRow(chks,row_in_chunk,getRow()),offset,preds);
   }
-
+  @Override protected double[] score0(double[] data, double[] preds){return score0(data,preds,1,0);}
   @Override protected double[] score0(double[] data, double[] preds, double w, double o) {
-    if(_parms._family == Family.multinomial)
-      return scoreMultinomial(data, preds, w, o);
-    Row r = _row.get();
-    if(r == null) _row.set(r = _output._scoringDinfo.newDenseRow());
-    _output._scoringDinfo.extractDenseRow(data,r,w,0);
-    double eta = r.innerProduct(beta());
-    double mu = preds[0] = _parms.linkInv(eta + o);
-    if( _parms._family == Family.binomial ) { // threshold for prediction
-      preds[1] = 1.0 - mu; // class 0
-      preds[2] =       mu; // class 1
-    }
-    return preds;
+    return scoreRow(_output._scoringDinfo.extractDenseRow(data,getRow()),o,preds);
   }
 
   @Override protected void toJavaPredictBody(SBPrintStream body,
@@ -1075,12 +1006,16 @@ public class GLMModel extends Model<GLMModel,GLMModel.GLMParameters,GLMModel.GLM
       @Override
       public void generate(JCodeSB out) {
         JCodeGen.toClassWithArray(out, "static", "BETA", beta_internal()); // "The Coefficients"
+        JCodeGen.toClassWithArray(out, "static", "NUM_MEANS", _output._dinfo._numMeans,"Imputed numeric values");
+        JCodeGen.toClassWithArray(out, "static", "CAT_MODES", _output._dinfo._catModes,"Imputed categorical values.");
         JCodeGen.toStaticVar(out, "CATOFFS", dinfo()._catOffsets, "Categorical Offsets");
       }
     });
-
     body.ip("final double [] b = BETA.VALUES;").nl();
-
+    if(_parms._missing_values_handling == MissingValuesHandling.MeanImputation){
+      body.ip("for(int i = 0; i < " + _output._dinfo._cats + "; ++i) if(Double.isNaN(data[i])) data[i] = CAT_MODES.VALUES[i];").nl();
+      body.ip("for(int i = 0; i < " + _output._dinfo._nums + "; ++i) if(Double.isNaN(data[i + " + _output._dinfo._cats + "])) data[i+" + _output._dinfo._cats + "] = NUM_MEANS.VALUES[i];").nl();
+    }
     if(_parms._family != Family.multinomial) {
       body.ip("double eta = 0.0;").nl();
       if (!_parms._use_all_factor_levels) { // skip level 0 of all factors
@@ -1107,7 +1042,6 @@ public class GLMModel extends Model<GLMModel,GLMModel.GLMParameters,GLMModel.GLM
         body.ip("double mu = hex.genmodel.GenModel.GLM_").p(_parms._link.toString()).p("Inv(eta");
       else
         body.ip("double mu = hex.genmodel.GenModel.GLM_tweedieInv(eta," + _parms._tweedie_link_power);
-//    if( _parms._link == hex.glm.GLMModel.GLMParameters.Link.tweedie ) body.p(",").p(_parms._tweedie_link_power);
       body.p(");").nl();
       if (_parms._family == Family.binomial) {
         body.ip("preds[0] = (mu > ").p(defaultThreshold()).p(") ? 1 : 0").p("; // threshold given by ROC").nl();
diff --git a/h2o-algos/src/main/java/hex/glm/GLMTask.java b/h2o-algos/src/main/java/hex/glm/GLMTask.java
index 6071e2e..d166934 100644
--- a/h2o-algos/src/main/java/hex/glm/GLMTask.java
+++ b/h2o-algos/src/main/java/hex/glm/GLMTask.java
@@ -3,8 +3,6 @@ package hex.glm;
 import hex.DataInfo;
 import hex.DataInfo.Row;
 
-import hex.DataInfo.Rows;
-import hex.FrameTask;
 import hex.FrameTask2;
 import hex.glm.GLMModel.GLMParameters;
 import hex.glm.GLMModel.GLMParameters.Link;
@@ -12,7 +10,6 @@ import hex.glm.GLMModel.GLMWeightsFun;
 import hex.glm.GLMModel.GLMWeights;
 import hex.gram.Gram;
 import hex.glm.GLMModel.GLMParameters.Family;
-import jsr166y.CountedCompleter;
 import water.H2O.H2OCountedCompleter;
 import water.*;
 import water.fvec.*;
@@ -123,34 +120,34 @@ public abstract class GLMTask  {
     @Override public void reduce(GLMResDevTaskMultinomial gt) {_likelihood += gt._likelihood;}
   }
 
- static class YMUTask extends MRTask<YMUTask> {
+ static public class YMUTask extends MRTask<YMUTask> {
    double _yMin = Double.POSITIVE_INFINITY, _yMax = Double.NEGATIVE_INFINITY;
    long _nobs;
-   double _wsum;
+   public double _wsum;
    final int _responseId;
    final int _weightId;
    final int _offsetId;
    final int _nums; // number of numeric columns
    final int _numOff;
-   final boolean _setIgnores;
-   final boolean _comupteWeightedSigma;
+   final boolean _computeWeightedSigma;
    final boolean _skipNAs;
+   final boolean _computeWeightedMeanSigmaResponse;
 
-   BasicStats _basicStats;
+   public BasicStats _basicStats;
+   public BasicStats _basicStatsResponse;
    double [] _yMu;
    double [] _means;
    final int _nClasses;
 
-
-   public YMUTask(DataInfo dinfo, int nclasses, boolean computeWeightedSigma, boolean setIgnores, boolean skipNAs){
+   public YMUTask(DataInfo dinfo, int nclasses, boolean computeWeightedSigma, boolean computeWeightedMeanSigmaResponse, boolean skipNAs, boolean haveResponse){
      _nums = dinfo._nums;
      _numOff = dinfo._cats;
-     _responseId = dinfo.responseChunkId(0);
+     _responseId = haveResponse ? dinfo.responseChunkId(0) : -1;
      _weightId = dinfo._weights?dinfo.weightChunkId():-1;
      _offsetId = dinfo._offset?dinfo.offsetChunkId():-1;
      _nClasses = nclasses;
-     _comupteWeightedSigma = computeWeightedSigma;
-     _setIgnores = setIgnores;
+     _computeWeightedSigma = computeWeightedSigma;
+     _computeWeightedMeanSigmaResponse = computeWeightedMeanSigmaResponse;
      _skipNAs = skipNAs;
      _means = dinfo._numMeans;
    }
@@ -163,21 +160,27 @@ public abstract class GLMTask  {
      for(int i = 0; i < chunks.length; ++i) {
        for (int r = chunks[i].nextNZ(-1); r < chunks[i]._len; r = chunks[i].nextNZ(r)) {
          if(skip[r])continue;
-         if((skip[r] = _skipNAs && chunks[i].isNA(r)) && _setIgnores)
+         if((skip[r] = _skipNAs && chunks[i].isNA(r)) && _weightId != -1)
           weight.set(r,0);
        }
      }
-     Chunk response = chunks[_responseId];
+     Chunk response = _responseId < 0 ? null : chunks[_responseId];
      double [] nums = null;
-     if(_comupteWeightedSigma) {
+     double [] numsResponse = null;
+     if(_computeWeightedSigma) {
        _basicStats = new BasicStats(_nums);
        nums = MemoryManager.malloc8d(_nums);
      }
+     if(_computeWeightedMeanSigmaResponse) {
+       _basicStatsResponse = new BasicStats(_nClasses);
+       numsResponse = MemoryManager.malloc8d(_nClasses);
+     }
      double w;
+     if (response == null) return;
      for(int r = 0; r < response._len; ++r) {
        if(skip[r] || (w = weight.atd(r)) == 0)
          continue;
-       if(_comupteWeightedSigma) {
+       if(_computeWeightedSigma) {
          for(int i = 0; i < _nums; ++i) {
            nums[i] = chunks[i + _numOff].atd(r);
            if(Double.isNaN(nums[i]))
@@ -185,13 +188,19 @@ public abstract class GLMTask  {
          }
          _basicStats.add(nums,w);
        }
-       double d = w*response.atd(r);
+       if(_computeWeightedMeanSigmaResponse) {
+         //FIXME: Add support for subtracting offset from response
+         for(int i = 0; i < _nClasses; ++i)
+           numsResponse[i] = chunks[chunks.length-_nClasses+i].atd(r);
+         _basicStatsResponse.add(numsResponse,w);
+       }
+       double d = response.atd(r);
        if(!Double.isNaN(d)) {
          assert !Double.isNaN(d);
          if (_nClasses > 2)
-           _yMu[(int) d] += 1;
+           _yMu[(int) d] += w;
          else
-           _yMu[0] += d;
+           _yMu[0] += w*d;
          if (d < _yMin)
            _yMin = d;
          if (d > _yMax)
@@ -203,9 +212,8 @@ public abstract class GLMTask  {
    }
    @Override public void postGlobal() {
      ArrayUtils.mult(_yMu,1.0/_wsum);
-     Futures fs = new Futures();
-     fs.blockForPending();
    }
+
    @Override public void reduce(YMUTask ymt) {
      if(_nobs > 0 && ymt._nobs > 0) {
        ArrayUtils.add(_yMu,ymt._yMu);
@@ -215,15 +223,18 @@ public abstract class GLMTask  {
          _yMin = ymt._yMin;
        if(_yMax < ymt._yMax)
          _yMax = ymt._yMax;
-       if(_comupteWeightedSigma) {
+       if(_computeWeightedSigma)
          _basicStats.reduce(ymt._basicStats);
-       }
+       if(_computeWeightedMeanSigmaResponse)
+         _basicStatsResponse.reduce(ymt._basicStatsResponse);
      } else if (_nobs == 0) {
        _yMu = ymt._yMu;
        _nobs = ymt._nobs;
+       _wsum = ymt._wsum;
        _yMin = ymt._yMin;
        _yMax = ymt._yMax;
        _basicStats = ymt._basicStats;
+       _basicStatsResponse = ymt._basicStatsResponse;
      }
    }
  }
diff --git a/h2o-algos/src/main/java/hex/glrm/GLRM.java b/h2o-algos/src/main/java/hex/glrm/GLRM.java
index ba96db8..e668f26 100644
--- a/h2o-algos/src/main/java/hex/glrm/GLRM.java
+++ b/h2o-algos/src/main/java/hex/glrm/GLRM.java
@@ -54,7 +54,6 @@ public class GLRM extends ModelBuilder<GLRMModel,GLRMModel.GLRMParameters,GLRMMo
   private transient GLRMParameters.Loss[] _lossFunc;
 
   @Override protected GLRMDriver trainModelImpl() { return new GLRMDriver(); }
-  @Override public long progressUnits() { return 2 + _parms._max_iterations; }
   @Override public ModelCategory[] can_build() { return new ModelCategory[]{ModelCategory.Clustering}; }
   public enum Initialization { Random, SVD, PlusPlus, User }
 
diff --git a/h2o-algos/src/main/java/hex/glrm/GLRMModel.java b/h2o-algos/src/main/java/hex/glrm/GLRMModel.java
index 07034d6..796ef6a 100644
--- a/h2o-algos/src/main/java/hex/glrm/GLRMModel.java
+++ b/h2o-algos/src/main/java/hex/glrm/GLRMModel.java
@@ -19,6 +19,7 @@ public class GLRMModel extends Model<GLRMModel,GLRMModel.GLRMParameters,GLRMMode
     public String algoName() { return "GLRM"; }
     public String fullName() { return "Generalized Low Rank Modeling"; }
     public String javaName() { return GLRMModel.class.getName(); }
+    @Override public long progressUnits() { return 2 + _max_iterations; }
     public DataInfo.TransformType _transform = DataInfo.TransformType.NONE; // Data transformation (demean to compare with PCA)
     public int _k = 1;                            // Rank of resulting XY matrix
     public GLRM.Initialization _init = GLRM.Initialization.PlusPlus;  // Initialization of Y matrix
diff --git a/h2o-algos/src/main/java/hex/gram/Gram.java b/h2o-algos/src/main/java/hex/gram/Gram.java
index 9422198..d267824 100644
--- a/h2o-algos/src/main/java/hex/gram/Gram.java
+++ b/h2o-algos/src/main/java/hex/gram/Gram.java
@@ -19,7 +19,7 @@ public final class Gram extends Iced<Gram> {
   boolean _hasIntercept;
   public double[][] _xx;
   double[] _diag;
-  public final int _diagN;
+  public int _diagN;
   final int _denseN;
   int _fullN;
   final static int MIN_TSKSZ=10000;
@@ -142,7 +142,7 @@ public final class Gram extends Iced<Gram> {
     return res;
   }
 
-  private static double f_eps = 1e-8;
+  private static double f_eps = 1e-7;
   private static final int MIN_PAR = 1000;
 
   private final void updateZij(int i, int j, double [][] Z, double [] gamma) {
@@ -215,18 +215,7 @@ public final class Gram extends Iced<Gram> {
    * @return Cholesky - cholesky decomposition fo the gram
    */
   public Cholesky qrCholesky(ArrayList<Integer> dropped_cols) {
-    final double [][] Z = getXX(true);
-    // put intercept first
-    int icpt_id = Z.length-1;
-    double d = Z[0][0];
-    Z[0][0] = Z[icpt_id][icpt_id];
-    Z[icpt_id][icpt_id] = d;
-    for(int i = 1; i < Z.length-1; ++i) {
-      d = Z[i][0];
-      Z[i][0] = Z[icpt_id][i];
-      Z[icpt_id][i] = d;
-    }
-    // todo add diagonal hack to save on the largest categorical variable
+    final double [][] Z = getXX(true,true);
     final double [][] R = new double[Z.length][];
     final double [] ZdiagInv = new double[Z.length];
     for(int i = 0; i < Z.length; ++i)
@@ -239,9 +228,9 @@ public final class Gram extends Iced<Gram> {
       for(int k = 0; k < j; ++k) // only need the diagonal, the rest is 0 (dot product of orthogonal vectors)
         zjj += gamma[k] * (gamma[k] * Z[k][k] - 2*Z[j][k]);
       ZdiagInv[j] = 1./zjj;
-      if(-f_eps < zjj && zjj < f_eps) { // collinear column, drop it!
+      if(zjj < f_eps) { // collinear column, drop it!
         zjj = 0;
-        dropped_cols.add(j);
+        dropped_cols.add(j-1);
         ZdiagInv[j] = 0;
       }
       Z[j][j] = zjj;
@@ -302,7 +291,7 @@ public final class Gram extends Iced<Gram> {
       if(Z[i][i] == 0) continue;
       int k = 0;
       for(int l = 0; l <= i; ++l) {
-        if(k < dropped_cols.size() && l == dropped_cols.get(k)) {
+        if(k < dropped_cols.size() && l == (dropped_cols.get(k)+1)) {
           ++k;
           continue;
         }
@@ -310,14 +299,59 @@ public final class Gram extends Iced<Gram> {
       }
       ++j;
     }
-    if((dropped_cols.get(dropped_cols.size()-1)) == ZdiagInv.length-1){
-      dropped_cols.remove(dropped_cols.size()-1);
-      dropped_cols.add(0,0); // first and last columns are switched so that the intercept is the first drugin the QR decomp
-    }
     return new Cholesky(Rnew,new double[0], true);
   }
 
 
+  public int [] dropZeroCols(){
+    ArrayList<Integer> zeros = new ArrayList<>();
+    if(_diag != null)
+      for(int i = 0; i < _diag.length; ++i)
+        if(_diag[i] == 0)zeros.add(i);
+    int diagZeros = zeros.size();
+    for(int i = 0; i < _xx.length; ++i)
+      if(_xx[i][_xx[i].length-1] == 0)
+        zeros.add(_xx[i].length-1);
+    if(zeros.size() == 0) return new int[0];
+    int [] ary = new int[zeros.size() + 1];
+    ary[ary.length-1] = -1;
+    for(int i = 0; i < zeros.size(); ++i)
+      ary[i] = zeros.get(i);
+    int j = 0;
+    if(diagZeros > 0) {
+      double [] diag = MemoryManager.malloc8d(_diagN - diagZeros);
+      int k = 0;
+      for(int i = 0; i < _diagN; ++i)
+        if (ary[j] == i) {
+          ++j;
+        } else  diag[k++] = _diag[i];
+      _diag = diag;
+    }
+    double [][] xxNew = new double[_xx.length-ary.length+diagZeros+1][];
+    int iNew = 0;
+    for(int i = 0; i < _xx.length; ++i) {
+      if((_diagN + i) == ary[j]){
+        ++j; continue;
+      }
+      if(j == 0) {
+        xxNew[iNew++] = _xx[i];
+        continue;
+      }
+      int l = 0,m = 0;
+      double [] x = MemoryManager.malloc8d(_xx[i].length-j);
+      for(int k = 0; k < _xx[i].length; ++k)
+        if(k == ary[l]) {
+          ++l;
+        } else
+          x[m++] = _xx[i][k];
+      xxNew[iNew++] = x;
+    }
+    _xx = xxNew;
+    _diagN = _diag.length;
+    _fullN = _xx[_xx.length-1].length;
+    return Arrays.copyOf(ary,ary.length-1);
+  }
+
   public String toString(){
     if(_fullN >= 1000){
       if(_denseN >= 1000) return "Gram(" + _fullN + ")";
@@ -484,19 +518,27 @@ public final class Gram extends Iced<Gram> {
     return chol;
   }
 
-  public double[][] getXX(){return getXX(false);}
-  public double[][] getXX(boolean lowerDiag) {
+  public double[][] getXX(){return getXX(false, false);}
+  public double[][] getXX(boolean lowerDiag, boolean icptFist) {
     final int N = _fullN;
     double[][] xx = new double[N][];
     for( int i = 0; i < N; ++i )
       xx[i] = MemoryManager.malloc8d(lowerDiag?i+1:N);
+    int off = 0;
+    if(icptFist) {
+      double [] icptRow = _xx[_xx.length-1];
+      xx[0][0] = icptRow[icptRow.length-1];
+      for(int i = 0; i < icptRow.length-1; ++i)
+        xx[i+1][0] = icptRow[i];
+      off = 1;
+    }
     for( int i = 0; i < _diag.length; ++i )
-      xx[i][i] = _diag[i];
-    for( int i = 0; i < _xx.length; ++i ) {
+      xx[i+off][i+off] = _diag[i];
+    for( int i = 0; i < _xx.length - off; ++i ) {
       for( int j = 0; j < _xx[i].length; ++j ) {
-        xx[i + _diag.length][j] = _xx[i][j];
+        xx[i + _diag.length + off][j + off] = _xx[i][j];
         if(!lowerDiag)
-          xx[j][i + _diag.length] = _xx[i][j];
+          xx[j + off][i + _diag.length + off] = _xx[i][j];
       }
     }
     return xx;
@@ -906,9 +948,10 @@ public final class Gram extends Iced<Gram> {
     public final void   solve(double[] y) {
       if( !isSPD() ) throw new NonSPDMatrixException();
       if(_icptFirst) {
-        double d = y[y.length-1];
-        y[y.length-1] = y[0];
-        y[0] = d;
+        double icpt = y[y.length-1];
+        for(int i = y.length-1; i > 0; --i)
+          y[i] = y[i-1];
+        y[0] = icpt;
       }
       // diagonal
       for( int k = 0; k < _diag.length; ++k )
@@ -932,9 +975,10 @@ public final class Gram extends Iced<Gram> {
       for( int k = _diag.length - 1; k >= 0; --k )
         y[k] /= _diag[k];
       if(_icptFirst) {
-        double d = y[y.length-1];
-        y[y.length-1] = y[0];
-        y[0] = d;
+        double icpt = y[0];
+        for(int i = 1; i < y.length; ++i)
+          y[i-1] = y[i];
+        y[y.length-1] = icpt;
       }
     }
     public final boolean isSPD() {return _isSPD;}
@@ -1039,7 +1083,7 @@ public final class Gram extends Iced<Gram> {
 
   public void mul(double [] x, double [] res){
     Arrays.fill(res,0);
-    if(XX == null) XX = getXX(false);
+    if(XX == null) XX = getXX(false,false);
     for(int i = 0; i < XX.length; ++i){
       double d  = 0;
       double [] xi = XX[i];
@@ -1101,6 +1145,9 @@ public final class Gram extends Iced<Gram> {
       _nobs += gt._nobs;
     }
   }
-  public static class NonSPDMatrixException extends RuntimeException {}
+  public static class NonSPDMatrixException extends RuntimeException {
+    public NonSPDMatrixException(){}
+    public NonSPDMatrixException(String msg){super(msg);}
+  }
 }
 
diff --git a/h2o-algos/src/main/java/hex/grep/Grep.java b/h2o-algos/src/main/java/hex/grep/Grep.java
index 145a826..69d21fe 100644
--- a/h2o-algos/src/main/java/hex/grep/Grep.java
+++ b/h2o-algos/src/main/java/hex/grep/Grep.java
@@ -19,7 +19,6 @@ import java.util.regex.PatternSyntaxException;
 public class Grep extends ModelBuilder<GrepModel,GrepModel.GrepParameters,GrepModel.GrepOutput> {
   public Grep( GrepModel.GrepParameters parms ) { super(parms); init(false); }
   @Override protected GrepDriver trainModelImpl() { return new GrepDriver(); }
-  @Override public long progressUnits() { return _parms.train().numRows(); }
   @Override public ModelCategory[] can_build() { return new ModelCategory[]{ModelCategory.Unknown}; }
   @Override public BuilderVisibility builderVisibility() { return BuilderVisibility.Experimental; };
 
diff --git a/h2o-algos/src/main/java/hex/grep/GrepModel.java b/h2o-algos/src/main/java/hex/grep/GrepModel.java
index 8689a27..7ee2ac0 100644
--- a/h2o-algos/src/main/java/hex/grep/GrepModel.java
+++ b/h2o-algos/src/main/java/hex/grep/GrepModel.java
@@ -12,6 +12,7 @@ public class GrepModel extends Model<GrepModel,GrepModel.GrepParameters,GrepMode
     public String algoName() { return "Grep"; }
     public String fullName() { return "Grep"; }
     public String javaName() { return GrepModel.class.getName(); }
+    @Override public long progressUnits() { return train() != null ? train().numRows() : 1; }
     public String _regex;       // The regex
   }
 
diff --git a/h2o-algos/src/main/java/hex/kmeans/KMeans.java b/h2o-algos/src/main/java/hex/kmeans/KMeans.java
index 5bc4719..d470478 100755
--- a/h2o-algos/src/main/java/hex/kmeans/KMeans.java
+++ b/h2o-algos/src/main/java/hex/kmeans/KMeans.java
@@ -28,7 +28,6 @@ public class KMeans extends ClusteringModelBuilder<KMeansModel,KMeansModel.KMean
   public enum Initialization { Random, PlusPlus, Furthest, User }
   /** Start the KMeans training Job on an F/J thread. */
   @Override protected KMeansDriver trainModelImpl() { return new KMeansDriver();  }
-  @Override public long progressUnits() { return _parms._max_iterations; }
 
   // Called from an http request
   public KMeans( KMeansModel.KMeansParameters parms         ) { super(parms    ); init(false); }
diff --git a/h2o-algos/src/main/java/hex/kmeans/KMeansModel.java b/h2o-algos/src/main/java/hex/kmeans/KMeansModel.java
index 6f54549..ec4a77d 100755
--- a/h2o-algos/src/main/java/hex/kmeans/KMeansModel.java
+++ b/h2o-algos/src/main/java/hex/kmeans/KMeansModel.java
@@ -21,6 +21,7 @@ public class KMeansModel extends ClusteringModel<KMeansModel,KMeansModel.KMeansP
     public String algoName() { return "KMeans"; }
     public String fullName() { return "K-means"; }
     public String javaName() { return KMeansModel.class.getName(); }
+    @Override public long progressUnits() { return _max_iterations; }
     public int _max_iterations = 1000;     // Max iterations
     public boolean _standardize = true;    // Standardize columns
     public long _seed = System.nanoTime(); // RNG seed
diff --git a/h2o-algos/src/main/java/hex/naivebayes/NaiveBayes.java b/h2o-algos/src/main/java/hex/naivebayes/NaiveBayes.java
index 911958c..e1db60c 100644
--- a/h2o-algos/src/main/java/hex/naivebayes/NaiveBayes.java
+++ b/h2o-algos/src/main/java/hex/naivebayes/NaiveBayes.java
@@ -26,7 +26,6 @@ import java.util.List;
 public class NaiveBayes extends ModelBuilder<NaiveBayesModel,NaiveBayesParameters,NaiveBayesOutput> {
   public boolean isSupervised(){return true;}
   @Override protected NaiveBayesDriver trainModelImpl() { return new NaiveBayesDriver(); }
-  @Override public long progressUnits() { return 6; }
   @Override public ModelCategory[] can_build() { return new ModelCategory[]{ ModelCategory.Unknown }; }
 
   @Override
diff --git a/h2o-algos/src/main/java/hex/naivebayes/NaiveBayesModel.java b/h2o-algos/src/main/java/hex/naivebayes/NaiveBayesModel.java
index 961d0b2..9f24b79 100644
--- a/h2o-algos/src/main/java/hex/naivebayes/NaiveBayesModel.java
+++ b/h2o-algos/src/main/java/hex/naivebayes/NaiveBayesModel.java
@@ -27,6 +27,7 @@ public class NaiveBayesModel extends Model<NaiveBayesModel,NaiveBayesModel.Naive
     public String algoName() { return "NaiveBayes"; }
     public String fullName() { return "Naive Bayes"; }
     public String javaName() { return NaiveBayesModel.class.getName(); }
+    @Override public long progressUnits() { return 6; }
   }
 
   public static class NaiveBayesOutput extends Model.Output {
diff --git a/h2o-algos/src/main/java/hex/pca/PCA.java b/h2o-algos/src/main/java/hex/pca/PCA.java
index 33f398c..c76c7eb 100755
--- a/h2o-algos/src/main/java/hex/pca/PCA.java
+++ b/h2o-algos/src/main/java/hex/pca/PCA.java
@@ -33,7 +33,6 @@ public class PCA extends ModelBuilder<PCAModel,PCAModel.PCAParameters,PCAModel.P
   // Number of columns in training set (p)
   private transient int _ncolExp;    // With categoricals expanded into 0/1 indicator cols
   @Override protected PCADriver trainModelImpl() { return new PCADriver(); }
-  @Override public long progressUnits() { return _parms._pca_method == PCAParameters.Method.GramSVD ? 5 : 3; }
   @Override public ModelCategory[] can_build() { return new ModelCategory[]{ ModelCategory.Clustering }; }
 
   @Override protected void checkMemoryFootPrint() {
diff --git a/h2o-algos/src/main/java/hex/pca/PCAModel.java b/h2o-algos/src/main/java/hex/pca/PCAModel.java
index 42fc03c..e338d03 100755
--- a/h2o-algos/src/main/java/hex/pca/PCAModel.java
+++ b/h2o-algos/src/main/java/hex/pca/PCAModel.java
@@ -22,6 +22,8 @@ public class PCAModel extends Model<PCAModel,PCAModel.PCAParameters,PCAModel.PCA
     public String algoName() { return "PCA"; }
     public String fullName() { return "Principle Components Analysis"; }
     public String javaName() { return PCAModel.class.getName(); }
+    @Override public long progressUnits() { return _pca_method == PCAParameters.Method.GramSVD ? 5 : 3; }
+
     public DataInfo.TransformType _transform = DataInfo.TransformType.NONE; // Data transformation
     public Method _pca_method = Method.GramSVD;   // Method for computing PCA
     public int _k = 1;                     // Number of principal components
diff --git a/h2o-algos/src/main/java/hex/schemas/DRFV3.java b/h2o-algos/src/main/java/hex/schemas/DRFV3.java
index 2b27d02..110d8b9 100644
--- a/h2o-algos/src/main/java/hex/schemas/DRFV3.java
+++ b/h2o-algos/src/main/java/hex/schemas/DRFV3.java
@@ -14,6 +14,7 @@ public class DRFV3 extends SharedTreeV3<DRF,DRFV3, DRFV3.DRFParametersV3> {
         "nfolds",
         "keep_cross_validation_predictions",
         "score_each_iteration",
+        "score_tree_interval",
         "fold_assignment",
         "fold_column",
 				"response_column",
diff --git a/h2o-algos/src/main/java/hex/schemas/DeepLearningV3.java b/h2o-algos/src/main/java/hex/schemas/DeepLearningV3.java
index 6b4d03e..b8d6715 100755
--- a/h2o-algos/src/main/java/hex/schemas/DeepLearningV3.java
+++ b/h2o-algos/src/main/java/hex/schemas/DeepLearningV3.java
@@ -59,6 +59,7 @@ public class DeepLearningV3 extends ModelBuilderSchema<DeepLearning,DeepLearning
         "initial_weight_scale",
         "loss",
         "distribution",
+        "quantile_alpha",
         "tweedie_power",
         "score_interval",
         "score_training_samples",
@@ -401,15 +402,18 @@ public class DeepLearningV3 extends ModelBuilderSchema<DeepLearning,DeepLearning
      * be used for classification as well (where it emphasizes the error on all
      * output classes, not just for the actual class).
      */
-    @API(help = "Loss function", values = { "Automatic", "CrossEntropy", "Quadratic", "Huber", "Absolute" }, required = false, level = API.Level.secondary, direction=API.Direction.INOUT, gridable = true)
+    @API(help = "Loss function", values = { "Automatic", "CrossEntropy", "Quadratic", "Huber", "Absolute", "Quantile" }, required = false, level = API.Level.secondary, direction=API.Direction.INOUT, gridable = true)
     public DeepLearningParameters.Loss loss;
 
-    @API(help = "Distribution function", values = { "AUTO", "bernoulli", "multinomial", "gaussian", "poisson", "gamma", "tweedie", "laplace", "huber" }, level = API.Level.secondary, gridable = true)
+    @API(help = "Distribution function", values = { "AUTO", "bernoulli", "multinomial", "gaussian", "poisson", "gamma", "tweedie", "laplace", "huber", "quantile" }, level = API.Level.secondary, gridable = true)
     public Distribution.Family distribution;
 
-    @API(help = "Tweedie Power", level = API.Level.secondary)
+    @API(help = "Tweedie Power", level = API.Level.secondary, gridable = true)
     public double tweedie_power;
 
+    @API(help="Desired quantile for quantile regression (from 0.0 to 1.0)", level = API.Level.secondary, gridable = true)
+    public double quantile_alpha;
+
     /*Scoring*/
     /**
      * The minimum time (in seconds) to elapse between model scoring. The actual
diff --git a/h2o-algos/src/main/java/hex/schemas/GBMV3.java b/h2o-algos/src/main/java/hex/schemas/GBMV3.java
index 28c8abe..8ec5b44 100755
--- a/h2o-algos/src/main/java/hex/schemas/GBMV3.java
+++ b/h2o-algos/src/main/java/hex/schemas/GBMV3.java
@@ -16,6 +16,7 @@ public class GBMV3 extends SharedTreeV3<GBM,GBMV3,GBMV3.GBMParametersV3> {
         "nfolds",
         "keep_cross_validation_predictions",
         "score_each_iteration",
+        "score_tree_interval",
         "fold_assignment",
         "fold_column",
 				"response_column",
@@ -43,6 +44,7 @@ public class GBMV3 extends SharedTreeV3<GBM,GBMV3,GBMV3.GBMParametersV3> {
 				"build_tree_one_node",
         "learn_rate",
         "distribution",
+        "quantile_alpha",
         "tweedie_power",
         "checkpoint",
         "sample_rate",
@@ -54,9 +56,12 @@ public class GBMV3 extends SharedTreeV3<GBM,GBMV3,GBMV3.GBMParametersV3> {
     @API(help="Learning rate (from 0.0 to 1.0)", gridable = true)
     public float learn_rate;
 
-    @API(help = "Distribution function", values = { "AUTO", "bernoulli", "multinomial", "gaussian", "poisson", "gamma", "tweedie", "laplace" }, gridable = true)
+    @API(help = "Distribution function", values = { "AUTO", "bernoulli", "multinomial", "gaussian", "poisson", "gamma", "tweedie", "laplace", "quantile" }, gridable = true)
     public Distribution.Family distribution;
 
+    @API(help="Desired quantile for quantile regression (from 0.0 to 1.0)", level = API.Level.secondary, gridable = true)
+    public double quantile_alpha;
+
     @API(help = "Tweedie Power (between 1 and 2)", level = API.Level.secondary, gridable = true)
     public double tweedie_power;
 
diff --git a/h2o-algos/src/main/java/hex/schemas/SharedTreeV3.java b/h2o-algos/src/main/java/hex/schemas/SharedTreeV3.java
index 4be7885..d42e640 100755
--- a/h2o-algos/src/main/java/hex/schemas/SharedTreeV3.java
+++ b/h2o-algos/src/main/java/hex/schemas/SharedTreeV3.java
@@ -75,5 +75,8 @@ public class SharedTreeV3<B extends SharedTree, S extends SharedTreeV3<B,S,P>, P
 
     @API(help = "Column sample rate per tree (from 0.0 to 1.0)", gridable = true)
     public float col_sample_rate_per_tree;
+
+    @API(help="Score the model after every so many trees. Disabled if set to 0.", level = API.Level.secondary, gridable = false)
+    public int score_tree_interval;
   }
 }
diff --git a/h2o-algos/src/main/java/hex/svd/SVD.java b/h2o-algos/src/main/java/hex/svd/SVD.java
index a053fbe..51647db 100644
--- a/h2o-algos/src/main/java/hex/svd/SVD.java
+++ b/h2o-algos/src/main/java/hex/svd/SVD.java
@@ -41,14 +41,6 @@ public class SVD extends ModelBuilder<SVDModel,SVDModel.SVDParameters,SVDModel.S
   private transient int _ncolExp;    // With categoricals expanded into 0/1 indicator cols
 
   @Override protected SVDDriver trainModelImpl() { return new SVDDriver(); }
-  @Override public long progressUnits() {
-    switch(_parms._svd_method) {
-    case GramSVD:    return 2;
-    case Power:      return 1 + _parms._nv;
-    case Randomized: return 5 + _parms._max_iterations;
-    default:         return _parms._nv;
-    }
-  }
   @Override public ModelCategory[] can_build() { return new ModelCategory[]{ ModelCategory.DimReduction }; }
   @Override public BuilderVisibility builderVisibility() { return BuilderVisibility.Experimental; }
 
diff --git a/h2o-algos/src/main/java/hex/svd/SVDModel.java b/h2o-algos/src/main/java/hex/svd/SVDModel.java
index e58ae0a..c40f4b4 100644
--- a/h2o-algos/src/main/java/hex/svd/SVDModel.java
+++ b/h2o-algos/src/main/java/hex/svd/SVDModel.java
@@ -17,6 +17,14 @@ public class SVDModel extends Model<SVDModel,SVDModel.SVDParameters,SVDModel.SVD
     public String algoName() { return "SVD"; }
     public String fullName() { return "Singular Value Decomposition"; }
     public String javaName() { return SVDModel.class.getName(); }
+    @Override public long progressUnits() {
+      switch(_svd_method) {
+        case GramSVD:    return 2;
+        case Power:      return 1 + _nv;
+        case Randomized: return 5 + _max_iterations;
+        default:         return _nv;
+      }
+    }
     public DataInfo.TransformType _transform = DataInfo.TransformType.NONE; // Data transformation (demean to compare with PCA)
     public Method _svd_method = Method.GramSVD;   // Method for computing SVD
     public int _nv = 1;    // Number of right singular vectors to calculate
diff --git a/h2o-algos/src/main/java/hex/tree/SharedTree.java b/h2o-algos/src/main/java/hex/tree/SharedTree.java
index 4ac95f8..677dafe 100755
--- a/h2o-algos/src/main/java/hex/tree/SharedTree.java
+++ b/h2o-algos/src/main/java/hex/tree/SharedTree.java
@@ -52,8 +52,6 @@ public abstract class SharedTree<M extends SharedTreeModel<M,P,O>, P extends Sha
 
   public boolean isSupervised(){return true;}
 
-  @Override public long progressUnits() { return _parms._ntrees; }
-
   @Override protected boolean computePriorClassDistribution(){ return true;}
 
   /** Initialize the ModelBuilder, validating all arguments and preparing the
@@ -108,6 +106,7 @@ public abstract class SharedTree<M extends SharedTreeModel<M,P,O>, P extends Sha
     if (_parms._nbins_top_level >= 1<<16) error ("_nbins_top_level", "nbins_top_level must be < " + (1<<16));
     if (_parms._max_depth <= 0) error ("_max_depth", "_max_depth must be > 0.");
     if (_parms._min_rows <=0) error ("_min_rows", "_min_rows must be > 0.");
+    if (_parms._score_tree_interval < 0 || _parms._score_tree_interval > _parms._ntrees) error ("_score_tree_interval", "_score_tree_interval must be >= 0 and <= _ntrees.");
     if (!(0.0 < _parms._sample_rate && _parms._sample_rate <= 1.0))
       error("_sample_rate", "sample_rate should be in interval ]0,1] but it is " + _parms._sample_rate);
     if (!(0.0 < _parms._col_sample_rate_per_tree && _parms._col_sample_rate_per_tree <= 1.0))
@@ -467,14 +466,18 @@ public abstract class SharedTree<M extends SharedTreeModel<M,P,O>, P extends Sha
     long sinceLastScore = now-_timeLastScoreStart;
     boolean updated = false;
     _job.update(0,"Built " + _model._output._ntrees + " trees so far (out of " + _parms._ntrees + ").");
-    // Now model already contains tid-trees in serialized form
-    if( _parms._score_each_iteration ||
-            finalScoring ||
-            (now-_firstScore < _parms._initial_score_interval) || // Score every time for 4 secs
-            // Throttle scoring to keep the cost sane; limit to a 10% duty cycle & every 4 secs
-            (sinceLastScore > _parms._score_interval && // Limit scoring updates to every 4sec
-                    (double)(_timeLastScoreEnd-_timeLastScoreStart)/sinceLastScore < 0.1) ) { // 10% duty cycle
 
+    boolean timeToScore = (now-_firstScore < _parms._initial_score_interval) || // Score every time for 4 secs
+        // Throttle scoring to keep the cost sane; limit to a 10% duty cycle & every 4 secs
+        (sinceLastScore > _parms._score_interval && // Limit scoring updates to every 4sec
+            (double)(_timeLastScoreEnd-_timeLastScoreStart)/sinceLastScore < 0.1); //10% duty cycle
+
+    boolean manualInterval = _parms._score_tree_interval > 0 && _model._output._ntrees % _parms._score_tree_interval == 0;
+
+    // Now model already contains tid-trees in serialized form
+    if( _parms._score_each_iteration || finalScoring || // always score under these circumstances
+        (timeToScore && _parms._score_tree_interval == 0) || // use time-based duty-cycle heuristic only if the user didn't specify _score_tree_interval
+        manualInterval) {
       checkMemoryFootPrint();
 
       // If validation is specified we use a model for scoring, so we need to
@@ -738,7 +741,7 @@ public abstract class SharedTree<M extends SharedTreeModel<M,P,O>, P extends Sha
    * @return initial value
    */
   protected double getInitialValue() {
-    return new InitialValue(_parms._distribution, _parms._tweedie_power).doAll(
+    return new InitialValue(_parms).doAll(
             _response,
             hasWeightCol() ? _weights : _response.makeCon(1),
             hasOffsetCol() ? _offset : _response.makeCon(0)
@@ -747,7 +750,7 @@ public abstract class SharedTree<M extends SharedTreeModel<M,P,O>, P extends Sha
 
   // Helper MRTask to compute the initial value
   private static class InitialValue extends MRTask<InitialValue> {
-    public  InitialValue(Distribution.Family family, double power) { _dist = new Distribution(family, power); }
+    public  InitialValue(Model.Parameters parms) { _dist = new Distribution(parms); }
     final private Distribution _dist;
     private double _num;
     private double _denom;
diff --git a/h2o-algos/src/main/java/hex/tree/SharedTreeModel.java b/h2o-algos/src/main/java/hex/tree/SharedTreeModel.java
index baf2d37..6e0bc17 100755
--- a/h2o-algos/src/main/java/hex/tree/SharedTreeModel.java
+++ b/h2o-algos/src/main/java/hex/tree/SharedTreeModel.java
@@ -34,13 +34,17 @@ public abstract class SharedTreeModel<M extends SharedTreeModel<M,P,O>, P extend
 
     public boolean _build_tree_one_node = false;
 
+    public int _score_tree_interval = 0; // score every so many trees (no matter what)
+
     public int _initial_score_interval = 4000; //Adding this parameter to take away the hard coded value of 4000 for scoring the first  4 secs
 
     public int _score_interval = 4000; //Adding this parameter to take away the hard coded value of 4000 for scoring each iteration every 4 secs
 
     public float _sample_rate = 0.632f; //fraction of rows to sample for each tree
 
-    @Override protected long nFoldSeed() { 
+    @Override public long progressUnits() { return _ntrees; }
+
+    @Override protected long nFoldSeed() {
       return _seed == -1 ? (_seed = RandomUtils.getRNG(System.nanoTime()).nextLong()) : _seed;
     }
 
@@ -82,7 +86,7 @@ public abstract class SharedTreeModel<M extends SharedTreeModel<M,P,O>, P extend
 
   @Override
   public double deviance(double w, double y, double f) {
-    return new Distribution(_parms._distribution, _parms._tweedie_power).deviance(w, y, f);
+    return new Distribution(_parms).deviance(w, y, f);
   }
 
   @Override public ModelMetrics.MetricBuilder makeMetricBuilder(String[] domain) {
diff --git a/h2o-algos/src/main/java/hex/tree/gbm/GBM.java b/h2o-algos/src/main/java/hex/tree/gbm/GBM.java
index 8ab3c4d..1c9386d 100755
--- a/h2o-algos/src/main/java/hex/tree/gbm/GBM.java
+++ b/h2o-algos/src/main/java/hex/tree/gbm/GBM.java
@@ -4,7 +4,6 @@ import hex.Distribution;
 import hex.ModelCategory;
 import hex.quantile.Quantile;
 import hex.quantile.QuantileModel;
-import hex.schemas.GBMV3;
 import hex.tree.*;
 import hex.tree.DTree.DecidedNode;
 import hex.tree.DTree.LeafNode;
@@ -120,6 +119,9 @@ public class GBM extends SharedTree<GBMModel,GBMModel.GBMParameters,GBMModel.GBM
     case laplace:
       if (isClassifier()) error("_distribution", H2O.technote(2, "Laplace requires the response to be numeric."));
       break;
+    case quantile:
+      if (isClassifier()) error("_distribution", H2O.technote(2, "Quantile requires the response to be numeric."));
+      break;
     case AUTO:
       break;
     default:
@@ -142,11 +144,13 @@ public class GBM extends SharedTree<GBMModel,GBMModel.GBMParameters,GBMModel.GBM
       if (!(1 <= _mtry && _mtry <= _ncols)) throw new IllegalArgumentException("Computed mtry should be in interval <1,"+_ncols+"> but it is " + _mtry);
 
       // for Bernoulli, we compute the initial value with Newton-Raphson iteration, otherwise it might be NaN here
-      _initialPrediction = _nclass > 2 || _parms._distribution == Distribution.Family.laplace ? 0 : getInitialValue();
+      _initialPrediction = _nclass > 2 || _parms._distribution == Distribution.Family.laplace || _parms._distribution == Distribution.Family.quantile ? 0 : getInitialValue();
       if (_parms._distribution == Distribution.Family.bernoulli) {
         if (hasOffsetCol()) _initialPrediction = getInitialValueBernoulliOffset(_train);
       } else if (_parms._distribution == Distribution.Family.laplace) {
         _initialPrediction = getInitialValueQuantile(0.5);
+      } else if (_parms._distribution == Distribution.Family.quantile) {
+        _initialPrediction = getInitialValueQuantile(_parms._quantile_alpha);
       }
       _model._output._init_f = _initialPrediction; //always write the initial value here (not just for Bernoulli)
 
@@ -243,7 +247,7 @@ public class GBM extends SharedTree<GBMModel,GBMModel.GBMParameters,GBMModel.GBM
         Chunk ys = chk_resp(chks);
         Chunk offset = chk_offset(chks);
         Chunk weight = hasWeightCol() ? chk_weight(chks) : new C0DChunk(1, chks[0]._len);
-        Distribution dist = new Distribution(Distribution.Family.bernoulli);
+        Distribution dist = new Distribution(_parms);
         for( int row = 0; row < ys._len; row++) {
           double w = weight.atd(row);
           if (w == 0) continue;
@@ -273,7 +277,7 @@ public class GBM extends SharedTree<GBMModel,GBMModel.GBMParameters,GBMModel.GBM
         Chunk preds = chk_tree(chks, 0); // Prior tree sums
         Chunk wk = chk_work(chks, 0); // Place to store residuals
         double fs[] = _nclass > 1 ? new double[_nclass+1] : null;
-        Distribution dist = new Distribution(_parms._distribution, _parms._tweedie_power);
+        Distribution dist = new Distribution(_parms);
         for( int row = 0; row < wk._len; row++) {
           if( ys.isNA(row) ) continue;
           double f = preds.atd(row) + offset.atd(row);
@@ -412,6 +416,8 @@ public class GBM extends SharedTree<GBMModel,GBMModel.GBMParameters,GBMModel.GBM
       GammaPass gp = new GammaPass(ktrees, leafs, _parms._distribution).doAll(_train);
       if (_parms._distribution == Distribution.Family.laplace) {
         fitBestConstantsQuantile(ktrees, leafs, 0.5); //special case for Laplace: compute the median for each leaf node and store that as prediction
+      } else if (_parms._distribution == Distribution.Family.quantile) {
+        fitBestConstantsQuantile(ktrees, leafs, _parms._quantile_alpha); //compute the alpha-quantile for each leaf node and store that as prediction
       } else {
         fitBestConstants(ktrees, leafs, gp);
       }
@@ -521,11 +527,11 @@ public class GBM extends SharedTree<GBMModel,GBMModel.GBMParameters,GBMModel.GBM
       H2O.submitTask(sqt);
       sqt.join();
 
-      for (int i = 0; i < ktrees[0]._len - leafs[0]; i++) {
+      for (int i = 0; i < sqt._quantiles.length; i++) {
         float val = (float) (_parms._learn_rate * sqt._quantiles[i]);
         assert !Float.isNaN(val) && !Float.isInfinite(val);
-        ((LeafNode) ktrees[0].node(leafs[0] + i))._pred = val;
-//        Log.info("Leaf " + (leafs[0]+i) + " has median: " + sqt._quantiles[i]);
+        ((LeafNode) ktrees[0].node((int)strata.min() + i))._pred = val;
+//        Log.info("Leaf " + ((int)strata.min()+i) + " has median: " + sqt._quantiles[i]);
       }
     }
 
@@ -574,7 +580,7 @@ public class GBM extends SharedTree<GBMModel,GBMModel.GBMParameters,GBMModel.GBM
                 || _dist == Distribution.Family.gamma
                 || _dist == Distribution.Family.tweedie)
         {
-          return new Distribution(_dist, _parms._tweedie_power).link(g);
+          return new Distribution(_parms).link(g);
         } else {
           return g;
         }
@@ -610,7 +616,7 @@ public class GBM extends SharedTree<GBMModel,GBMModel.GBMParameters,GBMModel.GBM
           // If we have all constant responses, then we do not split even the
           // root and the residuals should be zero.
           if( tree.root() instanceof LeafNode ) continue;
-          Distribution dist = new Distribution(_parms._distribution, _parms._tweedie_power);
+          Distribution dist = new Distribution(_parms);
           for( int row=0; row<nids._len; row++ ) { // For all rows
             int nid = (int)nids.at8(row);          // Get Node to decide from
 
@@ -635,8 +641,8 @@ public class GBM extends SharedTree<GBMModel,GBMModel.GBMParameters,GBMModel.GBM
             assert !ress.isNA(row);
 
             // OOB rows get placed properly (above), but they don't affect the computed Gamma (below)
-            // For Laplace distribution, we need to compute the median of (y-offset-preds == y-f), will be done outside of here
-            if (wasOOBRow || _parms._distribution == Distribution.Family.laplace) continue;
+            // For Laplace/Quantile distribution, we need to compute the median of (y-offset-preds == y-f), will be done outside of here
+            if (wasOOBRow || _parms._distribution == Distribution.Family.laplace || _parms._distribution == Distribution.Family.quantile) continue;
 
             // Compute numerator and denominator of terminal node estimate (gamma)
             double w = hasWeightCol() ? chk_weight(chks).atd(row) : 1; //weight
@@ -688,7 +694,7 @@ public class GBM extends SharedTree<GBMModel,GBMModel.GBMParameters,GBMModel.GBM
   // turns the results into a probability distribution.
   @Override protected double score1( Chunk chks[], double weight, double offset, double fs[/*nclass*/], int row ) {
     double f = chk_tree(chks,0).atd(row) + offset;
-    double p = new Distribution(_parms._distribution, _parms._tweedie_power).linkInv(f);
+    double p = new Distribution(_parms).linkInv(f);
     if( _parms._distribution == Distribution.Family.bernoulli ) {
       fs[2] = p;
       fs[1] = 1.0-p;
diff --git a/h2o-algos/src/main/java/hex/tree/gbm/GBMModel.java b/h2o-algos/src/main/java/hex/tree/gbm/GBMModel.java
index b61bdcb..88007f4 100755
--- a/h2o-algos/src/main/java/hex/tree/gbm/GBMModel.java
+++ b/h2o-algos/src/main/java/hex/tree/gbm/GBMModel.java
@@ -39,7 +39,7 @@ public class GBMModel extends SharedTreeModel<GBMModel,GBMModel.GBMParameters,GB
     super.score0(data, preds, weight, offset);    // These are f_k(x) in Algorithm 10.4
     if (_parms._distribution == Distribution.Family.bernoulli) {
       double f = preds[1] + _output._init_f + offset; //Note: class 1 probability stored in preds[1] (since we have only one tree)
-      preds[2] = new Distribution(Distribution.Family.bernoulli).linkInv(f);
+      preds[2] = new Distribution(_parms).linkInv(f);
       preds[1] = 1.0 - preds[2];
     } else if (_parms._distribution == Distribution.Family.multinomial) { // Kept the initial prediction for binomial
       if (_output.nclasses() == 2) { //1-tree optimization for binomial
@@ -49,7 +49,7 @@ public class GBMModel extends SharedTreeModel<GBMModel,GBMModel.GBMParameters,GB
       hex.genmodel.GenModel.GBM_rescale(preds);
     } else { //Regression
       double f = preds[0] + _output._init_f + offset;
-      preds[0] = new Distribution(_parms._distribution, _parms._tweedie_power).linkInv(f);
+      preds[0] = new Distribution(_parms).linkInv(f);
     }
     return preds;
   }
@@ -60,7 +60,7 @@ public class GBMModel extends SharedTreeModel<GBMModel,GBMModel.GBMParameters,GB
     // the loss function.
     if( _parms._distribution == Distribution.Family.bernoulli ) {
       body.ip("preds[2] = preds[1] + ").p(_output._init_f).p(";").nl();
-      body.ip("preds[2] = " + new Distribution(_parms._distribution).linkInvString("preds[2]") + ";").nl();
+      body.ip("preds[2] = " + new Distribution(_parms).linkInvString("preds[2]") + ";").nl();
       body.ip("preds[1] = 1.0-preds[2];").nl();
       if (_parms._balance_classes)
         body.ip("hex.genmodel.GenModel.correctProbabilities(preds, PRIOR_CLASS_DISTRIB, MODEL_CLASS_DISTRIB);").nl();
@@ -69,7 +69,7 @@ public class GBMModel extends SharedTreeModel<GBMModel,GBMModel.GBMParameters,GB
     }
     if( _output.nclasses() == 1 ) { // Regression
       body.ip("preds[0] += ").p(_output._init_f).p(";").nl();
-      body.ip("preds[0] = " + new Distribution(_parms._distribution, _parms._tweedie_power).linkInvString("preds[0]") + ";").nl();
+      body.ip("preds[0] = " + new Distribution(_parms).linkInvString("preds[0]") + ";").nl();
       return;
     }
     if( _output.nclasses()==2 ) { // Kept the initial prediction for binomial
diff --git a/h2o-algos/src/main/java/hex/util/LinearAlgebraUtils.java b/h2o-algos/src/main/java/hex/util/LinearAlgebraUtils.java
index 334f46b..92c698f 100644
--- a/h2o-algos/src/main/java/hex/util/LinearAlgebraUtils.java
+++ b/h2o-algos/src/main/java/hex/util/LinearAlgebraUtils.java
@@ -52,7 +52,7 @@ public class LinearAlgebraUtils {
       if (Double.isNaN(row[col])) {
         if (dinfo._imputeMissing)
           cidx = dinfo._catModes[col];
-        else if (dinfo._catMissing[col] == 0)
+        else if (!dinfo._catMissing[col])
           continue;   // Skip if entry missing and no NA bucket. All indicators will be zero.
         else
           cidx = dinfo._catOffsets[col+1]-1;  // Otherwise, missing value turns into extra (last) factor
@@ -171,7 +171,7 @@ public class LinearAlgebraUtils {
             if (Double.isNaN(a)) {
               if (_ainfo._imputeMissing)
                 cidx = _ainfo._catModes[p];
-              else if (_ainfo._catMissing[p] == 0)
+              else if (!_ainfo._catMissing[p])
                 continue;   // Skip if entry missing and no NA bucket. All indicators will be zero.
               else
                 cidx = _ainfo._catOffsets[p+1]-1;     // Otherwise, missing value turns into extra (last) factor
diff --git a/h2o-algos/src/main/java/hex/word2vec/Word2Vec.java b/h2o-algos/src/main/java/hex/word2vec/Word2Vec.java
index 8a62b33..be2d9b3 100644
--- a/h2o-algos/src/main/java/hex/word2vec/Word2Vec.java
+++ b/h2o-algos/src/main/java/hex/word2vec/Word2Vec.java
@@ -14,7 +14,6 @@ public class Word2Vec extends ModelBuilder<Word2VecModel,Word2VecModel.Word2VecP
   public enum NormModel { HSM, NegSampling }
   public Word2Vec(Word2VecModel.Word2VecParameters parms) { super(parms); }
   @Override protected Word2VecDriver trainModelImpl() { return new Word2VecDriver(); }
-  @Override public long progressUnits() { return _parms._epochs; }
 
   /** Initialize the ModelBuilder, validating all arguments and preparing the
    *  training frame.  This call is expected to be overridden in the subclasses
diff --git a/h2o-algos/src/main/java/hex/word2vec/Word2VecModel.java b/h2o-algos/src/main/java/hex/word2vec/Word2VecModel.java
index acc497b..3e96ee6 100644
--- a/h2o-algos/src/main/java/hex/word2vec/Word2VecModel.java
+++ b/h2o-algos/src/main/java/hex/word2vec/Word2VecModel.java
@@ -223,6 +223,7 @@ public class Word2VecModel extends Model<Word2VecModel, Word2VecParameters, Word
     public String algoName() { return "Word2Vec"; }
     public String fullName() { return "Word2Vec"; }
     public String javaName() { return Word2VecModel.class.getName(); }
+    @Override public long progressUnits() { return _epochs; }
     static final int MAX_VEC_SIZE = 10000;
 
     public Word2Vec.WordModel _wordModel = Word2Vec.WordModel.SkipGram;
diff --git a/h2o-algos/src/test/java/hex/deeplearning/DeepLearningGradientCheck.java b/h2o-algos/src/test/java/hex/deeplearning/DeepLearningGradientCheck.java
index b244efd..063b31d 100644
--- a/h2o-algos/src/test/java/hex/deeplearning/DeepLearningGradientCheck.java
+++ b/h2o-algos/src/test/java/hex/deeplearning/DeepLearningGradientCheck.java
@@ -47,6 +47,7 @@ public class DeepLearningGradientCheck extends TestUtil {
       for (Distribution.Family dist : new Distribution.Family[]{
               Distribution.Family.gaussian,
               Distribution.Family.laplace,
+              Distribution.Family.quantile,
               Distribution.Family.huber,
               Distribution.Family.gamma,
               Distribution.Family.poisson,
@@ -90,6 +91,7 @@ public class DeepLearningGradientCheck extends TestUtil {
                 parms._activation = act;
                 parms._adaptive_rate = adaptive;
                 parms._rate = 1e-4;
+                parms._quantile_alpha = 0.2;
                 parms._momentum_start = 0.9;
                 parms._momentum_stable = 0.99;
                 DeepLearningModelInfo.gradientCheck = null;
diff --git a/h2o-algos/src/test/java/hex/deeplearning/DeepLearningMissingTest.java b/h2o-algos/src/test/java/hex/deeplearning/DeepLearningMissingTest.java
index 1b6c72c..eab9149 100644
--- a/h2o-algos/src/test/java/hex/deeplearning/DeepLearningMissingTest.java
+++ b/h2o-algos/src/test/java/hex/deeplearning/DeepLearningMissingTest.java
@@ -37,13 +37,15 @@ public class DeepLearningMissingTest extends TestUtil {
     StringBuilder sb = new StringBuilder();
     for (DeepLearningParameters.MissingValuesHandling mvh :
             new DeepLearningParameters.MissingValuesHandling[]{
-            DeepLearningParameters.MissingValuesHandling.Skip,
-            DeepLearningParameters.MissingValuesHandling.MeanImputation })
+                    DeepLearningParameters.MissingValuesHandling.MeanImputation,
+                    DeepLearningParameters.MissingValuesHandling.Skip
+            })
     {
       double sumerr = 0;
       Map<Double,Double> map = new TreeMap<>();
-      for (double missing_fraction : new double[]{0, 0.1, 0.25, 0.5, 0.75, 0.99}) {
+      for (double missing_fraction : new double[]{0, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.99}) {
 
+        double err=0;
         try {
           Scope.enter();
           NFSFileVec  nfs = NFSFileVec.make(find_test_file("smalldata/junit/weather.csv"));
@@ -75,8 +77,8 @@ public class DeepLearningMissingTest extends TestUtil {
           p._ignored_columns = new String[]{train._names[1],train._names[22]}; //only for weather data
           p._missing_values_handling = mvh;
           p._loss = DeepLearningParameters.Loss.Huber;
-          p._activation = DeepLearningParameters.Activation.Tanh;
-          p._hidden = new int[]{100,100};
+          p._activation = DeepLearningParameters.Activation.Rectifier;
+          p._hidden = new int[]{50,50};
           p._l1 = 1e-5;
           p._input_dropout_ratio = 0.2;
           p._epochs = 3;
@@ -97,16 +99,14 @@ public class DeepLearningMissingTest extends TestUtil {
           mymodel = dl.trainModel().get();
 
           // Extract the scoring on validation set from the model
-          double err = mymodel.loss();
+          err = mymodel.loss();
 
           Log.info("Missing " + missing_fraction * 100 + "% -> logloss: " + err);
-          map.put(missing_fraction, err);
-          sumerr += err;
-          Scope.exit();
         } catch(Throwable t) {
           t.printStackTrace();
-          throw new RuntimeException(t);
+          err = 100;
         } finally {
+          Scope.exit();
           // cleanup
           if (mymodel != null) {
             mymodel.delete();
@@ -115,6 +115,8 @@ public class DeepLearningMissingTest extends TestUtil {
           if (test != null) test.delete();
           if (data != null) data.delete();
         }
+        map.put(missing_fraction, err);
+        sumerr += err;
       }
       sb.append("\nMethod: ").append(mvh.toString()).append("\n");
       sb.append("missing fraction --> Error\n");
@@ -125,8 +127,8 @@ public class DeepLearningMissingTest extends TestUtil {
       sumErr.put(mvh, sumerr);
     }
     Log.info(sb.toString());
-    Assert.assertTrue(sumErr.get(DeepLearningParameters.MissingValuesHandling.Skip) > 4.9);
-    Assert.assertTrue(sumErr.get(DeepLearningParameters.MissingValuesHandling.MeanImputation) < 4.0);
+    Assert.assertEquals(501.37629982829094, sumErr.get(DeepLearningParameters.MissingValuesHandling.Skip), 1e-2);
+    Assert.assertEquals(sumErr.get(DeepLearningParameters.MissingValuesHandling.MeanImputation), 5.94659155607, 1e-7);
   }
 }
 
diff --git a/h2o-algos/src/test/java/hex/deeplearning/DeepLearningProstateTest.java b/h2o-algos/src/test/java/hex/deeplearning/DeepLearningProstateTest.java
index dce8b35..b686b84 100755
--- a/h2o-algos/src/test/java/hex/deeplearning/DeepLearningProstateTest.java
+++ b/h2o-algos/src/test/java/hex/deeplearning/DeepLearningProstateTest.java
@@ -24,7 +24,7 @@ import java.util.Random;
 import static hex.ConfusionMatrix.buildCM;
 
 public class DeepLearningProstateTest extends TestUtil {
-  @BeforeClass() public static void setup() { stall_till_cloudsize(5); }
+  @BeforeClass() public static void setup() { stall_till_cloudsize(1); }
 
   @Test public void run() throws Exception { runFraction(0.00002f); }
 
diff --git a/h2o-algos/src/test/java/hex/deeplearning/DeepLearningTest.java b/h2o-algos/src/test/java/hex/deeplearning/DeepLearningTest.java
index 618c6a6..7b61eab 100755
--- a/h2o-algos/src/test/java/hex/deeplearning/DeepLearningTest.java
+++ b/h2o-algos/src/test/java/hex/deeplearning/DeepLearningTest.java
@@ -705,10 +705,10 @@ public class DeepLearningTest extends TestUtil {
 
       pred = dl.score(parms.train());
       hex.ModelMetricsBinomial mm = hex.ModelMetricsBinomial.getFromDKV(dl, parms.train());
-      assertEquals(0.7222222222222222, mm.auc_obj()._auc, 1e-8);
+      assertEquals(0.7592592592592592, mm.auc_obj()._auc, 1e-8);
 
       double mse = dl._output._training_metrics.mse();
-      assertEquals(0.31599425403539766, mse, 1e-8); //Note: better results than non-shuffled
+      assertEquals(0.314813341867078, mse, 1e-8); //Note: better results than non-shuffled
 
 //      assertTrue(dl.testJavaScoring(tfr, fr2=dl.score(tfr, 1e-5)); //PUBDEV-1900
       dl.delete();
@@ -784,10 +784,10 @@ public class DeepLearningTest extends TestUtil {
 
       pred = dl.score(parms.train());
       hex.ModelMetricsBinomial mm = hex.ModelMetricsBinomial.getFromDKV(dl, parms.train());
-      assertEquals(0.7777777777777778, mm.auc_obj()._auc, 1e-8);
+      assertEquals(0.7592592592592592, mm.auc_obj()._auc, 1e-8);
 
       double mse = dl._output._training_metrics.mse();
-      assertEquals(0.32223485418125575, mse, 1e-8);
+      assertEquals(0.3116490253190556, mse, 1e-8);
 
 //      Assert.assertTrue(dl.testJavaScoring(tfr,fr2=dl.score(tfr),1e-5)); //PUBDEV-1900
       dl.delete();
@@ -1435,5 +1435,31 @@ public class DeepLearningTest extends TestUtil {
       if (dl2 != null) dl2.delete();
     }
   }
+  @Test
+  public void testCrossValidation() {
+    Frame tfr = null;
+    DeepLearningModel dl = null;
+
+    try {
+      tfr = parse_test_file("./smalldata/gbm_test/BostonHousing.csv");
+      DeepLearningParameters parms = new DeepLearningParameters();
+      parms._train = tfr._key;
+      parms._response_column = tfr.lastVecName();
+      parms._reproducible = true;
+      parms._hidden = new int[]{20,20};
+      parms._seed = 0xdecaf;
+      parms._nfolds = 4;
+
+      dl = new DeepLearning(parms).trainModel().get();
+
+      Assert.assertEquals(dl._output._training_metrics._MSE,12.892871729257042,1e-6);
+      Assert.assertEquals(dl._output._cross_validation_metrics._MSE,17.42844560821736,1e-6);
+
+    } finally {
+      if (tfr != null) tfr.delete();
+      if (dl != null) dl.deleteCrossValidationModels();
+      if (dl != null) dl.delete();
+    }
+  }
 }
 
diff --git a/h2o-algos/src/test/java/hex/glm/GLMBasicTestBinomial.java b/h2o-algos/src/test/java/hex/glm/GLMBasicTestBinomial.java
index c9989c3..42bee16 100644
--- a/h2o-algos/src/test/java/hex/glm/GLMBasicTestBinomial.java
+++ b/h2o-algos/src/test/java/hex/glm/GLMBasicTestBinomial.java
@@ -13,6 +13,7 @@ import water.*;
 import water.exceptions.H2OModelBuilderIllegalArgumentException;
 import water.fvec.*;
 
+import java.util.Arrays;
 import java.util.HashMap;
 
 import static org.junit.Assert.assertEquals;
@@ -418,6 +419,7 @@ public class GLMBasicTestBinomial extends TestUtil {
     params._intercept = false;
     params._objective_epsilon = 0;
     params._gradient_epsilon = 1e-6;
+    params._missing_values_handling = MissingValuesHandling.Skip;
     params._max_iterations = 100; // not expected to reach max iterations here
     for(Solver s:new Solver[]{Solver.AUTO,Solver.IRLSM,Solver.L_BFGS /*, Solver.COORDINATE_DESCENT_NAIVE, Solver.COORDINATE_DESCENT*/}) {
       Frame scoreTrain = null, scoreTest = null;
@@ -440,7 +442,7 @@ public class GLMBasicTestBinomial extends TestUtil {
         // compare validation res dev matches R
         // sum(binomial()$dev.resids(y=test$CAPSULE,mu=p,wt=1))
         // [1]80.92923
-        assertEquals(80.92923, GLMTest.residualDevianceTest(model), CD? 1e-2:1e-4);
+        assertTrue(80.92923 >= GLMTest.residualDevianceTest(model) - 1e-2);
 //      compare validation null dev against R
 //      sum(binomial()$dev.resids(y=test$CAPSULE,mu=.5,wt=1))
 //      [1] 124.7665
@@ -1013,6 +1015,7 @@ public class GLMBasicTestBinomial extends TestUtil {
     params._standardize = false;
     params._train = _prostateTrain._key;
     params._compute_p_values = true;
+    params._missing_values_handling = MissingValuesHandling.Skip;
     params._lambda = new double[]{0};
     GLM job0 = null;
     try {
@@ -1062,6 +1065,7 @@ public class GLMBasicTestBinomial extends TestUtil {
       double[] zvals_expected = new double[]{-2.99223901, 1.24208800, -0.14610616, 0.04428674, -0.46826589, 2.24843259, 3.13779030, 1.44550154, 1.18227779, 2.71377864, -1.11887108, 4.67333842};
       double[] pvals_expected = new double[]{2.769394e-03, 2.142041e-01, 8.838376e-01, 9.646758e-01, 6.395945e-01, 2.454862e-02, 1.702266e-03, 1.483171e-01, 2.370955e-01, 6.652060e-03, 2.631951e-01, 2.963429e-06};
       String[] names_actual = model._output.coefficientNames();
+      System.out.println("names actual = " + Arrays.toString(names_actual));
       HashMap<String, Integer> coefMap = new HashMap<>();
       for (int i = 0; i < names_expected.length; ++i)
         coefMap.put(names_expected[i], i);
diff --git a/h2o-algos/src/test/java/hex/glm/GLMBasicTestRegression.java b/h2o-algos/src/test/java/hex/glm/GLMBasicTestRegression.java
index f123aa3..a1775e7 100644
--- a/h2o-algos/src/test/java/hex/glm/GLMBasicTestRegression.java
+++ b/h2o-algos/src/test/java/hex/glm/GLMBasicTestRegression.java
@@ -1,6 +1,7 @@
 package hex.glm;
 
 import hex.ModelMetricsRegressionGLM;
+import hex.deeplearning.DeepLearningModel;
 import hex.glm.GLMModel.GLMParameters;
 import hex.glm.GLMModel.GLMParameters.Family;
 import hex.glm.GLMModel.GLMParameters.Solver;
@@ -414,6 +415,7 @@ public class GLMBasicTestRegression extends TestUtil {
     parms._response_column = "Infections";
     parms._compute_p_values = true;
     parms._objective_epsilon = 0;
+    parms._missing_values_handling = DeepLearningModel.DeepLearningParameters.MissingValuesHandling.Skip;
 
     GLMModel model = null;
     try {
@@ -473,6 +475,7 @@ public class GLMBasicTestRegression extends TestUtil {
     parms._alpha = new double[]{0};
     parms._response_column = "Claims";
     parms._compute_p_values = true;
+    parms._missing_values_handling = DeepLearningModel.DeepLearningParameters.MissingValuesHandling.Skip;
 
     GLMModel model = null;
     try {
@@ -539,6 +542,7 @@ public class GLMBasicTestRegression extends TestUtil {
     params._train = _prostateTrain._key;
     params._compute_p_values = true;
     params._lambda = new double[]{0};
+    params._missing_values_handling = DeepLearningModel.DeepLearningParameters.MissingValuesHandling.Skip;
     try {
       params._solver = Solver.L_BFGS;
       new GLM(params).trainModel().get();
diff --git a/h2o-algos/src/test/java/hex/glm/GLMTest.java b/h2o-algos/src/test/java/hex/glm/GLMTest.java
index 1368412..deb1868 100644
--- a/h2o-algos/src/test/java/hex/glm/GLMTest.java
+++ b/h2o-algos/src/test/java/hex/glm/GLMTest.java
@@ -1397,9 +1397,11 @@ public class GLMTest  extends TestUtil {
       params._train = fr._key;
       params._lambda = new double[]{0};
       params._standardize = false;
+//      params._missing_values_handling = MissingValuesHandling.Skip;
       GLM glm = new GLM(params,glmkey("prostate_model"));
       model = glm.trainModel().get();
       HashMap<String, Double> coefs = model.coefficients();
+      System.out.println(coefs);
       for(int i = 0; i < cfs1.length; ++i)
         assertEquals(vals[i], coefs.get(cfs1[i]),1e-4);
       assertEquals(512.3, nullDeviance(model),1e-1);
diff --git a/h2o-algos/src/test/java/water/ModelSerializationTest.java b/h2o-algos/src/test/java/water/ModelSerializationTest.java
index 0729eba..8e8140c 100644
--- a/h2o-algos/src/test/java/water/ModelSerializationTest.java
+++ b/h2o-algos/src/test/java/water/ModelSerializationTest.java
@@ -185,6 +185,7 @@ public class ModelSerializationTest extends TestUtil {
       public String algoName() { return "Blah"; }
       public String fullName() { return "Blah"; }
       public String javaName() { return BlahModel.class.getName(); }
+      @Override public long progressUnits() { return 0; }
     }
     static class BlahOutput extends Model.Output {
       public BlahOutput(boolean hasWeights, boolean hasOffset, boolean hasFold) {
diff --git a/h2o-algos/testMultiNode.sh b/h2o-algos/testMultiNode.sh
index b0ff1bc..028e03a 100755
--- a/h2o-algos/testMultiNode.sh
+++ b/h2o-algos/testMultiNode.sh
@@ -1,5 +1,13 @@
 #!/bin/bash
 
+# Argument parsing
+if [ "$1" = "jacoco" ]
+then
+    JACOCO_ENABLED=true
+else
+    JACOCO_ENABLED=false
+fi
+
 # Clean out any old sandbox, make a new one
 OUTDIR=sandbox
 rm -fr $OUTDIR; mkdir -p $OUTDIR
@@ -44,7 +52,19 @@ fi
 #   build/classes/main - Main h2o core classes
 #   build/classes/test - Test h2o core classes
 #   build/resources/main - Main resources (e.g. page.html)
-JVM="nice $JAVA_CMD -ea -Xmx3g -Xms3g -cp build/libs/h2o-algos-test.jar${SEP}build/libs/h2o-algos.jar${SEP}../h2o-core/build/libs/h2o-core-test.jar${SEP}../h2o-core/build/libs/h2o-core.jar${SEP}../h2o-genmodel/build/libs/h2o-genmodel.jar${SEP}../lib/*"
+
+MAX_MEM="-Xmx3g"
+
+# Check if coverage should be run
+if [ $JACOCO_ENABLED = true ]
+then
+    AGENT="../jacoco/jacocoagent.jar"
+    COVERAGE="-javaagent:$AGENT=destfile=build/jacoco/h2o-algos.exec"
+    MAX_MEM="-Xmx8g"
+else
+    COVERAGE=""
+fi
+JVM="nice $JAVA_CMD $COVERAGE -ea $MAX_MEM -Xms3g -cp build/libs/h2o-algos-test.jar${SEP}build/libs/h2o-algos.jar${SEP}../h2o-core/build/libs/h2o-core-test.jar${SEP}../h2o-core/build/libs/h2o-core.jar${SEP}../h2o-genmodel/build/libs/h2o-genmodel.jar${SEP}../lib/*"
 echo "$JVM" > $OUTDIR/jvm_cmd.txt
 # Ahhh... but the makefile runs the tests skipping the jar'ing step when possible.
 # Also, sometimes see test files in the main-class directory, so put the test
@@ -80,9 +100,17 @@ $JVM water.H2O -name $CLUSTER_NAME -baseport $CLUSTER_BASEPORT -ga_opt_out 1> $O
 $JVM water.H2O -name $CLUSTER_NAME -baseport $CLUSTER_BASEPORT -ga_opt_out 1> $OUTDIR/out.3 2>&1 & PID_3=$!
 $JVM water.H2O -name $CLUSTER_NAME -baseport $CLUSTER_BASEPORT -ga_opt_out 1> $OUTDIR/out.4 2>&1 & PID_4=$!
 
+# If coverage is being run, then pass a system variable flag so that timeout limits are increased.
+if [ $JACOCO_ENABLED = true ]
+then
+    JACOCO_FLAG="-Dtest.jacocoEnabled=true"
+else
+    JACOCO_FLAG=""
+fi
+
 # Launch last driver JVM.  All output redir'd at the OS level to sandbox files.
 echo Running h2o-algos junit tests...
-($JVM -Dai.h2o.name=$CLUSTER_NAME -Dai.h2o.baseport=$CLUSTER_BASEPORT -Dai.h2o.ga_opt_out=yes $JUNIT_RUNNER $JUNIT_TESTS_BOOT `cat $OUTDIR/tests.txt` 2>&1 ; echo $? > $OUTDIR/status.0) 1> $OUTDIR/out.0 2>&1
+($JVM -Dai.h2o.name=$CLUSTER_NAME -Dai.h2o.baseport=$CLUSTER_BASEPORT -Dai.h2o.ga_opt_out=yes $JACOCO_FLAG $JUNIT_RUNNER $JUNIT_TESTS_BOOT `cat $OUTDIR/tests.txt` 2>&1 ; echo $? > $OUTDIR/status.0) 1> $OUTDIR/out.0 2>&1
 
 grep EXECUTION $OUTDIR/out.0 | cut "-d " -f22,19 | awk '{print $2 " " $1}'| sort -gr | head -n 10 >> $OUTDIR/out.0
 
diff --git a/h2o-assembly/build.gradle b/h2o-assembly/build.gradle
index e91c3e9..d7a7fa8 100644
--- a/h2o-assembly/build.gradle
+++ b/h2o-assembly/build.gradle
@@ -13,7 +13,9 @@ shadowJar {
   mergeServiceFiles()
   classifier = ''
   // CDH 5.3.0 provides joda-time v1.6 which is too old, shadow the library instead
-  relocate 'org.joda.time', 'ai.h2o.org.joda.time'
+  if (!project.hasProperty("jacocoCoverage")) {
+    relocate 'org.joda.time', 'ai.h2o.org.joda.time'
+  }
   exclude 'META-INF/*.DSA'
   exclude 'META-INF/*.SF'
   exclude 'synchronize.properties'
diff --git a/h2o-core/src/main/java/hex/Distribution.java b/h2o-core/src/main/java/hex/Distribution.java
index cd02b90..f4976a5 100644
--- a/h2o-core/src/main/java/hex/Distribution.java
+++ b/h2o-core/src/main/java/hex/Distribution.java
@@ -2,7 +2,6 @@ package hex;
 
 import water.H2O;
 import water.Iced;
-import water.util.Log;
 
 /**
  * Distribution functions to be used by ML Algos
@@ -13,34 +12,33 @@ public class Distribution extends Iced {
     AUTO,         //model-specific behavior
     bernoulli,    //binomial classification (nclasses == 2)
     multinomial,  //classification (nclasses >= 2)
-    gaussian, poisson, gamma, tweedie, huber, laplace //regression
+    gaussian, poisson, gamma, tweedie, huber, laplace, quantile //regression
   }
 
-  /**
-   * Short constructor for non-Tweedie distributions
-   * @param distribution
-   */
-  public Distribution(Family distribution) {
-    assert(distribution != Family.tweedie);
-    this.distribution = distribution;
-    this.tweediePower = 0;
+  // Default constructor for non-Tweedie and non-Quantile families
+  public Distribution(Family family) {
+    distribution = family;
+    assert(family != Family.tweedie);
+    assert(family != Family.quantile);
+    tweediePower = 1.5;
+    quantileAlpha = 0.5;
   }
 
   /**
-   * Constructor to be used for Tweedie (and if uncertain)
-   * @param distribution
-   * @param tweediePower Tweedie Power
+   * @param params
    */
-  public Distribution(Family distribution, double tweediePower) {
-    this.distribution = distribution;
+  public Distribution(Model.Parameters params) {
+    distribution = params._distribution;
+    tweediePower = params._tweedie_power;
+    quantileAlpha = params._quantile_alpha;
     assert(tweediePower >1 && tweediePower <2);
-    this.tweediePower = tweediePower;
   }
   static public double MIN_LOG = -19;
   static public double MAX = 1e19;
 
   public final Family distribution;
   public final double tweediePower; //tweedie power
+  public final double quantileAlpha; //for quantile regression
 
   // helper - sanitized exponential function
   public static double exp(double x) {
@@ -83,6 +81,8 @@ public class Distribution extends Iced {
         }
       case laplace:
         return w * Math.abs(y-f); // weighted absolute deviance == weighted absolute error
+      case quantile:
+        return y > f ? w*quantileAlpha*(y-f) : w*(1-quantileAlpha)*(f-y);
       case bernoulli:
         return -2 * w * (y * f - log(1 + exp(f)));
       case poisson:
@@ -123,6 +123,8 @@ public class Distribution extends Iced {
         }
       case laplace:
         return f > y ? -1 : 1;
+      case quantile:
+        return y > f ? quantileAlpha : quantileAlpha-1;
       default:
         throw H2O.unimpl();
     }
@@ -139,6 +141,7 @@ public class Distribution extends Iced {
       case gaussian:
       case huber:
       case laplace:
+      case quantile:
         return f;
       case bernoulli:
         return log(f/(1-f));
@@ -163,6 +166,7 @@ public class Distribution extends Iced {
       case gaussian:
       case huber:
       case laplace:
+      case quantile:
         return f;
       case bernoulli:
         return 1 / (1 + exp(-f));
@@ -187,6 +191,7 @@ public class Distribution extends Iced {
       case gaussian:
       case huber:
       case laplace:
+      case quantile:
         return f;
       case bernoulli:
         return "1/(1+" + expString("-" + f) + ")";
diff --git a/h2o-core/src/main/java/hex/Model.java b/h2o-core/src/main/java/hex/Model.java
index 42a6659..bed6a28 100755
--- a/h2o-core/src/main/java/hex/Model.java
+++ b/h2o-core/src/main/java/hex/Model.java
@@ -98,8 +98,10 @@ public abstract class Model<M extends Model<M,P,O>, P extends Model.Parameters,
     protected long nFoldSeed() { return new Random().nextLong(); }
     public FoldAssignmentScheme _fold_assignment = FoldAssignmentScheme.AUTO;
     public Distribution.Family _distribution = Distribution.Family.AUTO;
-    public double _tweedie_power = 1.5f;
+    public double _tweedie_power = 1.5;
+    public double _quantile_alpha = 0.5;
     protected double defaultStoppingTolerance() { return 1e-3; }
+    abstract public long progressUnits();
 
     // TODO: This field belongs in the front-end column-selection process and
     // NOT in the parameters - because this requires all model-builders to have
@@ -839,7 +841,6 @@ public abstract class Model<M extends Model<M,P,O>, P extends Model.Parameters,
       }
     }
     @Override public void reduce( BigScore bs ) { if(_mb != null)_mb.reduce(bs._mb); }
-
     @Override protected void postGlobal() { if(_mb != null)_mb.postGlobal(); }
   }
 
@@ -850,6 +851,7 @@ public abstract class Model<M extends Model<M,P,O>, P extends Model.Parameters,
   public double[] score0( Chunk chks[], int row_in_chunk, double[] tmp, double[] preds ) {
     return score0(chks, 1, 0, row_in_chunk, tmp, preds);
   }
+
   public double[] score0( Chunk chks[], double weight, double offset, int row_in_chunk, double[] tmp, double[] preds ) {
     assert(_output.nfeatures() == tmp.length);
     for( int i=0; i< tmp.length; i++ )
diff --git a/h2o-core/src/main/java/hex/ModelBuilder.java b/h2o-core/src/main/java/hex/ModelBuilder.java
index 2867124..ec1e686 100644
--- a/h2o-core/src/main/java/hex/ModelBuilder.java
+++ b/h2o-core/src/main/java/hex/ModelBuilder.java
@@ -161,7 +161,7 @@ abstract public class ModelBuilder<M extends Model<M,P,O>, P extends Model.Param
       throw H2OModelBuilderIllegalArgumentException.makeFromBuilder(this);
     _start_time = System.currentTimeMillis();
     if( !nFoldCV() )
-      return _job.start(trainModelImpl(), progressUnits());
+      return _job.start(trainModelImpl(), _parms.progressUnits());
 
     // cross-validation needs to be forked off to allow continuous (non-blocking) progress bar
     return _job.start(new H2O.H2OCountedCompleter() {
@@ -170,7 +170,7 @@ abstract public class ModelBuilder<M extends Model<M,P,O>, P extends Model.Param
           computeCrossValidation();
           tryComplete();
         }
-      }, (1/*for all pre-fold work*/+nFoldWork()+1/*for all the post-fold work*/) * progressUnits());
+      }, (1/*for all pre-fold work*/+nFoldWork()+1/*for all the post-fold work*/) * _parms.progressUnits());
   }
 
   /** Train a model as part of a larger Job; the Job already exists and has started. */
@@ -186,7 +186,6 @@ abstract public class ModelBuilder<M extends Model<M,P,O>, P extends Model.Param
   /** Model-specific implementation of model training
    * @return A F/J Job, which, when executed, does the build.  F/J is NOT started.  */
   abstract protected Driver trainModelImpl();
-  abstract protected long progressUnits();
 
   /**
    * How many should be trained in parallel during N-fold cross-validation?
@@ -442,7 +441,7 @@ abstract public class ModelBuilder<M extends Model<M,P,O>, P extends Model.Param
     }
     mainModel._output._cross_validation_metrics = mbs[0].makeModelMetrics(mainModel, _parms.train(), null, preds);
     if (preds!=null) preds.remove();
-    mainModel._output._cross_validation_metrics._description = N + "-fold cross-validation on training data";
+    mainModel._output._cross_validation_metrics._description = N + "-fold cross-validation on training data (Metrics computed for combined holdout predictions)";
     Log.info(mainModel._output._cross_validation_metrics.toString());
 
     // Now, the main model is complete (has cv metrics)
@@ -587,11 +586,11 @@ abstract public class ModelBuilder<M extends Model<M,P,O>, P extends Model.Param
           error("_response_column", "Response column '" + _parms._response_column + "' not found in the training frame");
       } else {
         if(_response == _offset)
-          error("_response", "Response must be different from offset_column");
+          error("_response_column", "Response column must be different from offset_column");
         if(_response == _weights)
-          error("_response", "Response must be different from weights_column");
+          error("_response_column", "Response column must be different from weights_column");
         if(_response == _fold)
-          error("_response", "Response must be different from fold_column");
+          error("_response_column", "Response column must be different from fold_column");
         _train.add(_parms._response_column, _response);
         ++res;
       }
@@ -926,6 +925,12 @@ abstract public class ModelBuilder<M extends Model<M,P,O>, P extends Model.Param
   }
 
   public void checkDistributions() {
+    if (_parms._distribution != Distribution.Family.tweedie) {
+      hide("_tweedie_power", "Tweedie power is only used for Tweedie distribution.");
+    }
+    if (_parms._distribution != Distribution.Family.quantile) {
+      hide("_quantile_alpha", "Quantile (alpha) is only used for Quantile regression.");
+    }
     if (_parms._distribution == Distribution.Family.poisson) {
       if (_response.min() < 0)
         error("_response", "Response must be non-negative for Poisson distribution.");
@@ -937,6 +942,9 @@ abstract public class ModelBuilder<M extends Model<M,P,O>, P extends Model.Param
         error("_tweedie_power", "Tweedie power must be between 1 and 2.");
       if (_response.min() < 0)
         error("_response", "Response must be non-negative for Tweedie distribution.");
+    } else if (_parms._distribution == Distribution.Family.quantile) {
+      if (_parms._quantile_alpha > 1 || _parms._quantile_alpha < 0)
+        error("_quantile_alpha", "Quantile (alpha) must be between 0 and 1.");
     }
   }
 
diff --git a/h2o-core/src/main/java/hex/grid/GridSearch.java b/h2o-core/src/main/java/hex/grid/GridSearch.java
index 330b767..661c82c 100644
--- a/h2o-core/src/main/java/hex/grid/GridSearch.java
+++ b/h2o-core/src/main/java/hex/grid/GridSearch.java
@@ -68,13 +68,12 @@ import java.util.Map;
  * @see #startGridSearch(Key, HyperSpaceWalker)
  */
 public final class GridSearch<MP extends Model.Parameters> extends Keyed<GridSearch> {
-  public enum Strategy { Unknown, Cartesian, Random } // search strategy
   public final Key<Grid> _result;
   public final Job<Grid> _job;
 
   /** Walks hyper space and for each point produces model parameters. It is
    *  used only locally to fire new model builders.  */
-  private final transient HyperSpaceWalker<MP> _hyperSpaceWalker;
+  private final transient HyperSpaceWalker<MP, ?> _hyperSpaceWalker;
 
   /** For advanced search methods we can put a time limit on the overall grid search.  This doesn't make much sense
    * for strict Cartesian.
@@ -82,16 +81,12 @@ public final class GridSearch<MP extends Model.Parameters> extends Keyed<GridSea
   private int _max_time_ms = Integer.MAX_VALUE;
 
 
-  private GridSearch(Key<Grid> gkey, HyperSpaceWalker<MP> hyperSpaceWalker) {
+  private GridSearch(Key<Grid> gkey, HyperSpaceWalker<MP, ?> hyperSpaceWalker) {
     _result = gkey;
     String algoName = hyperSpaceWalker.getParams().algoName();
     _job = new Job<>(gkey, Grid.class.getName(), algoName + " Grid Search");
     assert hyperSpaceWalker != null : "Grid search needs to know to how walk around hyper space!";
     _hyperSpaceWalker = hyperSpaceWalker;
-    // TODO: hacky: when we have SearchCriteria classes pass an instance down through the startGridSearch chain into this constructor.
-    if (_hyperSpaceWalker instanceof HyperSpaceWalker.RandomDiscreteValueWalker) {
-      this._max_time_ms = ((HyperSpaceWalker.RandomDiscreteValueWalker)_hyperSpaceWalker).max_time_ms();
-    }
     // Note: do not validate parameters of created model builders here!
     // Leave it to launch time, and just mark the corresponding model builder job as failed.
   }
@@ -122,13 +117,31 @@ public final class GridSearch<MP extends Model.Parameters> extends Keyed<GridSea
                      _hyperSpaceWalker.getParametersBuilderFactory().getFieldNamingStrategy());
       grid.delete_and_lock(_job);
     }
+
+    Model model = null;
+    HyperSpaceWalker.HyperSpaceIterator<MP> it = _hyperSpaceWalker.iterator();
+    long gridWork=0;
+    if (gridSize > 0) {//if total grid space is known, walk it all and count up models to be built (not subject to time-based or converge-based early stopping)
+      while (it.hasNext(model)) {
+        try {
+          gridWork += it.nextModelParameters(model).progressUnits();
+        } catch(Throwable ex) {
+          //swallow invalid combinations
+        }
+      }
+    } else {
+      //TODO: Future totally unbounded search: need a time-based progress bar
+      gridWork = Long.MAX_VALUE;
+    }
+    it.reset();
+
     // Install this as job functions
     return _job.start(new H2O.H2OCountedCompleter() {
       @Override public void compute2() {
         gridSearch(grid);
         tryComplete();
       }
-    }, gridSize);
+    }, gridWork);
   }
 
   /**
@@ -335,22 +348,9 @@ public final class GridSearch<MP extends Model.Parameters> extends Keyed<GridSea
       final MP params,
       final Map<String, Object[]> hyperParams,
       final ModelParametersBuilderFactory<MP> paramsBuilderFactory,
-      final Strategy strategy,
-      final int max_models,
-      final int max_time_ms,
-      long seed) {
-
-    // Create a walker to traverse the hyper space of model parameters.
-    // TODO: encapsulate this switch in a factory to make it pluggable.
-    BaseWalker<MP> hyperSpaceWalker;
-    if (strategy == Strategy.Cartesian)
-      hyperSpaceWalker = new HyperSpaceWalker.CartesianWalker<>(params, hyperParams, paramsBuilderFactory);
-    else if (strategy == Strategy.Random)
-      hyperSpaceWalker = new HyperSpaceWalker.RandomDiscreteValueWalker<>(params, hyperParams, paramsBuilderFactory, max_models, max_time_ms, seed);
-    else
-      throw new H2OIllegalArgumentException("strategy", "GridSearch", strategy);
-
-    return startGridSearch(destKey, hyperSpaceWalker);
+      final HyperSpaceSearchCriteria search_criteria) {
+
+    return startGridSearch(destKey, BaseWalker.WalkerFactory.create(params, hyperParams, paramsBuilderFactory, search_criteria));
   }
 
 
@@ -369,13 +369,17 @@ public final class GridSearch<MP extends Model.Parameters> extends Keyed<GridSea
    * an expensive operation.  If the models in question are "in progress", a 2nd build will NOT be
    * kicked off.  This is a non-blocking call.
    *
-   * @see #startGridSearch(Key, Model.Parameters, Map, ModelParametersBuilderFactory, Strategy, int, int, long)
+   * @see #startGridSearch(Key, Model.Parameters, Map, ModelParametersBuilderFactory, HyperSpaceSearchCriteria)
    */
   public static <MP extends Model.Parameters> Job<Grid> startGridSearch(final Key<Grid> destKey,
                                                                         final MP params,
                                                                         final Map<String, Object[]> hyperParams) {
-    return startGridSearch(destKey, params, hyperParams, new SimpleParametersBuilderFactory<MP>(),
-            Strategy.Cartesian, -1, -1, -1L);
+    return startGridSearch(
+            destKey,
+            params,
+            hyperParams,
+            new SimpleParametersBuilderFactory<MP>(),
+            new HyperSpaceSearchCriteria.CartesianSearchCriteria());
   }
 
   /**
@@ -391,7 +395,7 @@ public final class GridSearch<MP extends Model.Parameters> extends Keyed<GridSea
    */
   public static <MP extends Model.Parameters> Job<Grid> startGridSearch(
       final Key<Grid> destKey,
-      final HyperSpaceWalker<MP> hyperSpaceWalker) {
+      final HyperSpaceWalker<MP, ?> hyperSpaceWalker) {
     // Compute key for destination object representing grid
     MP params = hyperSpaceWalker.getParams();
     Key<Grid> gridKey = destKey != null ? destKey
diff --git a/h2o-core/src/main/java/hex/grid/HyperSpaceSearchCriteria.java b/h2o-core/src/main/java/hex/grid/HyperSpaceSearchCriteria.java
new file mode 100644
index 0000000..812aa94
--- /dev/null
+++ b/h2o-core/src/main/java/hex/grid/HyperSpaceSearchCriteria.java
@@ -0,0 +1,49 @@
+package hex.grid;
+
+import water.Iced;
+
+/**
+ * Search criteria for a hyperparameter search including directives for how to search and
+ * when to stop the search.
+ */
+public class HyperSpaceSearchCriteria extends Iced {
+  public enum Strategy { Unknown, Cartesian, RandomDiscrete } // search strategy
+
+  public final Strategy _strategy;
+  public final Strategy strategy() { return _strategy; }
+
+// TODO: add a factory which accepts a Strategy and calls the right constructor
+
+  public HyperSpaceSearchCriteria(Strategy strategy) {
+    this._strategy = strategy;
+  }
+
+  /**
+   * Search criteria for an exhaustive Cartesian hyperparameter search.
+   */
+  public static final class CartesianSearchCriteria extends HyperSpaceSearchCriteria {
+    public CartesianSearchCriteria() {
+      super(Strategy.Cartesian);
+    }
+  }
+
+  /**
+   * Search criteria for a hyperparameter search including directives for how to search and
+   * when to stop the search.
+   */
+  public static final class RandomDiscreteValueSearchCriteria extends HyperSpaceSearchCriteria {
+    private long _seed = -1;
+
+    // stopping criteria:
+    private int _max_models = Integer.MAX_VALUE;
+    private int _max_time_ms = Integer.MAX_VALUE;
+
+    public long seed() { return _seed; }
+    public int max_models() { return _max_models; }
+    public int max_time_ms() { return _max_time_ms; }
+
+    public RandomDiscreteValueSearchCriteria() {
+      super(Strategy.RandomDiscrete);
+    }
+  }
+}
diff --git a/h2o-core/src/main/java/hex/grid/HyperSpaceWalker.java b/h2o-core/src/main/java/hex/grid/HyperSpaceWalker.java
index 99c1055..01a88a5 100644
--- a/h2o-core/src/main/java/hex/grid/HyperSpaceWalker.java
+++ b/h2o-core/src/main/java/hex/grid/HyperSpaceWalker.java
@@ -2,11 +2,12 @@ package hex.grid;
 
 import hex.Model;
 import hex.ModelParametersBuilderFactory;
+import water.exceptions.H2OIllegalArgumentException;
 import water.util.Log;
 
 import java.util.*;
 
-public interface HyperSpaceWalker<MP extends Model.Parameters> {
+public interface HyperSpaceWalker<MP extends Model.Parameters, C extends HyperSpaceSearchCriteria> {
 
   interface HyperSpaceIterator<MP extends Model.Parameters> {
     /**
@@ -35,6 +36,8 @@ public interface HyperSpaceWalker<MP extends Model.Parameters> {
      */
     boolean hasNext(Model previousModel);
 
+    void reset();
+
     long timeRemaining();
 
     /**
@@ -51,7 +54,13 @@ public interface HyperSpaceWalker<MP extends Model.Parameters> {
      * @return  array of "untyped" values representing configuration of grid parameters
      */
     Object[] getCurrentRawParameters();
-  }
+  } // interface HyperSpaceIterator
+
+  /**
+   * Search criteria for the hyperparameter search including directives for how to search and
+   * when to stop the search.
+   */
+  public C search_criteria();
 
   /**
    * Returns an iterator to traverse this hyper-space.
@@ -93,7 +102,18 @@ public interface HyperSpaceWalker<MP extends Model.Parameters> {
    * values, where the String is a valid field name in the corresponding Model.Parameter, and the Object is
    * the field value (boxed as needed).
    */
-  abstract class BaseWalker<MP extends Model.Parameters> implements HyperSpaceWalker<MP> {
+  abstract class BaseWalker<MP extends Model.Parameters, C extends HyperSpaceSearchCriteria> implements HyperSpaceWalker<MP, C> {
+
+    /**
+     * @see #search_criteria()
+     */
+    final protected C _search_criteria;
+
+    /**
+     * Search criteria for the hyperparameter search including directives for how to search and
+     * when to stop the search.
+     */
+    public C search_criteria() { return _search_criteria; }
 
     /**
      * Parameters builder factory to create new instance of parameters.
@@ -123,18 +143,43 @@ public interface HyperSpaceWalker<MP extends Model.Parameters> {
     final protected int _maxHyperSpaceSize;
 
     /**
+     * Java hackery so we can have a factory method on a class with type params.
+     */
+    public static class WalkerFactory<MP extends Model.Parameters, C extends HyperSpaceSearchCriteria> {
+      /**
+       * Factory method to create an instance based on the given HyperSpaceSearchCriteria instance.
+       */
+      public static <MP extends Model.Parameters, C extends HyperSpaceSearchCriteria>
+        HyperSpaceWalker create(MP params,
+                                              Map<String, Object[]> hyperParams,
+                                            ModelParametersBuilderFactory<MP> paramsBuilderFactory,
+                                            C search_criteria) {
+        HyperSpaceSearchCriteria.Strategy strategy = search_criteria.strategy();
+
+        if (strategy == HyperSpaceSearchCriteria.Strategy.Cartesian)
+          return new HyperSpaceWalker.CartesianWalker<>(params, hyperParams, paramsBuilderFactory, (HyperSpaceSearchCriteria.CartesianSearchCriteria) search_criteria);
+        else if (strategy == HyperSpaceSearchCriteria.Strategy.RandomDiscrete )
+          return new HyperSpaceWalker.RandomDiscreteValueWalker<>(params, hyperParams, paramsBuilderFactory, (HyperSpaceSearchCriteria.RandomDiscreteValueSearchCriteria) search_criteria);
+        else
+          throw new H2OIllegalArgumentException("strategy", "GridSearch", strategy);
+      }
+    }
+
+    /**
      *
      * @param paramsBuilderFactory
      * @param hyperParams
      */
     public BaseWalker(MP params,
-                           Map<String, Object[]> hyperParams,
-                           ModelParametersBuilderFactory<MP> paramsBuilderFactory) {
+                      Map<String, Object[]> hyperParams,
+                      ModelParametersBuilderFactory<MP> paramsBuilderFactory,
+                      C search_criteria) {
       _params = params;
       _hyperParams = hyperParams;
       _paramsBuilderFactory = paramsBuilderFactory;
       _hyperParamNames = hyperParams.keySet().toArray(new String[0]);
       _maxHyperSpaceSize = computeMaxSizeOfHyperSpace();
+      _search_criteria = search_criteria;
     }
 
     @Override
@@ -198,12 +243,13 @@ public interface HyperSpaceWalker<MP extends Model.Parameters> {
    * Hyperparameter space walker which visits each combination of hyperparameters in order.
    */
   public static class CartesianWalker<MP extends Model.Parameters>
-          extends BaseWalker<MP> {
+          extends BaseWalker<MP, HyperSpaceSearchCriteria.CartesianSearchCriteria> {
 
     public CartesianWalker(MP params,
-                      Map<String, Object[]> hyperParams,
-                      ModelParametersBuilderFactory<MP> paramsBuilderFactory) {
-      super(params, hyperParams, paramsBuilderFactory);
+                           Map<String, Object[]> hyperParams,
+                           ModelParametersBuilderFactory<MP> paramsBuilderFactory,
+                           HyperSpaceSearchCriteria.CartesianSearchCriteria search_criteria) {
+      super(params, hyperParams, paramsBuilderFactory, search_criteria);
     }
 
     @Override
@@ -245,6 +291,10 @@ public interface HyperSpaceWalker<MP extends Model.Parameters> {
           return false;
         }
 
+        @Override public void reset() {
+          _currentHyperparamIndices = null;
+        }
+
         @Override
         public long timeRemaining() { return Long.MAX_VALUE; }
 
@@ -291,14 +341,9 @@ public interface HyperSpaceWalker<MP extends Model.Parameters> {
    * given in explicit lists as they are with CartesianWalker.
    */
   public static class RandomDiscreteValueWalker<MP extends Model.Parameters>
-      extends BaseWalker<MP> {
+      extends BaseWalker<MP, HyperSpaceSearchCriteria.RandomDiscreteValueSearchCriteria> {
     Random random;
 
-    // stopping criteria:
-    private int _max_models;
-    private int _max_time_ms;
-    public int max_time_ms() { return _max_time_ms; }
-
     /** All visited hyper params permutations, including the current one. */
     private List<int[]> _visitedPermutations = new ArrayList<>();
     private Set<Integer> _visitedPermutationHashes = new LinkedHashSet<>(); // for fast dupe lookup
@@ -306,13 +351,9 @@ public interface HyperSpaceWalker<MP extends Model.Parameters> {
     public RandomDiscreteValueWalker(MP params,
                                      Map<String, Object[]> hyperParams,
                                      ModelParametersBuilderFactory<MP> paramsBuilderFactory,
-                                     int max_models,
-                                     int max_time_ms,
-                                     long seed) {
-      super(params, hyperParams, paramsBuilderFactory);
-      random = new Random(seed); // TODO: allow the user to set the seed
-      this._max_models = max_models;
-      this._max_time_ms = max_time_ms;
+                                     HyperSpaceSearchCriteria.RandomDiscreteValueSearchCriteria search_criteria) {
+      super(params, hyperParams, paramsBuilderFactory, search_criteria);
+      random = new Random(search_criteria.seed());
     }
 
     @Override
@@ -346,8 +387,6 @@ public interface HyperSpaceWalker<MP extends Model.Parameters> {
             MP commonModelParams = (MP) _params.clone();
             // Fill model parameters
             MP params = getModelParams(commonModelParams, hypers);
-            // We have another model parameters
-            Log.info("About to build model: " + _currentPermutationNum);
             return params;
           } else {
             throw new NoSuchElementException("No more elements to explore in hyper-space!");
@@ -357,12 +396,20 @@ public interface HyperSpaceWalker<MP extends Model.Parameters> {
         @Override
         public boolean hasNext(Model previousModel) {
           // _currentPermutationNum is 1-based
-          return _currentPermutationNum < _maxHyperSpaceSize && _currentPermutationNum < _max_models;
+          return _currentPermutationNum < _maxHyperSpaceSize && _currentPermutationNum < search_criteria().max_models();
+        }
+
+        @Override
+        public void reset() {
+          _currentPermutationNum = 0;
+          _currentHyperparamIndices = null;
+          _visitedPermutations.clear();
+          _visitedPermutationHashes.clear();
         }
 
         @Override
         public long timeRemaining() {
-          return _max_time_ms - (System.currentTimeMillis() - _start_time);
+          return search_criteria().max_time_ms() - (System.currentTimeMillis() - _start_time);
         }
 
         @Override
diff --git a/h2o-core/src/main/java/hex/quantile/Quantile.java b/h2o-core/src/main/java/hex/quantile/Quantile.java
index f49f39d..28b9c16 100755
--- a/h2o-core/src/main/java/hex/quantile/Quantile.java
+++ b/h2o-core/src/main/java/hex/quantile/Quantile.java
@@ -21,7 +21,6 @@ public class Quantile extends ModelBuilder<QuantileModel,QuantileModel.QuantileP
   public Quantile( QuantileModel.QuantileParameters parms ) { super(parms); init(false); }
   public Quantile( QuantileModel.QuantileParameters parms, Job job ) { super(parms, job); init(false); }
   @Override public Driver trainModelImpl() { return new QuantileDriver(); }
-  @Override public long progressUnits() { return train().numCols()*_parms._probs.length; }
   @Override public ModelCategory[] can_build() { return new ModelCategory[]{ModelCategory.Unknown}; }
   // any number of chunks is fine - don't rebalance - it's not worth it for a few passes over the data (at most)
   @Override protected int desiredChunks(final Frame original_fr, boolean local) { return 1;  }
diff --git a/h2o-core/src/main/java/hex/quantile/QuantileModel.java b/h2o-core/src/main/java/hex/quantile/QuantileModel.java
index 219d171..99c65b9 100644
--- a/h2o-core/src/main/java/hex/quantile/QuantileModel.java
+++ b/h2o-core/src/main/java/hex/quantile/QuantileModel.java
@@ -17,6 +17,7 @@ public class QuantileModel extends Model<QuantileModel,QuantileModel.QuantilePar
     public String algoName() { return "Quantiles"; }
     public String fullName() { return "Quantiles"; }
     public String javaName() { return QuantileModel.class.getName(); }
+    @Override public long progressUnits() { return train().numCols()*_probs.length; }
   }
 
   public static class QuantileOutput extends Model.Output {
diff --git a/h2o-core/src/main/java/hex/schemas/GridSearchSchema.java b/h2o-core/src/main/java/hex/schemas/GridSearchSchema.java
index 8101333..1729063 100644
--- a/h2o-core/src/main/java/hex/schemas/GridSearchSchema.java
+++ b/h2o-core/src/main/java/hex/schemas/GridSearchSchema.java
@@ -2,8 +2,6 @@ package hex.schemas;
 
 import hex.Model;
 import hex.grid.Grid;
-import hex.grid.GridSearch;
-import hex.grid.GridSearch.Strategy;
 import water.H2O;
 import water.Key;
 import water.api.*;
@@ -13,7 +11,6 @@ import water.util.IcedHashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.Properties;
-import java.util.Random;
 
 /**
  * This is a common grid search schema composed of two parameters: default parameters for a builder
@@ -44,17 +41,8 @@ public class GridSearchSchema<G extends Grid<MP>,
   @API(help = "Destination id for this grid; auto-generated if not specified.", required = false, direction = API.Direction.INOUT)
   public KeyV3.GridKeyV3 grid_id;
 
-  @API(help="Hyperparameter search strategy, either \"Cartesian\" or \"Random\".", required = false, values = { "Cartesian", "Random" }, direction = API.Direction.INOUT)
-  public Strategy strategy = Strategy.Cartesian;
-
-  @API(help="Maximum number of models to build.", required = false, direction = API.Direction.INOUT)
-  public int max_models = Integer.MAX_VALUE;
-
-  @API(help="Maximum time to spend building models, in mS.  The highest possible is ~24.855 days.", required = false, direction = API.Direction.INOUT)
-  public int max_time_ms = Integer.MAX_VALUE;
-
-  @API(help="Seed for randomized search criteria.", required = false, direction = API.Direction.INOUT)
-  public long seed = (new Random().nextLong());
+  @API(help="Hyperparameter search criteria, including strategy and early stopping directives.  If it is not given, exhaustive Cartesian is used.", required = false, direction = API.Direction.INOUT)
+  public HyperSpaceSearchCriteriaV99 search_criteria;
 
   //
   // Outputs
@@ -78,26 +66,31 @@ public class GridSearchSchema<G extends Grid<MP>,
       parms.remove("hyper_parameters");
     }
 
-    // Ugh:
-    if (parms.containsKey("strategy"))
-      try { strategy = GridSearch.Strategy.valueOf((String)parms.get("strategy")); }
-      catch (IllegalArgumentException iae) { throw new H2OIllegalArgumentException("strategy", (String)parms.get("strategy")); }
-      finally { parms.remove("strategy"); }
-
-    if (parms.containsKey("max_models"))
-      try { max_models = Integer.valueOf((String)parms.get("max_models")); }
-      catch (NumberFormatException nfe) { throw new H2OIllegalArgumentException("max_models", (String)parms.get("max_models")); }
-      finally { parms.remove("max_models"); }
-
-    if (parms.containsKey("max_time_ms"))
-      try { max_time_ms = Integer.valueOf((String)parms.get("max_time_ms")); }
-      catch (NumberFormatException nfe) { throw new H2OIllegalArgumentException("max_time_ms", (String)parms.get("max_time_ms")); }
-      finally { parms.remove("max_time_ms"); }
-
-    if (parms.containsKey("seed"))
-      try { seed = Long.valueOf((String)parms.get("seed")); }
-      catch (NumberFormatException nfe) { throw new H2OIllegalArgumentException("seed", (String)parms.get("seed")); }
-      finally { parms.remove("seed"); }
+    if( parms.containsKey("search_criteria") ) {
+      Properties p = water.util.JSONUtils.parseToProperties(parms.getProperty("search_criteria"));
+
+      if (! p.containsKey("strategy")) {
+        throw new H2OIllegalArgumentException("search_criteria.strategy", "null");
+      }
+
+      // TODO: move this into a factory method in HyperSpaceSearchCriteriaV99
+      String strategy = (String)p.get("strategy");
+      if ("Cartesian".equals(strategy)) {
+        search_criteria = new HyperSpaceSearchCriteriaV99.CartesianSearchCriteriaV99();
+      } else if ("RandomDiscrete".equals(strategy)) {
+        search_criteria = new HyperSpaceSearchCriteriaV99.RandomDiscreteValueSearchCriteriaV99();
+      } else {
+        throw new H2OIllegalArgumentException("search_criteria.strategy", strategy);
+      }
+
+      search_criteria.fillWithDefaults();
+      search_criteria.fillFromParms(p);
+      parms.remove("search_criteria");
+    } else {
+      // Fall back to Cartesian if there's no search_criteria specified.
+      search_criteria = new HyperSpaceSearchCriteriaV99.CartesianSearchCriteriaV99();
+    }
+
 
     if (parms.containsKey("grid_id")) { grid_id = new KeyV3.GridKeyV3(Key.<Grid>make(parms.getProperty("grid_id"))); parms.remove("grid_id"); }
 
diff --git a/h2o-core/src/main/java/hex/schemas/HyperSpaceSearchCriteriaV99.java b/h2o-core/src/main/java/hex/schemas/HyperSpaceSearchCriteriaV99.java
new file mode 100644
index 0000000..8912352
--- /dev/null
+++ b/h2o-core/src/main/java/hex/schemas/HyperSpaceSearchCriteriaV99.java
@@ -0,0 +1,73 @@
+package hex.schemas;
+
+import hex.grid.HyperSpaceSearchCriteria;
+import water.api.API;
+import water.api.Schema;
+import water.exceptions.H2OIllegalArgumentException;
+
+/**
+ * Search criteria for a hyperparameter search including directives for how to search and
+ * when to stop the search.
+ */
+public class HyperSpaceSearchCriteriaV99<I, S> extends Schema<HyperSpaceSearchCriteria, HyperSpaceSearchCriteriaV99.CartesianSearchCriteriaV99> {
+
+  @API(help = "Hyperparameter space search strategy.", required = true, values = { "Unknown", "Cartesian", "RandomDiscrete" }, direction = API.Direction.INOUT)
+  public HyperSpaceSearchCriteria.Strategy strategy;
+
+// TODO: add a factory which accepts a Strategy and calls the right constructor
+
+  /**
+   * Search criteria for an exhaustive Cartesian hyperparameter search.
+   */
+  public static class CartesianSearchCriteriaV99 extends HyperSpaceSearchCriteriaV99<HyperSpaceSearchCriteria.CartesianSearchCriteria, CartesianSearchCriteriaV99> {
+    public CartesianSearchCriteriaV99() {
+      strategy = HyperSpaceSearchCriteria.Strategy.Cartesian;
+    }
+  }
+
+  /**
+   * Search criteria for random hyperparameter search using hyperparameter values given by
+   * lists. Includes directives for how to search and when to stop the search.
+   */
+  public static class RandomDiscreteValueSearchCriteriaV99 extends HyperSpaceSearchCriteriaV99<HyperSpaceSearchCriteria.RandomDiscreteValueSearchCriteria, RandomDiscreteValueSearchCriteriaV99> {
+    public RandomDiscreteValueSearchCriteriaV99() {
+      strategy = HyperSpaceSearchCriteria.Strategy.RandomDiscrete;
+    }
+
+    public RandomDiscreteValueSearchCriteriaV99(long seed, int max_models, int max_time_ms) {
+      strategy = HyperSpaceSearchCriteria.Strategy.RandomDiscrete;
+      this.seed = seed;
+      this.max_models = max_models;
+      this.max_time_ms = max_time_ms;
+    }
+
+    @API(help = "Seed for random number generator; used for reproducibility.", required = false, direction = API.Direction.INOUT)
+    public long seed;
+
+    @API(help = "Maximum number of models to build (optional).", required = false, direction = API.Direction.INOUT)
+    public int max_models;
+
+    @API(help = "Maximum time to spend building models (optional).", required = false, direction = API.Direction.INOUT)
+    public int max_time_ms;
+  }
+
+  /**
+   * Fill with the default values from the corresponding Iced object.
+   */
+  public S fillWithDefaults() {
+    HyperSpaceSearchCriteria defaults = null;
+
+    if (HyperSpaceSearchCriteria.Strategy.Cartesian == strategy) {
+      defaults = new HyperSpaceSearchCriteria.CartesianSearchCriteria();
+    } else if (HyperSpaceSearchCriteria.Strategy.RandomDiscrete == strategy) {
+      defaults = new HyperSpaceSearchCriteria.RandomDiscreteValueSearchCriteria();
+    } else {
+      throw new H2OIllegalArgumentException("search_criteria.strategy", strategy.toString());
+    }
+
+    fillFromImpl(defaults);
+
+    return (S) this;
+  }
+
+}
diff --git a/h2o-core/src/main/java/water/H2O.java b/h2o-core/src/main/java/water/H2O.java
index 22740c7..9236274 100644
--- a/h2o-core/src/main/java/water/H2O.java
+++ b/h2o-core/src/main/java/water/H2O.java
@@ -291,6 +291,13 @@ final public class H2O {
 
       return result.toString();
     }
+
+    /**
+     * Whether this H2O instance was launched on hadoop (using 'hadoop jar h2odriver.jar') or not.
+     */
+    public boolean launchedWithHadoopJar() {
+      return hdfs_skip;
+    }
   }
 
   public static void parseFailed(String message) {
diff --git a/h2o-core/src/main/java/water/MRTask.java b/h2o-core/src/main/java/water/MRTask.java
index 290f131..05f348e 100644
--- a/h2o-core/src/main/java/water/MRTask.java
+++ b/h2o-core/src/main/java/water/MRTask.java
@@ -227,7 +227,7 @@ public abstract class MRTask<T extends MRTask<T>> extends DTask<T> implements Fo
     if( _output_types == null ) return null;
     final int noutputs = _output_types.length;
     Vec[] vecs = new Vec[noutputs];
-    if( _appendables==null )  // Zero rows?
+    if( _appendables==null || _appendables.length == 0)  // Zero rows?
       for( int i = 0; i < noutputs; i++ )
         vecs[i] = _fr.anyVec().makeZero();
     else {
diff --git a/h2o-core/src/main/java/water/api/GridSearchHandler.java b/h2o-core/src/main/java/water/api/GridSearchHandler.java
index 18064e6..ba4f942 100644
--- a/h2o-core/src/main/java/water/api/GridSearchHandler.java
+++ b/h2o-core/src/main/java/water/api/GridSearchHandler.java
@@ -5,6 +5,7 @@ import hex.ModelBuilder;
 import hex.ModelParametersBuilderFactory;
 import hex.grid.Grid;
 import hex.grid.GridSearch;
+import hex.grid.HyperSpaceSearchCriteria;
 import hex.schemas.GridSearchSchema;
 import water.H2O;
 import water.Job;
@@ -62,8 +63,13 @@ public class GridSearchHandler<G extends Grid<MP>,
     gss.init_meta();
     gss.parameters = (P)TypeMap.newFreezable(paramSchemaName);
     gss.parameters.init_meta();
+
+    // Get default parameters, then overlay the passed-in values
     ModelBuilder builder = ModelBuilder.make(algoURLName,null,null); // Default parameter settings
     gss.parameters.fillFromImpl(builder._parms); // Defaults for this builder into schema
+
+
+
     gss.fillFromParms(parms);   // Override defaults from user parms
 
     // Verify list of hyper parameters
@@ -82,7 +88,7 @@ public class GridSearchHandler<G extends Grid<MP>,
                                                  params,
                                                  gss.hyper_parameters,
                                                  new DefaultModelParametersBuilderFactory<MP, P>(),
-            gss.strategy, gss.max_models, gss.max_time_ms, gss.seed);
+                                                 (HyperSpaceSearchCriteria)gss.search_criteria.createAndFillImpl());
 
     // Fill schema with job parameters
     // FIXME: right now we have to remove grid parameters which we sent back
diff --git a/h2o-core/src/main/java/water/api/ModelBuilderHandler.java b/h2o-core/src/main/java/water/api/ModelBuilderHandler.java
index 693fedf..37b0897 100644
--- a/h2o-core/src/main/java/water/api/ModelBuilderHandler.java
+++ b/h2o-core/src/main/java/water/api/ModelBuilderHandler.java
@@ -42,8 +42,6 @@ public class ModelBuilderHandler<B extends ModelBuilder, S extends ModelBuilderS
     schema.parameters.fillFromParms(parms);         // Overwrite with user parms
     schema.parameters.fillImpl(builder._parms);     // Merged parms back over Model.Parameter object
     builder.init(false);          // validate parameters
-    if (builder.error_count() > 0)// Check for any parameter errors and bail now
-      throw H2OModelBuilderIllegalArgumentException.makeFromBuilder(builder);
 
     _t_start = System.currentTimeMillis();
     if( doTrain ) builder.trainModel();
diff --git a/h2o-core/src/main/java/water/api/ModelParametersSchema.java b/h2o-core/src/main/java/water/api/ModelParametersSchema.java
index db06251..340eea9 100644
--- a/h2o-core/src/main/java/water/api/ModelParametersSchema.java
+++ b/h2o-core/src/main/java/water/api/ModelParametersSchema.java
@@ -89,7 +89,7 @@ public class ModelParametersSchema<P extends Model.Parameters, S extends ModelPa
   @API(help = "Early stopping based on convergence of stopping_metric. Stop if simple moving average of length k of the stopping_metric does not improve for k:=stopping_rounds scoring events (0 to disable)", level = API.Level.secondary, direction=API.Direction.INOUT, gridable = true)
   public int stopping_rounds;
 
-  @API(help = "Maximum allowed runtime in seconds for model training. Use 0 to disable. For cross-validation or grid searches, this limit applies to all sub-models.", level = API.Level.secondary, direction=API.Direction.INOUT, gridable = true)
+  @API(help = "Maximum allowed runtime in seconds for model training. Use 0 to disable.", level = API.Level.secondary, direction=API.Direction.INOUT, gridable = true)
   public double max_runtime_secs;
 
   /**
diff --git a/h2o-core/src/main/java/water/parser/ParseDataset.java b/h2o-core/src/main/java/water/parser/ParseDataset.java
index cbeefa9..f7a9b0d 100644
--- a/h2o-core/src/main/java/water/parser/ParseDataset.java
+++ b/h2o-core/src/main/java/water/parser/ParseDataset.java
@@ -975,6 +975,7 @@ public final class ParseDataset {
       String isConstantStr = isConstant ? "constant" : "";
       String numLevelsStr = isCategorical ? String.format("%d", v.domain().length) : "";
 
+      boolean launchedWithHadoopJar = H2O.ARGS.launchedWithHadoopJar();
       boolean printLogSeparatorToStdout = false;
       boolean printColumnToStdout;
       {
@@ -984,7 +985,9 @@ public final class ParseDataset {
         // Print information to stdout for this many trailing columns.
         final int MAX_TAIL_TO_PRINT_ON_STDOUT = 10;
 
-        if (vecArr.length <= (MAX_HEAD_TO_PRINT_ON_STDOUT + MAX_TAIL_TO_PRINT_ON_STDOUT)) {
+        if (launchedWithHadoopJar) {
+          printColumnToStdout = true;
+        } else if (vecArr.length <= (MAX_HEAD_TO_PRINT_ON_STDOUT + MAX_TAIL_TO_PRINT_ON_STDOUT)) {
           // For small numbers of columns, print them all.
           printColumnToStdout = true;
         } else if (i < MAX_HEAD_TO_PRINT_ON_STDOUT) {
diff --git a/h2o-core/src/main/java/water/persist/PersistFS.java b/h2o-core/src/main/java/water/persist/PersistFS.java
index 102fb72..db555e5 100644
--- a/h2o-core/src/main/java/water/persist/PersistFS.java
+++ b/h2o-core/src/main/java/water/persist/PersistFS.java
@@ -19,7 +19,7 @@ final class PersistFS extends Persist {
   PersistFS(File root) {
     _root = root;
     _dir = new File(root, "ice" + H2O.API_PORT);
-    //deleteRecursive(_dir);
+    deleteRecursive(_dir);
     // Make the directory as-needed
     root.mkdirs();
     if( !(root.isDirectory() && root.canRead() && root.canWrite()) )
diff --git a/h2o-core/src/main/java/water/rapids/ASTBinOp.java b/h2o-core/src/main/java/water/rapids/ASTBinOp.java
index a4ce8d4..529f0e3 100644
--- a/h2o-core/src/main/java/water/rapids/ASTBinOp.java
+++ b/h2o-core/src/main/java/water/rapids/ASTBinOp.java
@@ -349,10 +349,11 @@ class ASTRound extends ASTBinOp {
     if(Double.isNaN(x)) return x;
     double sgn = x < 0 ? -1 : 1;
     x = Math.abs(x);
+    if( (int) digits != digits) digits = Math.round(digits);
     double power_of_10 = (int)Math.pow(10, (int)digits);
     return sgn*(digits == 0
                 // go to the even digit
-                ? (x % 1 >= 0.5 && !(Math.floor(x)%2==0))
+                ?( x % 1 > 0.5 || (x % 1 == 0.5 && !(Math.floor(x)%2==0)))
                 ? Math.ceil(x)
                 : Math.floor(x)
                 : Math.floor(x * power_of_10 + 0.5) / power_of_10);
@@ -363,6 +364,8 @@ class ASTSignif extends ASTBinOp {
   public String str() { return "signif"; }
   double op(double x, double digits) {
     if(Double.isNaN(x)) return x;
+    if(digits < 1) digits = 1; //mimic R's base::signif
+    if( (int) digits != digits) digits = Math.round(digits);
     java.math.BigDecimal bd = new java.math.BigDecimal(x);
     bd = bd.round(new java.math.MathContext((int)digits, java.math.RoundingMode.HALF_EVEN));
     return bd.doubleValue();
diff --git a/h2o-core/src/main/java/water/rapids/ASTCut.java b/h2o-core/src/main/java/water/rapids/ASTCut.java
index 2003b9f..5821a92 100644
--- a/h2o-core/src/main/java/water/rapids/ASTCut.java
+++ b/h2o-core/src/main/java/water/rapids/ASTCut.java
@@ -5,6 +5,9 @@ import water.fvec.Chunk;
 import water.fvec.Frame;
 import water.fvec.NewChunk;
 import water.fvec.Vec;
+import water.util.MathUtils;
+
+import java.util.Arrays;
 
 public class ASTCut extends ASTPrim {
   @Override
@@ -15,10 +18,11 @@ public class ASTCut extends ASTPrim {
   @Override Val apply( Env env, Env.StackHelp stk, AST asts[] ) {
     Frame fr = stk.track(asts[1].exec(env)).getFrame();
     double[] cuts  = check(asts[2]);
+    Arrays.sort(cuts);
     String[] labels= check2(asts[3]);
     final boolean lowest = asts[4].exec(env).getNum()==1;
     final boolean rite   = asts[5].exec(env).getNum()==1;
-    final int     digits = Math.max((int)asts[6].exec(env).getNum(),12); // cap at 12
+    final int     digits = Math.min((int) asts[6].exec(env).getNum(), 12); // cap at 12
 
     if(fr.vecs().length != 1 || fr.vecs()[0].isCategorical())
       throw new IllegalArgumentException("First argument must be a numeric column vector");
@@ -59,9 +63,9 @@ public class ASTCut extends ASTPrim {
         for (int r = 0; r < rows; ++r) {
           double x = c.atd(r);
           if (Double.isNaN(x) || (lowest && x <  cutz[0])
-                  || (!lowest && x <= cutz[0])
+                  || (!lowest && (x < cutz[0] || MathUtils.equalsWithinOneSmallUlp(x,cutz[0])) )
                   || (rite    && x >  cutz[cutz.length-1])
-                  || (!rite   && x >= cutz[cutz.length-1])) nc.addNum(Double.NaN); //slightly faster than nc.addNA();
+                  || (!rite   && (x > cutz[cutz.length-1] || MathUtils.equalsWithinOneSmallUlp(x,cutz[cutz.length-1]) )) ) nc.addNum(Double.NaN);
           else {
             for (int i = 1; i < cutz.length; ++i) {
               if (rite) {
diff --git a/h2o-core/src/main/java/water/rapids/ASTMerge.java b/h2o-core/src/main/java/water/rapids/ASTMerge.java
index d541a66..e8fd8d3 100644
--- a/h2o-core/src/main/java/water/rapids/ASTMerge.java
+++ b/h2o-core/src/main/java/water/rapids/ASTMerge.java
@@ -75,10 +75,7 @@ public class ASTMerge extends ASTPrim {
     // columns.  The hashed dataframe is completely replicated per-node
     boolean walkLeft;
     if( allLeft == allRite ) {
-      long lsize = 0, rsize = 0;
-      for( int i=ncols; i<l.numCols(); i++ ) lsize += l.vecs()[i].byteSize();
-      for( int i=ncols; i<r.numCols(); i++ ) rsize += r.vecs()[i].byteSize();
-      walkLeft = lsize > rsize;
+      walkLeft = l.numRows() > r.numRows();
     } else {
       walkLeft = allLeft;
     }
@@ -379,7 +376,7 @@ public class ASTMerge extends ASTPrim {
     void addRow(NewChunk[] nchks, Chunk[] chks, Vec[] vecs, int relRow, long absRow, BufferedString bStr) {
       int c=0;
       for( ;c< chks.length;++c) addElem(nchks[c],chks[c],relRow);
-      for( ;c<nchks.length;++c) addElem(nchks[c],vecs[_ncols],absRow,bStr);
+      for( ;c<nchks.length;++c) addElem(nchks[c],vecs[c - chks.length + _ncols],absRow,bStr);
     }
   }
 }
diff --git a/h2o-core/src/main/java/water/rapids/ASTUniOp.java b/h2o-core/src/main/java/water/rapids/ASTUniOp.java
index 847f818..c6f2823 100644
--- a/h2o-core/src/main/java/water/rapids/ASTUniOp.java
+++ b/h2o-core/src/main/java/water/rapids/ASTUniOp.java
@@ -50,7 +50,7 @@ abstract class ASTUniOp extends ASTPrim {
 
 class ASTCeiling extends ASTUniOp{ public String str() { return "ceiling";}double op(double d) { return Math.ceil (d); } }
 class ASTFloor extends ASTUniOp { public String str() { return "floor"; } double op(double d) { return Math.floor(d); } }
-class ASTNot   extends ASTUniOp { public String str() { return "!!"   ; } double op(double d) { return d==0?1:0; } }
+class ASTNot   extends ASTUniOp { public String str() { return "!!"   ; } double op(double d) { return Double.isNaN(d)?Double.NaN:d==0?1:0; } }
 class ASTTrunc extends ASTUniOp { public String str() { return "trunc"; } double op(double d) { return d>=0?Math.floor(d):Math.ceil(d);}}
 class ASTCos  extends ASTUniOp { public String str(){ return "cos";  } double op(double d) { return Math.cos(d);}}
 class ASTSin  extends ASTUniOp { public String str(){ return "sin";  } double op(double d) { return Math.sin(d);}}
@@ -357,23 +357,25 @@ class ASTMatch extends ASTPrim {
     else if( asts[2] instanceof ASTStr    ) strsTable2 = new String[]{asts[2].exec(env).getStr()};
     else throw new IllegalArgumentException("Expected numbers/strings. Got: "+asts[2].getClass());
 
+    final double nomatch = asts[3].exec(env).getNum();
+
     final String[] strsTable = strsTable2;
     final double[] dblsTable = dblsTable2;
 
     Frame rez = new MRTask() {
       @Override public void map(Chunk c, NewChunk n) {
         String[] domain = c.vec().domain();
-        int x, rows = c._len;
+        double x; int rows = c._len;
         for( int r = 0; r < rows; ++r) {
-          x = c.isNA(r) ? 0 : (strsTable==null ? in(dblsTable, c.atd(r)) : in(strsTable, domain[(int)c.at8(r)]));
-          n.addNum(x,0);
+          x = c.isNA(r) ? nomatch : (strsTable==null ? in(dblsTable, c.atd(r), nomatch) : in(strsTable, domain[(int)c.at8(r)], nomatch));
+          n.addNum(x);
         }
       }
     }.doAll(new byte[]{Vec.T_NUM}, fr.anyVec()).outputFrame();
     return new ValFrame(rez);
   }
-  private static int in(String[] matches, String s) { return Arrays.binarySearch(matches, s) >=0 ? 1: 0; }
-  private static int in(double[] matches, double d) { return binarySearchDoublesUlp(matches, 0,matches.length,d) >=0 ? 1: 0; }
+  private static double in(String[] matches, String s, double nomatch) { return Arrays.binarySearch(matches, s) >=0 ? 1: nomatch; }
+  private static double in(double[] matches, double d, double nomatch) { return binarySearchDoublesUlp(matches, 0,matches.length,d) >=0 ? 1: nomatch; }
 
   private static int binarySearchDoublesUlp(double[] a, int from, int to, double key) {
     int lo = from;
diff --git a/h2o-core/src/main/java/water/rapids/ASTVariance.java b/h2o-core/src/main/java/water/rapids/ASTVariance.java
index 200b7a5..893fe6f 100644
--- a/h2o-core/src/main/java/water/rapids/ASTVariance.java
+++ b/h2o-core/src/main/java/water/rapids/ASTVariance.java
@@ -6,116 +6,370 @@ import water.fvec.Chunk;
 import water.fvec.Frame;
 import water.fvec.Vec;
 import water.util.ArrayUtils;
+import java.util.Arrays;
 
 /** Variance between columns of a frame */
 class ASTVariance extends ASTPrim {
   @Override
-  public String[] args() { return new String[]{"ary", "x","y","use"}; }
-  private enum Mode { Everything, AllObs, CompleteObs }
-  @Override int nargs() { return 1+3; /* (var X Y use) */}
+  public String[] args() { return new String[]{"ary", "x","y","use", "symmetric"}; }
+  private enum Mode { Everything, AllObs, CompleteObs, PairwiseCompleteObs }
+  @Override int nargs() { return 1+4; /* (var X Y use symmetric) */}
   @Override public String str() { return "var"; }
   @Override Val apply( Env env, Env.StackHelp stk, AST asts[] ) {
     Frame frx = stk.track(asts[1].exec(env)).getFrame();
     Frame fry = stk.track(asts[2].exec(env)).getFrame();
     if( frx.numRows() != fry.numRows() )
       throw new IllegalArgumentException("Frames must have the same number of rows, found "+frx.numRows()+" and "+fry.numRows());
-    if( frx.numCols() != fry.numCols() )
-      throw new IllegalArgumentException("Frames must have the same number of columns, found "+frx.numCols()+" and "+fry.numCols());
-
     String use = stk.track(asts[3].exec(env)).getStr();
+    boolean symmetric = asts[4].exec(env).getNum()==1;
     Mode mode;
+    //In R, if the use arg is set, the na.rm arg has no effect (same result whether it is T or F). The na.rm param only 
+    // comes into play when no use arg is set. Without a use arg, setting na.rm = T is equivalent to use = "complete.obs",
+    // while setting na.rm = F (default) is equivalent to use = "everything". 
     switch( use ) {
-    case "everything":            mode = Mode.Everything; break;
-    case "all.obs":               mode = Mode.AllObs; break;
-    case "complete.obs":          mode = Mode.CompleteObs; break;
-    default: throw new IllegalArgumentException("unknown use mode, found: "+use);
+      case "everything":            mode = Mode.Everything; break;
+      case "all.obs":               mode = Mode.AllObs; break;
+      case "complete.obs":          mode = Mode.CompleteObs; break;
+      case "pairwise.complete.obs": mode = Mode.PairwiseCompleteObs; break;
+      default: throw new IllegalArgumentException("unknown use mode, found: "+use);
     }
-
-    return frx.numRows() == 1 ? scalar(frx,fry,mode) : array(frx,fry,mode);
+    
+    return frx.numRows() == 1 ? scalar(frx,fry,mode) : array(frx,fry,mode, symmetric);
   }
 
   // Scalar covariance for 1 row
-  private ValNum scalar( Frame frx, Frame fry, Mode mode ) {
+  private ValNum scalar( Frame frx, Frame fry, Mode mode) {
+    if( frx.numCols() != fry.numCols())
+      throw new IllegalArgumentException("Single rows have the same number of columns, found "+frx.numCols()+" and "+fry.numCols());
     Vec vecxs[] = frx.vecs();
     Vec vecys[] = fry.vecs();
-    double xmean=0, ymean=0, ncols = frx.numCols();
-    for( Vec v : vecxs ) xmean += v.at(0);
-    for( Vec v : vecys ) ymean += v.at(0);
-    xmean /= ncols; ymean /= ncols;
-   
-    double ss=0;
-    for( int r = 0; r < ncols; ++r )
-      ss += (vecxs[r].at(0) - xmean) * (vecys[r].at(0) - ymean);
-    if( Double.isNaN(ss) && mode.equals(Mode.AllObs) ) throw new IllegalArgumentException("Mode is 'all.obs' but NAs are present");
-    return new ValNum(ss/(ncols-1));
+    double xsum=0, ysum=0, NACount=0, ncols = frx.numCols(), xval, yval, ss=0;
+    for( int i=0; i< vecxs.length; i++) {
+      xval = vecxs[i].at(0);
+      yval = vecys[i].at(0);
+      if (Double.isNaN(xval) || Double.isNaN(yval))
+        NACount++;
+      else {
+        xsum += xval;
+        ysum += yval;
+        ss += xval * yval;
+      }
+    }
+    
+    if (NACount>0) {
+      if (mode.equals(Mode.AllObs)) throw new IllegalArgumentException("Mode is 'all.obs' but NAs are present");
+      if (mode.equals(Mode.Everything)) return new ValNum(Double.NaN);
+    }
+    return new ValNum((ss - xsum * ysum/(ncols - NACount)) / (ncols-1-NACount));
   }
 
   // Matrix covariance.  Compute covariance between all columns from each Frame
   // against each other.  Return a matrix of covariances which is frx.numCols
-  // wide and fry.numCols tall.
-  private Val array( Frame frx, Frame fry, Mode mode ) {
+  // tall and fry.numCols wide.
+  private Val array( Frame frx, Frame fry, Mode mode, boolean symmetric) {
     Vec[] vecxs = frx.vecs();  int ncolx = vecxs.length;
     Vec[] vecys = fry.vecs();  int ncoly = vecys.length;
-    double[] ymeans = new double[ncoly];
-    for( int y=0; y<ncoly; y++ ) // All the Y means
-      ymeans[y] = vecys[y].mean();
 
-    // Launch tasks; each does all Ys vs one X
-    CoVarTask[] cvts = new CoVarTask[ncolx];
-    for( int x=0; x<ncolx; x++ )
-      cvts[x] = new CoVarTask(vecxs[x].mean(),ymeans).dfork(new Frame(vecxs[x]).add(fry));
-    // Short cut for the 1-row-1-col result: return a scalar
-    if( ncolx==1 && ncoly==1 )
-      return new ValNum(cvts[0].getResult()._covs[0]/(frx.numRows()-1));
+    if (mode.equals(Mode.Everything) || mode.equals(Mode.AllObs)) {
+      CoVarTaskEverything[] cvts = new CoVarTaskEverything[ncoly];
+      if (symmetric) {
+        int[] idx  = new int[ncoly];
+        for (int y = 0; y < ncoly; y++) idx[y] = y;
+        int[] first_index = new int[]{0};
+        //compute covariances between column_i and and column_i, column_i+1, ... 
+        Frame reduced_fr = new Frame(frx);
+        for (int y = 0; y <ncoly; y++) {
+          cvts[y] = new CoVarTaskEverything().dfork(new Frame(vecys[y]).add(reduced_fr));
+          idx = ArrayUtils.removeIds(idx, first_index);
+          reduced_fr = new Frame(frx.vecs(idx));
+        }
+        //arrange the results into the bottom left of res_array. each successive cvts is 1 smaller in length
+        double[][] res_array = new double[ncoly][ncoly];
+        for (int y =0; y<ncoly; y++) {
+          double[] res_array_y = res_array[y];
+          CoVarTaskEverything cvtx = cvts[y].getResult();
+          if (mode.equals(Mode.AllObs))
+            for (double ss : cvtx._ss)
+              if (Double.isNaN(ss)) throw new IllegalArgumentException("Mode is 'all.obs' but NAs are present");
+          double[] res = ArrayUtils.div(ArrayUtils.subtract(cvtx._ss, ArrayUtils.mult(cvtx._xsum,
+                  ArrayUtils.div(cvtx._ysum, frx.numRows()))), frx.numRows() - 1);
+          System.arraycopy(res, 0, res_array_y, y, ncoly - y);
+        }
+        //copy over the bottom left of res_array to its top right
+        for (int y = 0; y < ncoly -1; y++) {
+          for (int x = y+1; x < ncoly ; x++) {
+            res_array[x][y] = res_array[y][x];
+          }
+        }
+        //set Frame
+        Vec[] res = new Vec[ncoly];
+        Key<Vec>[] keys = Vec.VectorGroup.VG_LEN1.addVecs(ncoly);
+        for (int y = 0; y < ncoly; y++) {
+          res[y] = Vec.makeVec(res_array[y], keys[y]);
+        }
+        return new ValFrame(new Frame(fry._names, res));
+      }
+      // Launch tasks; each does all Xs vs one Y
+      for (int y = 0; y < ncoly; y++)
+        cvts[y] = new CoVarTaskEverything().dfork(new Frame(vecys[y]).add(frx));
+      // Short cut for the 1-row-1-col result: return a scalar
+      if (ncolx == 1 && ncoly == 1) {
+        CoVarTaskEverything res = cvts[0].getResult();
+        if (mode.equals(Mode.AllObs) && Double.isNaN(res._ss[0])) throw new IllegalArgumentException("Mode is 'all.obs' but NAs are present");
+        return new ValNum((res._ss[0] - res._xsum[0] * res._ysum[0] / (frx.numRows())) / (frx.numRows() - 1));
+      }
+      // Gather all the Xs-vs-Y covariance arrays; divide by rows
+      Vec[] res = new Vec[ncoly];
+      Key<Vec>[] keys = Vec.VectorGroup.VG_LEN1.addVecs(ncoly);
+      for (int y = 0; y < ncoly; y++) {
+        CoVarTaskEverything cvtx = cvts[y].getResult();
+        if (mode.equals(Mode.AllObs))
+          for (double ss : cvtx._ss)
+            if (Double.isNaN(ss)) throw new IllegalArgumentException("Mode is 'all.obs' but NAs are present");
+        res[y] = Vec.makeVec(ArrayUtils.div(ArrayUtils.subtract(cvtx._ss, ArrayUtils.mult(cvtx._xsum,
+                ArrayUtils.div(cvtx._ysum, frx.numRows()))), frx.numRows() - 1), keys[y]);
+      }
 
-    // Gather all the Ys-vs-X covariance arrays; divide by rows
-    Vec[] res = new Vec[ncolx];
-    Key<Vec>[] keys = Vec.VectorGroup.VG_LEN1.addVecs(ncolx);
-    for( int x=0; x<ncolx; x++ )
-      res[x] = Vec.makeVec(ArrayUtils.div(cvts[x].getResult()._covs, (frx.numRows()-1)), keys[x]);
+      // CNC - For fun, uncomment this code to scale all values by their
+      // respective std-devs, basically standardizing the results.  This gives
+      // something similar to a r^2 correlation where 1.0 (max possible value)
+      // indicates perfect linearity (maybe someting weaker: perfect equality?),
+      // and -1 perfectly anti-linear (90% twist), and zero is remains
+      // uncorrelated (with actual covariance, zero is also uncorrelated but
+      // non-zero values are scaled by the columns' numeric range).
+      //
+      //for( int x=0; x<ncolx; x++ ) {
+      //  double ds[] = ArrayUtils.div(cvts[x].getResult()._covs, (frx.numRows()-1));
+      //  ArrayUtils.div(cvts[x].getResult()._covs, vecxs[x].sigma());
+      //  for( int y=0; y<ncoly; y++ )
+      //    ds[y] /= vecys[y].sigma();
+      //  res[x] = Vec.makeVec(ds, keys[x]);
+      //}
+      return new ValFrame(new Frame(fry._names, res));
+    }
+    if (mode.equals(Mode.CompleteObs)) {
+      CoVarTaskComplete cvs = new CoVarTaskComplete(ncolx, ncoly).doAll(new Frame(fry).add(frx));
+      
+      if (ncolx == 1 && ncoly == 1)
+        return new ValNum((cvs._ss[0][0] - cvs._xsum[0] * cvs._ysum[0] / (frx.numRows() - cvs._NACount)) / (frx.numRows() - cvs._NACount - 1));
+      
+      //All Xs-and-Ys
+      Vec[] res = new Vec[ncoly];
+      Key<Vec>[] keys = Vec.VectorGroup.VG_LEN1.addVecs(ncoly);      
+      for (int y= 0; y < ncoly; y++) {
+        res[y] = Vec.makeVec(ArrayUtils.div(ArrayUtils.subtract(cvs._ss[y], ArrayUtils.mult(cvs._xsum.clone(),
+                (cvs._ysum[y] / (frx.numRows() - cvs._NACount)))), (frx.numRows() - 1 - cvs._NACount)), keys[y]);
+      }
+      return new ValFrame(new Frame(fry._names, res));
+    }
+    else { //if (mode.equals(Mode.PairwiseCompleteObs)) {
+      CoVarTaskPairwise[] cvts = new CoVarTaskPairwise[ncoly];
+      if (symmetric) {
+        int[] idx  = new int[ncoly];
+        for (int y = 0; y < ncoly; y++) idx[y] = y;
+        int[] first_index = new int[]{0};
+        //compute covariances between column_i and and column_i, column_i+1, ... 
+        Frame reduced_fr = new Frame(frx);
+        for (int y = 0; y <ncoly; y++) {
+          cvts[y] = new CoVarTaskPairwise().dfork(new Frame(vecys[y]).add(reduced_fr));
+          idx = ArrayUtils.removeIds(idx, first_index);
+          reduced_fr = new Frame(frx.vecs(idx));
+        }
+        //arrange the results into the bottom left of res_array. each successive cvts is 1 smaller in length
+        double[][] res_array = new double[ncoly][ncoly];
+        for (int y =0; y<ncoly; y++) {
+          double[] res_array_y = res_array[y];
+          CoVarTaskPairwise cvtx = cvts[y].getResult();
 
-    // CNC - For fun, uncomment this code to scale all values by their
-    // respective std-devs, basically standardizing the results.  This gives
-    // something similar to a r^2 correlation where 1.0 (max possible value)
-    // indicates perfect linearity (maybe someting weaker: perfect equality?),
-    // and -1 perfectly anti-linear (90% twist), and zero is remains
-    // uncorrelated (with actual covariance, zero is also uncorrelated but
-    // non-zero values are scaled by the columns' numeric range).
-    //
-    //for( int x=0; x<ncolx; x++ ) {
-    //  double ds[] = ArrayUtils.div(cvts[x].getResult()._covs, (frx.numRows()-1));
-    //  ArrayUtils.div(cvts[x].getResult()._covs, vecxs[x].sigma());
-    //  for( int y=0; y<ncoly; y++ )
-    //    ds[y] /= vecys[y].sigma();
-    //  res[x] = Vec.makeVec(ds, keys[x]);
-    //}
-    return new ValFrame(new Frame(frx._names,res));
+          double[] res = ArrayUtils.div(ArrayUtils.subtract(cvtx._ss, ArrayUtils.mult(cvtx._xsum,
+                  ArrayUtils.div(cvtx._ysum, ArrayUtils.subtract(frx.numRows(), cvtx._NACount.clone())))), 
+                  ArrayUtils.subtract(frx.numRows() - 1, cvtx._NACount.clone()));
+          System.arraycopy(res, 0, res_array_y, y, ncoly - y);
+        }
+        //copy over the bottom left of res_array to its top right
+        for (int y = 0; y < ncoly -1; y++) {
+          for (int x = y+1; x < ncoly ; x++) {
+            res_array[x][y] = res_array[y][x];
+          }
+        }
+        //set Frame
+        Vec[] res = new Vec[ncoly];
+        Key<Vec>[] keys = Vec.VectorGroup.VG_LEN1.addVecs(ncoly);
+        for (int y = 0; y < ncoly; y++) {
+          res[y] = Vec.makeVec(res_array[y], keys[y]);
+        }
+        return new ValFrame(new Frame(fry._names, res));
+      }
+      for (int y = 0; y < ncoly; y++)
+        cvts[y] = new CoVarTaskPairwise().dfork(new Frame(vecys[y]).add(frx));
+      // Short cut for the 1-row-1-col result: return a scalar
+      if (ncolx == 1 && ncoly == 1) {
+        CoVarTaskPairwise res = cvts[0].getResult();
+        return new ValNum((res._ss[0] - res._xsum[0] * res._ysum[0] / (frx.numRows() - res._NACount[0])) / (frx.numRows() - 1 - res._NACount[0]));
+      }
+      // Gather all the Xs-vs-Y covariance arrays; divide by rows
+      Vec[] res = new Vec[ncoly];
+      Key<Vec>[] keys = Vec.VectorGroup.VG_LEN1.addVecs(ncoly);
+      for (int y = 0; y < ncoly; y++) {
+        CoVarTaskPairwise cvtx = cvts[y].getResult();
+        res[y] = Vec.makeVec(ArrayUtils.div(ArrayUtils.subtract(cvtx._ss, ArrayUtils.mult(cvtx._xsum,
+                ArrayUtils.div(cvtx._ysum, ArrayUtils.subtract(frx.numRows(), cvtx._NACount.clone())))), 
+                ArrayUtils.subtract(frx.numRows() - 1, cvtx._NACount.clone())), keys[y]);
+      }
+      
+      return new ValFrame(new Frame(fry._names, res));
+    }
   }
 
-  private static class CoVarTask extends MRTask<CoVarTask> {
-    double[] _covs;
-    final double _xmean, _ymeans[];
-    CoVarTask( double xmean, double[] ymeans ) { _xmean = xmean; _ymeans = ymeans; }
+
+  private static class CoVarTaskEverything extends MRTask<CoVarTaskEverything> {
+    double[] _ss, _xsum, _ysum;
+    CoVarTaskEverything() {}
     @Override public void map( Chunk cs[] ) {
-      final int ncols = cs.length-1;
-      final Chunk cx = cs[0];
-      final int len = cx._len;
-      _covs = new double[ncols];
-      for( int y=0; y<ncols; y++ ) {
-        double sum = 0;
-        final Chunk cy = cs[y+1];
-        final double ymean = _ymeans[y];
-        for( int row=0; row<len; row++ ) 
-          sum += (cx.atd(row)-_xmean)*(cy.atd(row)-ymean);
-        _covs[y] = sum;
+      final int ncolsx = cs.length-1;
+      final Chunk cy = cs[0];
+      final int len = cy._len;
+      _ss = new double[ncolsx];
+      _xsum = new double[ncolsx];
+      _ysum = new double[ncolsx];
+      double xval, yval;
+      for( int x=0; x<ncolsx; x++ ) {
+        double ss = 0, xsum = 0, ysum = 0;
+        final Chunk cx = cs[x+1];
+        for( int row=0; row<len; row++ ) {
+          xval = cx.atd(row);
+          yval = cy.atd(row);
+          xsum += xval;
+          ysum += yval;
+          ss += xval * yval;
+        }
+        _ss[x] = ss;
+        _xsum[x] = xsum;
+        _ysum[x] = ysum;
       }
     }
-    @Override public void reduce( CoVarTask cvt ) { ArrayUtils.add(_covs,cvt._covs); }
+    @Override public void reduce( CoVarTaskEverything cvt ) {
+      ArrayUtils.add(_ss,cvt._ss);
+      ArrayUtils.add(_xsum, cvt._xsum);
+      ArrayUtils.add(_ysum, cvt._ysum);
+    }
+  }
+
+  private static class CoVarTaskComplete extends MRTask<CoVarTaskComplete> {
+    double[][] _ss;
+    double[] _xsum, _ysum;
+    long _NACount;
+    int _ncolx, _ncoly;
+    CoVarTaskComplete(int ncolx, int ncoly) { _ncolx = ncolx; _ncoly = ncoly;}
+    @Override public void map( Chunk cs[] ) {
+      
+      _ss = new double[_ncoly][_ncolx];
+      _xsum = new double[_ncolx];
+      _ysum = new double[_ncoly];
+
+      double[] xvals = new double[_ncolx];
+      double[] yvals = new double[_ncoly];
+
+      double xval, yval;
+      double[] _ss_y;
+      boolean add;
+      int len = cs[0]._len;
+      for (int row = 0; row < len; row++) {
+        add = true;
+        //reset existing arrays to 0 rather than initializing new ones to save on garbage collection
+        Arrays.fill(xvals, 0);
+        Arrays.fill(yvals, 0);
+        
+        for (int y = 0; y < _ncoly; y++) {
+          final Chunk cy = cs[y];
+          yval = cy.atd(row);
+          //if any yval along a row is NA, discard the entire row
+          if (Double.isNaN(yval)) {
+            _NACount++;
+            add = false;
+            break;
+          }
+          yvals[y] = yval;
+        }
+        if (add) {
+          for (int x = 0; x < _ncolx; x++) {
+            final Chunk cx = cs[x + _ncoly];
+            xval = cx.atd(row);
+            //if any xval along a row is NA, discard the entire row
+            if (Double.isNaN(xval)) {
+              _NACount++;
+              add = false;
+              break;
+            }
+            xvals[x] = xval;
+          }
+        }
+        //add is true iff row has been traversed and found no NAs among yvals and xvals  
+        if (add) {
+          for (int y=0; y < _ncoly; y++) {
+            _ss_y = _ss[y];
+            yval = yvals[y];
+            for (int x = 0; x < _ncolx; x++)
+              _ss_y[x] += xvals[x] * yval;
+          }
+          ArrayUtils.add(_xsum, xvals);
+          ArrayUtils.add(_ysum, yvals);
+        }
+      }
+    }
+    @Override public void reduce( CoVarTaskComplete cvt ) {
+      ArrayUtils.add(_ss,cvt._ss);
+      ArrayUtils.add(_xsum, cvt._xsum);
+      ArrayUtils.add(_ysum, cvt._ysum);
+      _NACount += cvt._NACount;
+    }
+  }
+
+  private static class CoVarTaskPairwise extends MRTask<CoVarTaskPairwise> {
+    double[] _ss, _xsum, _ysum;
+    long[] _NACount;
+    CoVarTaskPairwise() {}
+    @Override public void map( Chunk cs[] ) {
+      final int ncolsx = cs.length-1;
+      final Chunk cy = cs[0];
+      final int len = cy._len;
+      _ss = new double[ncolsx];
+      _xsum = new double[ncolsx];
+      _ysum = new double[ncolsx];
+      _NACount = new long[ncolsx];
+      double xval, yval;
+      for( int x=0; x<ncolsx; x++ ) {
+        double ss = 0, xsum = 0, ysum = 0;
+        long na = 0;
+        final Chunk cx = cs[x+1];
+        for( int row=0; row<len; row++ ) {
+          xval = cx.atd(row);
+          yval = cy.atd(row);
+          if (Double.isNaN(xval) || Double.isNaN(yval))
+            na++;
+          else {
+            xsum += xval;
+            ysum += yval; 
+            ss += xval * yval;
+          }
+        }
+        _ss[x] = ss;
+        _xsum[x] = xsum;
+        _ysum[x] = ysum;
+        _NACount[x] = na;
+      }
+    }
+    @Override public void reduce( CoVarTaskPairwise cvt ) { 
+      ArrayUtils.add(_ss,cvt._ss);
+      ArrayUtils.add(_xsum, cvt._xsum);
+      ArrayUtils.add(_ysum, cvt._ysum);
+      ArrayUtils.add(_NACount, cvt._NACount);
+    }
   }
 
   static double getVar(Vec v) {
-    double m = v.mean();
-    CoVarTask t = new CoVarTask(m,new double[]{m}).doAll(new Frame(v, v));
-    return t._covs[0] / (v.length() - 1);
+    CoVarTaskEverything res = new CoVarTaskEverything().doAll(new Frame(v, v));
+    return (res._ss[0] - res._xsum[0] * res._ysum[0] / (v.length())) / (v.length() - 1);
+    
   }
 }
diff --git a/h2o-core/src/main/java/water/util/ArrayUtils.java b/h2o-core/src/main/java/water/util/ArrayUtils.java
index 081ed66..7262b7b 100644
--- a/h2o-core/src/main/java/water/util/ArrayUtils.java
+++ b/h2o-core/src/main/java/water/util/ArrayUtils.java
@@ -253,6 +253,12 @@ public class ArrayUtils {
     for (int i=0; i<ary.length; i++) mult(ary[i], n);
     return ary;
   }
+  
+  public static double[] mult(double[] nums, double[] nums2) {
+    for (int i=0; i<nums.length; i++) nums[i] *= nums2[i];
+    return nums;
+  }
+  
   public static double[] invert(double[] ary) {
     if(ary == null) return null;
     for(int i=0;i<ary.length;i++) ary[i] = 1. / ary[i];
@@ -1242,7 +1248,7 @@ public class ArrayUtils {
     double [] res = new double[x.length-ids.length];
     int j = 0;
     for(int i = 0; i < x.length; ++i)
-      if(i != ids[j]) res[i-j] = x[i]; else ++j;
+      if(j == ids.length || i != ids[j]) res[i-j] = x[i]; else ++j;
     return res;
   }
 
@@ -1260,4 +1266,9 @@ public class ArrayUtils {
       if(d != 0)++res;
     return res;
   }
+
+  public static long[] subtract(long n, long[] nums) {
+    for (int i=0; i<nums.length; i++) nums[i] = n - nums[i];
+    return nums;
+  }
 }
diff --git a/h2o-core/src/main/java/water/util/JSONUtils.java b/h2o-core/src/main/java/water/util/JSONUtils.java
index 37f90fa..3e0c049 100644
--- a/h2o-core/src/main/java/water/util/JSONUtils.java
+++ b/h2o-core/src/main/java/water/util/JSONUtils.java
@@ -3,9 +3,14 @@ package water.util;
 import com.google.gson.Gson;
 import water.nbhm.NonBlockingHashMap;
 
+import java.util.Properties;
+
 public class JSONUtils {
 
   public static NonBlockingHashMap<String, Object> parse(String json) {
     return new Gson().fromJson(json, NonBlockingHashMap.class);
   }
+  public static Properties parseToProperties(String json) {
+    return new Gson().fromJson(json, Properties.class);
+  }
 }
diff --git a/h2o-core/src/main/java/water/util/Log.java b/h2o-core/src/main/java/water/util/Log.java
index 331ed2a..00fc123 100644
--- a/h2o-core/src/main/java/water/util/Log.java
+++ b/h2o-core/src/main/java/water/util/Log.java
@@ -303,7 +303,7 @@ abstract public class Log {
   private static synchronized org.apache.log4j.Logger createLog4j() {
     if( _logger != null ) return _logger; // Test again under lock
 
-    boolean launchedWithHadoopJar = H2O.ARGS.hdfs_skip;
+    boolean launchedWithHadoopJar = H2O.ARGS.launchedWithHadoopJar();
     String log4jConfiguration = System.getProperty ("h2o.log4j.configuration");
     boolean log4jConfigurationProvided = log4jConfiguration != null;
 
diff --git a/h2o-core/src/test/java/hex/ModelAdaptTest.java b/h2o-core/src/test/java/hex/ModelAdaptTest.java
index 796751d..590b5a0 100644
--- a/h2o-core/src/test/java/hex/ModelAdaptTest.java
+++ b/h2o-core/src/test/java/hex/ModelAdaptTest.java
@@ -20,6 +20,7 @@ public class ModelAdaptTest extends TestUtil {
       public String algoName() { return "A"; }
       public String fullName() { return "A"; }
       public String javaName() { return AModel.class.getName(); }
+      @Override public long progressUnits() { return 0; }
     }
     static class AOutput extends Model.Output { }
   }
diff --git a/h2o-core/src/test/java/water/TestUtil.java b/h2o-core/src/test/java/water/TestUtil.java
index 760a18d..9fdc52a 100644
--- a/h2o-core/src/test/java/water/TestUtil.java
+++ b/h2o-core/src/test/java/water/TestUtil.java
@@ -26,6 +26,7 @@ import static org.junit.Assert.assertTrue;
 
 @Ignore("Support for tests, but no actual tests here")
 public class TestUtil extends Iced {
+  public final static boolean JACOCO_ENABLED = Boolean.parseBoolean(System.getProperty("test.jacocoEnabled", "false"));
   private static boolean _stall_called_before = false;
   protected static int _initial_keycnt = 0;
   protected static int MINCLOUDSIZE;
@@ -41,7 +42,11 @@ public class TestUtil extends Iced {
       H2O.registerRestApis(System.getProperty("user.dir"));
       _stall_called_before = true;
     }
-    H2O.waitForCloudSize(x, 30000);
+    if (JACOCO_ENABLED) {
+      H2O.waitForCloudSize(x, 120000);
+    } else {
+      H2O.waitForCloudSize(x, 30000);
+    }
     _initial_keycnt = H2O.store_size();
   }
 
diff --git a/h2o-core/src/test/java/water/api/APIThrPriorTest.java b/h2o-core/src/test/java/water/api/APIThrPriorTest.java
index 737b3e5..d566294 100644
--- a/h2o-core/src/test/java/water/api/APIThrPriorTest.java
+++ b/h2o-core/src/test/java/water/api/APIThrPriorTest.java
@@ -158,6 +158,7 @@ class BogusModel extends Model<BogusModel,BogusModel.BogusParameters,BogusModel.
     public String algoName() { return "Bogus"; }
     public String fullName() { return "Bogus"; }
     public String javaName() { return BogusModel.class.getName(); }
+    @Override public long progressUnits() { return 0; }
   }
   public static class BogusOutput extends Model.Output { }
   BogusModel( Key selfKey, BogusParameters parms, BogusOutput output) { super(selfKey,parms,output); }
@@ -177,7 +178,6 @@ class Bogus extends ModelBuilder<BogusModel,BogusModel.BogusParameters,BogusMode
   @Override public BuilderVisibility builderVisibility() { return BuilderVisibility.Experimental; }
   public Bogus( BogusModel.BogusParameters parms ) { super(parms); init(false); }
   @Override protected Driver trainModelImpl() { return new BogusDriver(); }
-  @Override public long progressUnits() { return 0; }
   @Override public void init(boolean expensive) { super.init(expensive); }
 
   private class BogusDriver extends Driver {
diff --git a/h2o-core/src/test/java/water/junit/H2OTestRunner.java b/h2o-core/src/test/java/water/junit/H2OTestRunner.java
index 3eff5ba..9ce7aba 100644
--- a/h2o-core/src/test/java/water/junit/H2OTestRunner.java
+++ b/h2o-core/src/test/java/water/junit/H2OTestRunner.java
@@ -20,7 +20,6 @@ import java.util.List;
 public class H2OTestRunner {
 
   public Result run(String[] args) throws Exception {
-
     // List all classes - adapted from JUnitCore code
     List<Class<?>> classes = new ArrayList<Class<?>>();
     List<Failure> missingClasses = new ArrayList<Failure>();
@@ -51,7 +50,8 @@ public class H2OTestRunner {
 
   public static void main(String[] args) throws Exception {
     H2OTestRunner testRunner = new H2OTestRunner();
-    Result result = testRunner.run(args);
+    Result result = null;
+    result = testRunner.run(args);
     System.exit(result.wasSuccessful() ? 0 : 1);
   }
 }
diff --git a/h2o-core/src/test/java/water/rapids/RapidsTest.java b/h2o-core/src/test/java/water/rapids/RapidsTest.java
index da9884e..f4abfa8 100644
--- a/h2o-core/src/test/java/water/rapids/RapidsTest.java
+++ b/h2o-core/src/test/java/water/rapids/RapidsTest.java
@@ -205,10 +205,10 @@ public class RapidsTest extends TestUtil {
 
   @Test public void testVariance() {
     // Checking variance: scalar
-    String tree = "({x . (var x x \"everything\")} (rows a.hex [0]))";
+    String tree = "({x . (var x x \"everything\" FALSE)} (rows a.hex [0]))";
     checkTree(tree);
 
-    tree = "({x . (var x x \"everything\")} a.hex)";
+    tree = "({x . (var x x \"everything\" FALSE)} a.hex)";
     checkTree(tree);
 
     tree = "(table (trunc (cols a.hex 1)) FALSE)";
diff --git a/h2o-core/testClientNode.sh b/h2o-core/testClientNode.sh
index a7dd7ed..1acbfee 100644
--- a/h2o-core/testClientNode.sh
+++ b/h2o-core/testClientNode.sh
@@ -1,5 +1,13 @@
 #!/bin/bash
 
+# Argument parsing
+if [ "$1" = "jacoco" ]
+then
+    JACOCO_ENABLED=true
+else
+    JACOCO_ENABLED=false
+fi
+
 # Clean out any old sandbox, make a new one
 OUTDIR=sandbox
 rm -fr $OUTDIR; mkdir -p $OUTDIR
@@ -23,7 +31,20 @@ trap cleanup SIGTERM SIGINT
 #   build/libs/h2o-core.jar      - Main h2o core classes
 #   build/libs/test-h2o-core.jar - Test h2o core classes
 #   build/resources/main         - Main resources (e.g. page.html)
-JVM="nice java -ea -Xmx2g -Xms2g -cp build/classes/main${SEP}build/classes/test${SEP}../lib/*${SEP}../h2o-algos/build/classes/main${SEP}../h2o-app/build/classes/main${SEP}../h2o-genmodel/build/libs/h2o-genmodel.jar"
+
+MAX_MEM="-Xmx2g"
+
+# Check if coverage should be run
+if [ $JACOCO_ENABLED = true ]
+then
+    AGENT="../jacoco/jacocoagent.jar"
+    COVERAGE="-javaagent:$AGENT=destfile=build/jacoco/h2o-core_client.exec"
+    MAX_MEM="-Xmx8g"
+else
+    COVERAGE=""
+fi
+
+JVM="nice java $COVERAGE -ea $MAX_MEM -Xms2g -cp build/classes/main${SEP}build/classes/test${SEP}../lib/*${SEP}../h2o-algos/build/classes/main${SEP}../h2o-app/build/classes/main${SEP}../h2o-genmodel/build/libs/h2o-genmodel.jar"
 
 # Tests
 # Must run first, before the cloud locks (because it tests cloud locking)
@@ -43,10 +64,18 @@ $JVM water.H2OApp 1> $OUTDIR/out.2 2>&1 & PID_2=$!
 $JVM water.H2OApp 1> $OUTDIR/out.3 2>&1 & PID_3=$!
 #$JVM water.H2O 1> $OUTDIR/out.4 2>&1 & PID_4=$!
 
+# If coverage is being run, then pass a system variable flag so that timeout limits are increased.
+if [ $JACOCO_ENABLED = true ]
+then
+    JACOCO_FLAG="-Dtest.jacocoEnabled=true"
+else
+    JACOCO_FLAG=""
+fi
+
 # Launch last driver JVM.  All output redir'd at the OS level to sandbox files,
 # and tee'd to stdout so we can watch.
-($JVM -Dh2o.arg.client=true org.junit.runner.JUnitCore water.ClientTest 2>&1 ; echo $? > $OUTDIR/status.0) | tee --append $OUTDIR/out.0 
-($JVM -Dh2o.arg.client=true org.junit.runner.JUnitCore water.ClientTest 2>&1 ; echo $? > $OUTDIR/status.0) | tee --append $OUTDIR/out.0 
-($JVM -Dh2o.arg.client=true org.junit.runner.JUnitCore water.ClientTest 2>&1 ; echo $? > $OUTDIR/status.0) | tee --append $OUTDIR/out.0 
+($JVM -Dh2o.arg.client=true $JACOCO_FLAG org.junit.runner.JUnitCore water.ClientTest 2>&1 ; echo $? > $OUTDIR/status.0) | tee --append $OUTDIR/out.0
+($JVM -Dh2o.arg.client=true $JACOCO_FLAG org.junit.runner.JUnitCore water.ClientTest 2>&1 ; echo $? > $OUTDIR/status.0) | tee --append $OUTDIR/out.0
+($JVM -Dh2o.arg.client=true $JACOCO_FLAG org.junit.runner.JUnitCore water.ClientTest 2>&1 ; echo $? > $OUTDIR/status.0) | tee --append $OUTDIR/out.0
 
 cleanup
diff --git a/h2o-core/testMultiNode.sh b/h2o-core/testMultiNode.sh
index 5f46bd1..66bb3c7 100755
--- a/h2o-core/testMultiNode.sh
+++ b/h2o-core/testMultiNode.sh
@@ -1,5 +1,13 @@
 #!/bin/bash
 
+# Argument parsing
+if [ "$1" = "jacoco" ]
+then
+    JACOCO_ENABLED=true
+else
+    JACOCO_ENABLED=false
+fi
+
 # Clean out any old sandbox, make a new one
 OUTDIR=sandbox
 rm -fr $OUTDIR; mkdir -p $OUTDIR
@@ -52,9 +60,22 @@ else
     JAVA_CMD="${JAVA_CMD}"
   fi
 fi
-# Command to invoke test.  Note the explicit 3g sizes; if the JVM runs out of
+
+# Memory should be explicitly kept to 3g. If the JVM runs out of
 # memory on these tests, we need to diagnose the extra memory requirements
-JVM="nice $JAVA_CMD -Xmx3g -Xms3g -ea -cp build/classes/test${SEP}build/classes/main${SEP}../h2o-genmodel/build/libs/h2o-genmodel.jar${SEP}../lib/*"
+MAX_MEM="-Xmx3g"
+
+# Check if coverage should be run
+if [ $JACOCO_ENABLED = true ]
+then
+    AGENT="../jacoco/jacocoagent.jar"
+    COVERAGE="-javaagent:$AGENT=destfile=build/jacoco/h2o-core_multi.exec"
+    MAX_MEM="-Xmx8g"
+else
+    COVERAGE=""
+fi
+# Command to invoke test.
+JVM="nice $JAVA_CMD $COVERAGE $MAX_MEM -Xms3g -ea -cp build/classes/test${SEP}build/classes/main${SEP}../h2o-genmodel/build/libs/h2o-genmodel.jar${SEP}../lib/*"
 echo "$JVM" > $OUTDIR/jvm_cmd.txt
 
 # Tests
@@ -84,9 +105,17 @@ $JVM water.H2O -name $CLUSTER_NAME -baseport $CLUSTER_BASEPORT --ga_opt_out 1> $
 $JVM water.H2O -name $CLUSTER_NAME -baseport $CLUSTER_BASEPORT --ga_opt_out 1> $OUTDIR/out.3 2>&1 & PID_3=$!
 $JVM water.H2O -name $CLUSTER_NAME -baseport $CLUSTER_BASEPORT --ga_opt_out 1> $OUTDIR/out.4 2>&1 & PID_4=$!
 
+# If coverage is being run, then pass a system variable flag so that timeout limits are increased.
+if [ $JACOCO_ENABLED = true ]
+then
+    JACOCO_FLAG="-Dtest.jacocoEnabled=true"
+else
+    JACOCO_FLAG=""
+fi
+
 # Launch last driver JVM.  All output redir'd at the OS level to sandbox files.
 echo Running h2o-core junit tests...
-($JVM -Dai.h2o.name=$CLUSTER_NAME -Dai.h2o.baseport=$CLUSTER_BASEPORT -Dai.h2o.ga_opt_out=yes $JUNIT_RUNNER $JUNIT_TESTS_BOOT `cat $OUTDIR/tests.txt` 2>&1 ; echo $? > $OUTDIR/status.0) 1> $OUTDIR/out.0 2>&1
+($JVM -Dai.h2o.name=$CLUSTER_NAME -Dai.h2o.baseport=$CLUSTER_BASEPORT -Dai.h2o.ga_opt_out=yes $JACOCO_FLAG $JUNIT_RUNNER $JUNIT_TESTS_BOOT `cat $OUTDIR/tests.txt` 2>&1 ; echo $? > $OUTDIR/status.0) 1> $OUTDIR/out.0 2>&1
 
 grep EXECUTION $OUTDIR/out.0 | cut "-d " -f22,19 | awk '{print $2 " " $1}'| sort -gr | head -n 10 >> $OUTDIR/out.0
 
diff --git a/h2o-docs/src/booklets/v2_2015/source/DeepLearning_Vignette.tex b/h2o-docs/src/booklets/v2_2015/source/DeepLearning_Vignette.tex
index 0e5f132..880c50d 100644
--- a/h2o-docs/src/booklets/v2_2015/source/DeepLearning_Vignette.tex
+++ b/h2o-docs/src/booklets/v2_2015/source/DeepLearning_Vignette.tex
@@ -276,6 +276,7 @@ Specify the one of the following distribution functions for the response variabl
 
 \begin{itemize}
   \item Laplace
+  \item Quantile
   \item Huber
   \item Gaussian
 \end{itemize}\\
@@ -284,7 +285,7 @@ Specify the one of the following distribution functions for the response variabl
 
 \end{frame}
 
-Each distribution has a primary association with a particular loss function, but some distributions allow users to specify a non-default loss function from the group of loss functions specified in Table 2.  Bernoulli and multinomial are primarily associated with cross-entropy (also known as log-loss), Gaussian with Mean Squared Error, Laplace with Absolute loss and Huber with Huber loss.  For Poisson, Gamma, and Tweedie distributions, the loss function cannot be changed, so \texttt{loss} must be set to AUTO.    
+Each distribution has a primary association with a particular loss function, but some distributions allow users to specify a non-default loss function from the group of loss functions specified in Table 2.  Bernoulli and multinomial are primarily associated with cross-entropy (also known as log-loss), Gaussian with Mean Squared Error, Laplace with Absolute loss (a special case of Quantile with \texttt{quantile\_alpha=0.5}) and Huber with Huber loss.  For Poisson, Gamma, and Tweedie distributions, the loss function cannot be changed, so \texttt{loss} must be set to AUTO.    
 
 %The following choices for the loss function $L(W,B$ $|$ $ j)$ are summarized in Table 2. 
 
@@ -487,6 +488,9 @@ Early stopping based on convergence of a user-specified metric is an especially
 \item To compute the best number of epochs with cross-validation, simply specify \texttt{stopping\_rounds>0} as in the examples above, in combination with \texttt{nfolds>1}, and the main model will pick the ideal number of epochs from the convergence behavior of the \texttt{nfolds} cross-validation models.
 \end{itemize}
 
+\subsubsection{Time-based Early Stopping}
+To stop model training after a given amount of seconds, specify \texttt{max\_runtime\_secs > 0}. This option is also available for grid searches and models with cross-validation. Note: The model(s) will likely end up with fewer epochs than specified by \texttt{epochs}.
+
 \subsection{Additional Parameters} 
 
 Since there are dozens of possible parameter arguments when creating models, configuring H2O Deep Learning models may seem daunting. However, most parameters do not need to be modified; the default settings are recommended for most use cases. 
@@ -916,8 +920,9 @@ Refer to {\textbf{\nameref{sssec:AdaptiveLearning}}} for more details.
 
 \item \texttt{loss}: Specifies the loss option: \texttt{Automatic, CrossEntropy} (classification only), \texttt{Quadratic, Absolute}, or \texttt{Huber}. The default is \texttt{Automatic}. Refer to {\textbf{\nameref{sssec:ActivationLoss}}} for more details.
 
-\item \texttt{distribution}: Specifies the distribution function of the response: \texttt{AUTO, bernoulli, multinomial, poisson, gamma, \\ tweedie, laplace, huber,} or \texttt{gaussian}. 
+\item \texttt{distribution}: Specifies the distribution function of the response: \texttt{AUTO, bernoulli, multinomial, poisson, gamma, \\ tweedie, laplace, quantile, huber,} or \texttt{gaussian}. 
 
+\item {\texttt{quantile\_alpha}}: Desired quantile for quantile regression (from 0.0 to 1.0) when \texttt{distribution = "quantile"}.  The default is 0.5 (median, same as \texttt{distribution = "laplace"}).
 \item \texttt{tweedie\_power}: Specifies the Tweedie power when \texttt{distribution} is  \texttt{tweedie}. The range is from 1.0 to 2.0. 
 
 \item \texttt{score\_interval}: Specifies the minimum time (in seconds) between model scoring. The actual interval is determined by the number of training samples per iteration and the scoring duty cycle. To use all training set samples, specify \texttt{0}. The default is 5.
@@ -938,6 +943,8 @@ Refer to {\textbf{\nameref{sssec:AdaptiveLearning}}} for more details.
 
 \item \texttt{stopping\_tolerance}: Relative tolerance for metric-based stopping criterion Relative tolerance for metric-based stopping criterion (stop if relative improvement is not at least this much).
 
+\item \texttt{max\_runtime\_secs}: Maximum allowed runtime in seconds for model training. Use 0 to disable.
+
 \item \texttt{quiet\_mode}: Logical. Enables quiet mode for less output to standard output. The default is false.
 
 \item \texttt{max\_confusion\_matrix\_size}: For classification models, specifies the maximum size (in terms of classes) for displaying the confusion matrix. This option helps avoid printing extremely large confusion matrices. The default is 20.
diff --git a/h2o-docs/src/booklets/v2_2015/source/GBM_Vignette.tex b/h2o-docs/src/booklets/v2_2015/source/GBM_Vignette.tex
index 0c121cf..d81ca9a 100644
--- a/h2o-docs/src/booklets/v2_2015/source/GBM_Vignette.tex
+++ b/h2o-docs/src/booklets/v2_2015/source/GBM_Vignette.tex
@@ -23,9 +23,9 @@
 \\
 \bigskip
 \small
-\textsc{Cliff Click  \hspace{10pt} Michal Malohlava \hspace{10pt} Viraj Parmar}
+\textsc{Cliff Click  \hspace{10pt} Michal Malohlava \hspace{10pt} Arno Candel}
 
-\textsc{Hank Roark \hspace{10pt} Arno Candel}
+\textsc{Hank Roark \hspace{10pt} Viraj Parmar}
 
 \textsc{Edited by: Jessica Lanford}
 
@@ -61,7 +61,7 @@
 Gradient Boosted Models with H2O\\
 
   by Cliff Click, Michal Malohlava, \\
-Viraj Parmar, Hank Roark \&\  Arno Candel\\
+Arno Candel, Hank Roark \&\  Viraj Parmar\\
 Edited by: Jessica Lanford
 
 \bigskip
@@ -117,9 +117,7 @@ Topics include:
 
 \input{common/what_is_h2o.tex}
 
-%----------------------------------------------------------------------
-%----------------------------------------------------------------------
-%\input{generated_buildinfo.tex}
+\input{generated_buildinfo.tex}
 
 \input{common/installation.tex}
 
@@ -156,8 +154,8 @@ H2O's GBM functionalities include:
 \item fast and memory-efficient Java implementations of the algorithms
 \item the ability to run H2O from R, Python, Scala, or the intuitive web UI (Flow)
 \item automatic early stopping based on convergence of user-specified metrics to user-specified relative tolerance
-\item stochastic gradient boosting with column and row sampling for better generalization
-\item support for exponential families (Poisson, Gamma, Tweedie) and loss functions in addition to binomial (Bernoulli), Gaussian and multinomial distributions
+\item stochastic gradient boosting with column and row sampling (per split and per tree) for better generalization
+\item support for exponential families (Poisson, Gamma, Tweedie) and loss functions in addition to binomial (Bernoulli), Gaussian and multinomial distributions, such as Quantile regression
 \item grid search for hyperparameter optimization and model selection
 \item model export in plain Java code for deployment in production environments
 \item additional parameters for model tuning (for a complete listing of parameters, refer to the {\textbf{\nameref{ssec:Model Parameters}}} section.)
@@ -297,8 +295,11 @@ One nice feature for finding the optimal number of trees is early stopping based
 \item To compute the best number of trees with cross-validation, simply specify \texttt{stopping\_rounds>0} as in the examples above, in combination with \texttt{nfolds>1}, and the main model will pick the ideal number of trees from the convergence behavior of the \texttt{nfolds} cross-validation models.
 \end{itemize}
 
+\subsubsection{Time-based Early Stopping}
+To stop model training after a given amount of seconds, specify \texttt{max\_runtime\_secs > 0}. This option is also available for grid searches and models with cross-validation. Note: The model(s) will likely end up with fewer trees than specified by \texttt{ntrees}.
+
 \subsubsection{Stochastic GBM}
-Stochastic GBM is a way to improve generalization by sampling columns (per split) and rows (per tree) during the model building process. To control the sampling ratios use \texttt{sample\_rate} for rows and \texttt{col\_sample\_rate} for columns. Both parameters must range from 0 to 1.
+Stochastic GBM is a way to improve generalization by sampling columns (per split) and rows (per tree) during the model building process. To control the sampling ratios use \texttt{sample\_rate} for rows (per tree), \texttt{col\_sample\_rate\_per\_tree} for columns per tree and \texttt{col\_sample\_rate} for columns per split. All three parameters must range from 0 to 1, and default to 1.
 
 \section{Use Case: Airline Data Classification}
 Download the Airline dataset from: {\url{https://github.com/h2oai/h2o-2/blob/master/smalldata/airlines/allyears2k_headers.zip}} and save the .csv file to your working directory. 
@@ -438,7 +439,7 @@ Of course, sets of other parameters can be specified for a larger space of model
 
 
 \newpage
-\subsection{ Model Parameters}
+\subsection{Model Parameters}
 \label{ssec:Model Parameters}
 This section describes the functions of the parameters for GBM. 
 \begin{itemize}
@@ -453,13 +454,16 @@ This section describes the functions of the parameters for GBM.
 \item {\texttt{min\_rows}}: The minimum number of rows to assign to the terminal nodes. The default is 10.
 \item {\texttt{nbins}}: For numerical columns (real/int), build a histogram of at least the specified number of bins, then split at the best point The default is 20.
 \item {\texttt{nbins\_cats}}: For categorical columns (enum), build a histogram of the specified number of bins, then split at the best point. Higher values can lead to more overfitting.  The default is 1024. \label{nbins_cats}
+\item {\texttt{nbins\_top\_level}}: For numerical columns (real/int), build a histogram of (at most) this many bins at the root level, then decrease by factor of two per level.
 \item {\texttt{seed}}: Seed containing random numbers that affects sampling.
 \item{\texttt{mtries}}: Number of variables randomly sampled as candidates at each split. If {\texttt{-1}}, the square root of $p$ is used for classification,or $p/3$ for regression (where $p$ is the number of predictors). 
 \item{\texttt{sample\_rate}}: Row sample rate (from 0.0 to 1.0). 
+\item{\texttt{col\_sample\_rate}}: Column sample rate (per split) (from 0.0 to 1.0). 
 \item{\texttt{col\_sample\_rate\_per\_tree}}: Column sample rate per tree (from 0.0 to 1.0). 
 \item {\texttt{learn\_rate}}: An integer that defines the learning rate. The default is 0.1 and the range is 0.0 to 1.0.
-\item {\texttt{distribution}}: The distribution function options: {\texttt{AUTO, bernoulli, multinomial, gaussian, poisson, gamma}} or {\texttt{tweedie}}. The default is {\texttt{AUTO}}.
+\item {\texttt{distribution}}: The distribution function options: \texttt{AUTO, bernoulli, multinomial, gaussian, poisson, gamma, laplace,} \\\texttt{quantile} or {\texttt{tweedie}}. The default is {\texttt{AUTO}}.
 \item {\texttt{score\_each\_iteration}}: A boolean indicating whether to score during each iteration of model training.  The default is  {\texttt{FALSE}}.
+\item {\texttt{score\_tree\_interval}}: Score the model after every so many trees. Disabled if set to 0.
 \item \texttt{fold\_assignment}: Cross-validation fold assignment scheme, if  \\ \texttt{fold\_column} is not specified. The following options are supported: \texttt{AUTO, Random,} or \texttt{Modulo}. 
 \item \texttt{fold\_column}:  Column with cross-validation fold index assignment per observation. 
 \item \texttt{offset\_column}: Specify the offset column. {\textbf{Note}}: Offsets are per-row bias values that are used during model training. For Gaussian distributions, they can be seen as simple corrections to the response (y) column. Instead of learning to predict the response (y-row), the model learns to predict the (row) offset of the response column. For other distributions, the offset corrections are applied in the linearized space before applying the inverse link function to get the actual response values. 
@@ -468,19 +472,18 @@ This section describes the functions of the parameters for GBM.
 \item {\texttt{max\_confusion\_matrix\_size}}: Maximum size (number of classes) for confusion matrices to print in the H2O logs.  The default is 20.
 \item {\texttt{max\_hit\_ratio\_k}}: (for multi-class only) Maximum number (top K) of predictions to use for hit ratio computation.  To disable, enter  {\texttt{0}}. The default is 10.
 \item {\texttt{r2\_stopping}}: Stop making trees when the $R^2$ metric equals or exceeds this value.  The default is 0.999999.
-\item \texttt{regression\_stop}: Specifies the stopping criterion for regression error (MSE) on the training data scoring dataset. When the error is at or below this threshold, training stops. The default is 1e-6.  To disable, specify \texttt{-1}.
 \item \texttt{stopping\_rounds}: Early stopping based on convergence of \\\texttt{stopping\_metric}. Stop if simple moving average of length k of the \texttt{stopping\_metric} does not improve for k:=\texttt{stopping\_rounds} scoring events. Can only trigger after at least 2k scoring events. To disable, specify \texttt{0}.
 \item \texttt{stopping\_metric}: Metric to use for early stopping (\texttt{AUTO}: \texttt{logloss} for classification, \texttt{deviance} for regression). Can be any of \texttt{AUTO, deviance, logloss, MSE, AUC, r2, misclassification}.
 \item \texttt{stopping\_tolerance}: Relative tolerance for metric-based stopping criterion Relative tolerance for metric-based stopping criterion (stop if relative improvement is not at least this much).
-\item {\texttt{build\_tree\_one\_node}}: Specify if GBM should be run on one node only; no network 
-overhead but fewer CPUs used. Suitable for small datasets.  The default is {\texttt{FALSE}}.
+\item \texttt{max\_runtime\_secs}: Maximum allowed runtime in seconds for model training. Use 0 to disable.
+\item {\texttt{build\_tree\_one\_node}}: Specify if GBM should be run on one node only; no network overhead but fewer CPUs used. Suitable for small datasets.  The default is {\texttt{FALSE}}.
 \item {\texttt{binomial\_double\_trees}}: For binary classification: Builds twice as many trees (one per class), which can result in better accuracy. 
+\item {\texttt{quantile\_alpha}}: Desired quantile for quantile regression (from 0.0 to 1.0) when \texttt{distribution = "quantile"}.  The default is 0.5 (median, same as \texttt{distribution = "laplace"}).
 \item {\texttt{tweedie\_power}}: A numeric specifying the power for the Tweedie function when \texttt{distribution = "tweedie"}.  The default is 1.5.
 \item {\texttt{checkpoint}}: Enter a model key associated with a previously-trained model. Use this option to build a new model as a continuation of a previously-generated model.
 \item {\texttt{keep\_cross\_validation\_predictions}}: Specify whether to keep the predictions of the cross-validation models.   The default is {\texttt{FALSE}}.
 \item {\texttt{class\_sampling\_factors}}: Desired over/under-sampling ratios per class (in lexicographic order). If not specified, sampling factors will be automatically computed to obtain class balance during training. Requires \texttt{balance\_classes}.
 \item {\texttt{max\_after\_balance\_size}}: Maximum relative size of the training data after balancing class counts; can be less than 1.0.  The default is 5.
-\item {\texttt{nbins\_top\_level}}: For numerical columns (real/int), build a histogram of (at most) this many bins at the root level, then decrease by factor of two per level.
 \item \texttt{model\_id}: The unique ID assigned to the generated model. If not specified, an ID is generated automatically.
 \end{itemize}
 
@@ -532,11 +535,14 @@ overhead but fewer CPUs used. Suitable for small datasets.  The default is {\tex
 
 Cliff Click is the CTO and Co-Founder of H2O, makers of H2O, the open-source math and machine learning engine for Big Data. Cliff is invited to speak regularly at industry and academic conferences and has published many papers about HotSpot technology. He holds a PhD in Computer Science from Rice University and about 15 patents.
 
-
 \textbf{Michal Malohlava}
 
 Michal is a geek, developer, Java, Linux, programming languages enthusiast developing software for over 10 years. He obtained PhD from the Charles University in Prague in 2012 and post-doc at Purdue University. He participated in design and development of various systems including SOFA and Fractal component systems or jPapabench control system.
 
+\textbf{Arno Candel}
+
+Arno is the Chief Architect of H2O, a distributed and scalable open-source machine learning platform and the main author of H2O Deep Learning.  Arno holds a PhD and Masters summa cum laude in Physics from ETH Zurich, Switzerland. He has authored dozens of scientific papers and is a sought-after conference speaker. Arno was named 2014 Big Data All-Star by Fortune Magazine. Follow him on Twitter: @ArnoCandel.
+
 \textbf{Viraj Parmar}
 
 Prior to joining H2O as a data and math hacker intern, Viraj worked in a research group at the MIT Center for Technology and Design. His interests are in software engineering and large-scale machine learning. 
@@ -545,10 +551,6 @@ Prior to joining H2O as a data and math hacker intern, Viraj worked in a researc
 
 Hank is a Data Scientist and Hacker at H2O. Hank comes to H2O with a background turning data into products and system solutions and loves helping others find value in their data.  Hank has an SM from MIT in Engineering and Management and BS Physics from Georgia Tech.
 
-\textbf{Arno Candel}
-
-Arno is the Chief Architect of H2O, a distributed and scalable open-source machine learning platform and the main author of H2O Deep Learning.  Arno holds a PhD and Masters summa cum laude in Physics from ETH Zurich, Switzerland. He has authored dozens of scientific papers and is a sought-after conference speaker. Arno was named 2014 Big Data All-Star by Fortune Magazine. Follow him on Twitter: @ArnoCandel.
-
 \textbf{Jessica Lanford}
 
 Jessica is a word hacker and seasoned technical communicator at H2O.ai. She brings our product to life by documenting the many features and functionality of H2O. Having worked for some of the top companies in technology including Dell, AT$\&$T, and Lam Research, she is an expert at translating complex ideas to digestible articles.
diff --git a/h2o-docs/src/booklets/v2_2015/source/GLM_Vignette.tex b/h2o-docs/src/booklets/v2_2015/source/GLM_Vignette.tex
index a807bdd..2ff8b9e 100644
--- a/h2o-docs/src/booklets/v2_2015/source/GLM_Vignette.tex
+++ b/h2o-docs/src/booklets/v2_2015/source/GLM_Vignette.tex
@@ -369,8 +369,28 @@ function. Change the regression by changing the family to binomial.
 \waterExampleInR
 \lstinputlisting[style=R]{GLM_Vignette_code_examples/glm_binomial_example.R}
 
+
+\subsubsection{Multi-class classification (Multinomial Family)}
+Multinomial family generalization of binomial model to multi-class response. Similarly to binomial family, we will model conditional probability of observing class c given x. We will thus have a vector of coefficients for each of the output classes ($\beta$ is now a matrix). The probabilities are defined as 
+
+$$ \hat{y}_c = Pr(y = c|x) = {e^{x^{\top}\beta_c + \beta_{c0}} \over \sum_{k=1}^K (e^{x^{\top}\beta_k + \beta_{k0}}) }$$ 
+
+And the penalized negative log-likelihood of the problem is defined as:
+
+$$
+- \lbrack {1\over{N}} \sum_{i=1}^N\sum_{k=1}^K(y_{i,k}(x^{\top}\beta_k + \beta_{c0})) - log(\sum_{k=1}^Ke^{x^{\top}\beta_k + \beta_{k0}}) \rbrack + \lambda \lbrack {(1-\alpha) \over 2} \| \beta \|_F^2 + \sum_{j=1}^P \|beta_j\|_1 \rbrack 
+$$
+, where $\beta_c$ is vector of coefficients for class c and $y_{i,k}$ is kth element of a binary vector produced by expanding the response variable using one-hot encoding (i.e. $y_{i,k} == 1$ iff response at the ith observation is k, 0 otherwise. 
+
+Simple example using iris dataset:
+
+
+\waterExampleInR
+\lstinputlisting[style=R]{GLM_Vignette_code_examples/glm_multinomial.R}
+
 \waterExampleInPython
-\lstinputlisting[style=python]{GLM_Vignette_code_examples/glm_binomial_example.py}
+\lstinputlisting[style=python]{GLM_Vignette_code_examples/glm_multinomial.py}
+
 
 \subsubsection{Poisson Models}
 Poisson regression is typically used for datasets where the response represents counts and the errors are assumed to have a Poisson distribution. In general, it can be applied to any data where the response is non-negative. It models the dependency between the response and covariates as: 
@@ -658,6 +678,26 @@ unit variance.  This parameter must be enabled (using \texttt{standardize=TRUE})
 
 We recommend enabling standardization when using regularization (i.e. \texttt{lambda} is chosen by H2O or greater than 0). Only advanced users should disable standardization.
 
+\subsubsection{Auto-remove collinear columns}
+Collinear columns can cause problems during model fitting. The preferred way to deal with collinearity is to add some regularization (either L1, L2 or Elastic Net) which is the default H2O behavior. However, if you want non-regularized solution, you can choose to automatically remove collinear columns by setting the \texttt{remove\_collinear\_columns} option.
+
+The option can only be used with \texttt{IRLSM} solver and no regularization. If selected, H2O will automatically remove columns if it detects collinearity. Which columns are removed is depends on the order of the columns in the vector of coefficients (Intercept first, then categorical variables ordered by cadrinality from largest to smallest, then numbers).  
+
+\waterExampleInR
+\lstinputlisting[style=R]{GLM_Vignette_code_examples/glm_remove_collinear_columns.R}
+
+
+\subsubsection{P-Values}
+Z-score, standard error and p-values are classical statistical way of evaluating model quality. The p-values are essentially a hypothesis test on the value of each coefficient. High p-value means that the coefficient is unreliable (insiginificant) while low p-value suggest that the coefficient is statistically significant. 
+
+You can request p-values by setting the \\\texttt{compute\_p\_values} flag. It can only be used with \texttt{IRLSM} solver and no regularization. It is recommended that you also set \texttt{remove\_collinear\_columns} option, otherwise H2O will return an error (it can not add regularization if P-values are requested) if there are collinear columns.
+
+\textbf{Note:} GLM auto-standardizes data by default (recommended), this changes the p-value of the constant term (intercept). 
+
+\waterExampleInR
+\lstinputlisting[style=R]{GLM_Vignette_code_examples/glm_p_values.R}
+
+
 \subsubsection{K-fold Cross-Validation}
 
 All validation values can be computed using either the training dataset (the default option) or using K-fold
@@ -711,7 +751,7 @@ it is therefore less numerically stable and can be very slow as well due to slow
 
 \subsubsection{Offsets}
 
-\texttt{offset\_column} is an optional column name or index referring to a column in the training frame. This column specifies a prior known component to be included in the linear predictor during training. Offsets are per-row bias values that are used during model training. 
+\texttt{offset\_column} is an optional column name or index referring to a column in the training frame. This column specifies a prior known component to be included in the linear predictor during training. Offsets are per-row ``bias values'' that are used during model training. 
 
 For Gaussian distributions, they can be seen as simple corrections to the response (\texttt{y}) column. Instead of learning to predict the response (\texttt{y}-row), the model learns to predict the (row) offset of the response column. 
 
@@ -740,6 +780,8 @@ The proximal operator interface allows you to run the GLM with a proximal penalt
 given solution. There are many potential uses: for example, it can be used as part of an ADMM consensus algorithm
 to obtain a unified solution over separate H2O clouds or in Bayesian regression approximation.
 
+
+
 %----------------------------------------------------------------------
 %----------------------------------------------------------------------
 
@@ -1200,7 +1242,7 @@ H2O supports binomial models only; any extra levels in the test response will ge
 \item \texttt{tweedie\_variance\_power}: A numeric specifying the power for the variance function when \texttt{family = "tweedie"}. 
 \item \texttt{tweedie\_link\_power}: A numeric specifying the power for the link function when \texttt{family = "tweedie"}. 
 \item \texttt{alpha}: The elastic-net mixing parameter, which must be in $[0,1]$. The penalty is defined to be $P(\alpha,\beta) = (1-\alpha)/2||\beta||_2^2 + \alpha||\beta||_1 = \sum_j [(1-\alpha)/2 \beta_j^2 + \alpha|\beta_j|] $ so \texttt{alpha=1} is the Lasso penalty, while \texttt{alpha=0} is the ridge penalty. Default is 0.5.
-\item \texttt{prior}: (Optional) A numeric specifying the prior probability of class 1 in the response when \texttt{family = "binomial"}. The default value is the observation frequency of class 1. 
+\item \texttt{prior}: (Optional) A numeric specifying the prior probability of class 1 in the response when \texttt{family = "binomial"}. The default value is the observation frequency of class 1. Must be from (0,1) exclusive range or NULL (no prior).
 \item \texttt{lambda}: A non-negative value representing the shrinkage parameter, which multiplies $P(\alpha,\beta)$ in the objective. The larger lambda is, the more the coefficients are shrunk toward zero (and each other). When the value is 0, regularization is disabled and ordinary generalized linear models are fit. The default is 1e-05.
 \item \texttt{lambda\_search}: A logical value indicating whether to conduct a search over the space of lambda values, starting from the max lambda, given lambda will be interpreted as the min. lambda. Default is false.
 \item \texttt{nlambdas}: The number of lambda values when \texttt{lambda\_search = TRUE}. Default is -1.
@@ -1213,6 +1255,7 @@ H2O supports binomial models only; any extra levels in the test response will ge
 \item \texttt{offset\_column}: Specify the offset column. Note: Offsets are per-row bias values that are used during model training. For Gaussian distributions, they can be seen as simple corrections to the response (y) column. Instead of learning to predict the response (y-row), the model learns to predict the (row) offset of the response column. For other distributions, the offset corrections are applied in the linearized space before applying the inverse link function to get the actual response values. 
 \item \texttt{weights\_column}: Specify the weights column. Note: Weights are per-row observation weights. This is typically the number of times a row is repeated, but non-integer values are supported as well. During training, rows with higher weights matter more, due to the larger loss function pre-factor.
 \item \texttt{intercept}: Logical; includes a constant term (intercept) in the model. If there are factor columns in your model, then the intercept must be included. %% <--- still true? copied from `has_intercept`
+\item \texttt{max\_runtime\_secs}: Maximum allowed runtime in seconds for model training. Use 0 to disable.
 \end{itemize}
 %Additional: 
 %Strong Rules Enables: Uses strong rules to filter out inactive columns. Default is true. 
@@ -1269,7 +1312,7 @@ Tom is VP of Engineering of Customer and Pre-Sales Engineering at H2O and key to
 
 \textbf{Nadine Hussami}
 
-Nadine is a math hacker intern at H2O. She is a PhD student at Stanford University. Her main research interests include statistical learning and convex optimization. Before Stanford, she worked as an algorithm development engineer at Bina Technologies. She holds a Masters degree from Stanford in Electrical Engineering. 
+Nadine is a math hacker intern at H2O. She is a PhD student at Stanford University. Her main research interests include statistical learning and convex optimization. Before Stanford, she worked as an algorithm development engineer at Bina Technologies. She holds a Masters degree from Stanford in Electrical Engineering. 
 
 \textbf{Ariel Rao}
 
diff --git a/h2o-docs/src/booklets/v2_2015/source/GLM_Vignette_code_examples/glm_multinomial.R b/h2o-docs/src/booklets/v2_2015/source/GLM_Vignette_code_examples/glm_multinomial.R
new file mode 100644
index 0000000..5b60e0b
--- /dev/null
+++ b/h2o-docs/src/booklets/v2_2015/source/GLM_Vignette_code_examples/glm_multinomial.R
@@ -0,0 +1,3 @@
+iris_h2o = as.h2o(iris)
+m = h2o.glm(training_frame=iris_h2o,y="Species",x=1:4,family="multinomial")
+prit(m)
\ No newline at end of file
diff --git a/h2o-docs/src/booklets/v2_2015/source/GLM_Vignette_code_examples/glm_multinomial.py b/h2o-docs/src/booklets/v2_2015/source/GLM_Vignette_code_examples/glm_multinomial.py
new file mode 100644
index 0000000..9041adc
--- /dev/null
+++ b/h2o-docs/src/booklets/v2_2015/source/GLM_Vignette_code_examples/glm_multinomial.py
@@ -0,0 +1,6 @@
+import h2o
+from h2o.estimators.glm import H2OGeneralizedLinearEstimator
+h2o.init()
+h2o_df = h2o.import_file("http://h2o-public-test-data.s3.amazonaws.com/smalldata/iris/iris.csv")
+multinomial_fit = H2OGeneralizedLinearEstimator(family = "multinomial")
+multinomial_fit.train(y = 4, x = [0,1,2,3], training_frame = h2o_df)
\ No newline at end of file
diff --git a/h2o-docs/src/booklets/v2_2015/source/GLM_Vignette_code_examples/glm_p_values.R b/h2o-docs/src/booklets/v2_2015/source/GLM_Vignette_code_examples/glm_p_values.R
new file mode 100644
index 0000000..c2bb821
--- /dev/null
+++ b/h2o-docs/src/booklets/v2_2015/source/GLM_Vignette_code_examples/glm_p_values.R
@@ -0,0 +1,9 @@
+library(h2o)
+h2o.init()
+a = runif(100)
+b = runif(100)
+c = -3*a + 10
+df = data.frame(a,b,c)
+h2o_df = as.h2o(df)
+h2o.fit = h2o.glm(y = "c", x = c("a", "b"), training_frame = h2o_df, lambda=0,remove_collinear_columns=TRUE,compute_p_values=TRUE)
+h2o.fit
\ No newline at end of file
diff --git a/h2o-docs/src/booklets/v2_2015/source/GLM_Vignette_code_examples/glm_remove_collinear_columns.R b/h2o-docs/src/booklets/v2_2015/source/GLM_Vignette_code_examples/glm_remove_collinear_columns.R
new file mode 100644
index 0000000..0c5b5e5
--- /dev/null
+++ b/h2o-docs/src/booklets/v2_2015/source/GLM_Vignette_code_examples/glm_remove_collinear_columns.R
@@ -0,0 +1,9 @@
+library(h2o)
+h2o.init()
+a = runif(100)
+b = 2*a
+c = -3*a + 10
+df = data.frame(a,b,c)
+h2o_df = as.h2o(df)
+h2o.fit = h2o.glm(y = "c", x = c("a", "b"), training_frame = h2o_df, lambda=0,remove_collinear_columns=TRUE)
+h2o.fit
\ No newline at end of file
diff --git a/h2o-docs/src/product/flow/packs/test-large/100KRows2-5Cols.flow b/h2o-docs/src/product/flow/packs/test-large/100KRows2-5Cols.flow
index 26dcc55..b26c7e4 100644
--- a/h2o-docs/src/product/flow/packs/test-large/100KRows2-5Cols.flow
+++ b/h2o-docs/src/product/flow/packs/test-large/100KRows2-5Cols.flow
@@ -35,7 +35,7 @@
     },
     {
       "type": "cs",
-      "input": "buildModel 'glm', {\"model_id\":\"glm-83f9f370-ddc7-449f-895c-878a0d4152af\",\"training_frame\":\"WU_100KRows2.hex\",\"response_column\":\"C1\",\"solver\":\"IRLSM\",\"max_iterations\":50,\"beta_epsilon\":0,\"standardize\":true,\"family\":\"gaussian\",\"balance_classes\":false,\"link\":\"family_default\",\"tweedie_variance_power\":\"NaN\",\"tweedie_link_power\":\"NaN\",\"alpha\":[0.5],\"lambda\":[0.00001],\"lambda_search\":false,\"class_sampling_factors\":[],\"max_after_balance_size\":5,\"prior\":0,\"nlambdas\":-1,\"lambda_min_ratio\":-1}"
+      "input": "buildModel 'glm', {\"model_id\":\"glm-83f9f370-ddc7-449f-895c-878a0d4152af\",\"training_frame\":\"WU_100KRows2.hex\",\"response_column\":\"C1\",\"solver\":\"IRLSM\",\"max_iterations\":50,\"beta_epsilon\":0,\"standardize\":true,\"family\":\"gaussian\",\"balance_classes\":false,\"link\":\"family_default\",\"tweedie_variance_power\":\"NaN\",\"tweedie_link_power\":\"NaN\",\"alpha\":[0.5],\"lambda\":[0.00001],\"lambda_search\":false,\"class_sampling_factors\":[],\"max_after_balance_size\":5,\"prior\":-1,\"nlambdas\":-1,\"lambda_min_ratio\":-1}"
     },
     {
       "type": "cs",
diff --git a/h2o-docs/src/product/flow/packs/test-large/100KRows3KCols.flow b/h2o-docs/src/product/flow/packs/test-large/100KRows3KCols.flow
index 25da6d9..8c4c197 100644
--- a/h2o-docs/src/product/flow/packs/test-large/100KRows3KCols.flow
+++ b/h2o-docs/src/product/flow/packs/test-large/100KRows3KCols.flow
@@ -35,7 +35,7 @@
     },
     {
       "type": "cs",
-      "input": "buildModel 'glm', {\"model_id\":\"glm-f508fbfc-907c-4a39-b091-e64c5514c1ad\",\"training_frame\":\"WU_100KRows3KCols.hex\",\"response_column\":\"C1\",\"solver\":\"L_BFGS\",\"max_iterations\":50,\"beta_epsilon\":0,\"standardize\":true,\"family\":\"gaussian\",\"balance_classes\":false,\"link\":\"family_default\",\"tweedie_variance_power\":\"NaN\",\"tweedie_link_power\":\"NaN\",\"alpha\":[0.5],\"lambda\":[0.00001],\"lambda_search\":false,\"class_sampling_factors\":[],\"max_after_balance_size\":5,\"prior\":0,\"nlambdas\":-1,\"lambda_min_ratio\":-1}"
+      "input": "buildModel 'glm', {\"model_id\":\"glm-f508fbfc-907c-4a39-b091-e64c5514c1ad\",\"training_frame\":\"WU_100KRows3KCols.hex\",\"response_column\":\"C1\",\"solver\":\"L_BFGS\",\"max_iterations\":50,\"beta_epsilon\":0,\"standardize\":true,\"family\":\"gaussian\",\"balance_classes\":false,\"link\":\"family_default\",\"tweedie_variance_power\":\"NaN\",\"tweedie_link_power\":\"NaN\",\"alpha\":[0.5],\"lambda\":[0.00001],\"lambda_search\":false,\"class_sampling_factors\":[],\"max_after_balance_size\":5,\"prior\":-1,\"nlambdas\":-1,\"lambda_min_ratio\":-1}"
     },
     {
       "type": "cs",
diff --git a/h2o-docs/src/product/flow/packs/test-large/1MRows3KCols.flow b/h2o-docs/src/product/flow/packs/test-large/1MRows3KCols.flow
index fc5fa58..618a5a6 100644
--- a/h2o-docs/src/product/flow/packs/test-large/1MRows3KCols.flow
+++ b/h2o-docs/src/product/flow/packs/test-large/1MRows3KCols.flow
@@ -35,7 +35,7 @@
     },
     {
       "type": "cs",
-      "input": "buildModel 'glm', {\"model_id\":\"glm-6f37cc1d-9128-42fc-a964-adce52f47f64\",\"training_frame\":\"WU_1MRows3KCols.hex\",\"response_column\":\"C1\",\"solver\":\"L_BFGS\",\"max_iterations\":50,\"beta_epsilon\":0,\"standardize\":true,\"family\":\"gaussian\",\"balance_classes\":false,\"link\":\"family_default\",\"tweedie_variance_power\":\"NaN\",\"tweedie_link_power\":\"NaN\",\"alpha\":[0.5],\"lambda\":[0.00001],\"lambda_search\":false,\"class_sampling_factors\":[],\"max_after_balance_size\":5,\"prior\":0,\"nlambdas\":-1,\"lambda_min_ratio\":-1}"
+      "input": "buildModel 'glm', {\"model_id\":\"glm-6f37cc1d-9128-42fc-a964-adce52f47f64\",\"training_frame\":\"WU_1MRows3KCols.hex\",\"response_column\":\"C1\",\"solver\":\"L_BFGS\",\"max_iterations\":50,\"beta_epsilon\":0,\"standardize\":true,\"family\":\"gaussian\",\"balance_classes\":false,\"link\":\"family_default\",\"tweedie_variance_power\":\"NaN\",\"tweedie_link_power\":\"NaN\",\"alpha\":[0.5],\"lambda\":[0.00001],\"lambda_search\":false,\"class_sampling_factors\":[],\"max_after_balance_size\":5,\"prior\":-1,\"nlambdas\":-1,\"lambda_min_ratio\":-1}"
     },
     {
       "type": "cs",
diff --git a/h2o-docs/src/product/flow/packs/test-medium/BigCross.flow b/h2o-docs/src/product/flow/packs/test-medium/BigCross.flow
index c3831d1..31db8e9 100644
--- a/h2o-docs/src/product/flow/packs/test-medium/BigCross.flow
+++ b/h2o-docs/src/product/flow/packs/test-medium/BigCross.flow
@@ -35,7 +35,7 @@
     },
     {
       "type": "cs",
-      "input": "buildModel 'glm', {\"model_id\":\"glm-62983aab-d0ea-4fdb-af4a-3a254f47bd98\",\"training_frame\":\"BigCross1.hex\",\"response_column\":\"C1\",\"solver\":\"L_BFGS\",\"max_iterations\":-1,\"beta_epsilon\":0,\"standardize\":true,\"family\":\"gaussian\",\"balance_classes\":true,\"link\":\"family_default\",\"alpha\":[0.5],\"lambda\":[],\"lambda_search\":false,\"class_sampling_factors\":[],\"max_after_balance_size\":5,\"prior\":0,\"nlambdas\":-1,\"lambda_min_ratio\":-1}"
+      "input": "buildModel 'glm', {\"model_id\":\"glm-62983aab-d0ea-4fdb-af4a-3a254f47bd98\",\"training_frame\":\"BigCross1.hex\",\"response_column\":\"C1\",\"solver\":\"L_BFGS\",\"max_iterations\":-1,\"beta_epsilon\":0,\"standardize\":true,\"family\":\"gaussian\",\"balance_classes\":true,\"link\":\"family_default\",\"alpha\":[0.5],\"lambda\":[],\"lambda_search\":false,\"class_sampling_factors\":[],\"max_after_balance_size\":5,\"prior\":-1,\"nlambdas\":-1,\"lambda_min_ratio\":-1}"
     },
     {
       "type": "cs",
diff --git a/h2o-docs/src/product/flow/packs/test-medium/covtype_data.flow b/h2o-docs/src/product/flow/packs/test-medium/covtype_data.flow
index b6524ed..3f7875e 100644
--- a/h2o-docs/src/product/flow/packs/test-medium/covtype_data.flow
+++ b/h2o-docs/src/product/flow/packs/test-medium/covtype_data.flow
@@ -31,7 +31,7 @@
     },
     {
       "type": "cs",
-      "input": "buildModel 'glm', {\"model_id\":\"glm-0e5d9a55-e702-474f-9af8-ba27bad8c5a0\",\"training_frame\":\"covtype.hex\",\"response_column\":\"C55\",\"solver\":\"IRLSM\",\"max_iterations\":-1,\"beta_epsilon\":0,\"standardize\":true,\"family\":\"gaussian\",\"balance_classes\":false,\"link\":\"family_default\",\"alpha\":[0.5],\"lambda\":[],\"lambda_search\":false,\"class_sampling_factors\":[],\"max_after_balance_size\":5,\"prior\":0,\"nlambdas\":-1,\"lambda_min_ratio\":-1}"
+      "input": "buildModel 'glm', {\"model_id\":\"glm-0e5d9a55-e702-474f-9af8-ba27bad8c5a0\",\"training_frame\":\"covtype.hex\",\"response_column\":\"C55\",\"solver\":\"IRLSM\",\"max_iterations\":-1,\"beta_epsilon\":0,\"standardize\":true,\"family\":\"gaussian\",\"balance_classes\":false,\"link\":\"family_default\",\"alpha\":[0.5],\"lambda\":[],\"lambda_search\":false,\"class_sampling_factors\":[],\"max_after_balance_size\":5,\"prior\":-1,\"nlambdas\":-1,\"lambda_min_ratio\":-1}"
     },
     {
       "type": "cs",
diff --git a/h2o-docs/src/product/flow/packs/test-small/glm_synthetic_syn_binary_100x3000.flow b/h2o-docs/src/product/flow/packs/test-small/glm_synthetic_syn_binary_100x3000.flow
index 137609b..6dec943 100755
--- a/h2o-docs/src/product/flow/packs/test-small/glm_synthetic_syn_binary_100x3000.flow
+++ b/h2o-docs/src/product/flow/packs/test-small/glm_synthetic_syn_binary_100x3000.flow
@@ -36,7 +36,7 @@
     },
     {
       "type": "cs",
-      "input": "buildModel 'glm', {\"model_id\":\"glm-27aae283-2784-4230-a000-19a5cbce7a43\",\"training_frame\":\"syn_binary_100x3000_train\",\"validation_frame\":\"syn_binary_100x3000_test\",\"ignore_const_cols\":true,\"response_column\":\"C1\",\"family\":\"gaussian\",\"solver\":\"IRLSM\",\"alpha\":[0.3],\"lambda\":[0.002],\"lambda_search\":false,\"standardize\":false,\"non_negative\":false,\"score_each_iteration\":false,\"max_iterations\":-1,\"link\":\"family_default\",\"intercept\":true,\"objective_epsilon\":0.00001,\"beta_epsilon\":0.0001,\"gradient_epsilon\":0.0001,\"prior\":-1,\"max_active_predictors\":-1}"
+      "input": "buildModel 'glm', {\"model_id\":\"glm-27aae283-2784-4230-a000-19a5cbce7a43\",\"training_frame\":\"syn_binary_100x3000_train\",\"validation_frame\":\"syn_binary_100x3000_test\",\"ignore_const_cols\":true,\"response_column\":\"C1\",\"family\":\"gaussian\",\"solver\":\"L_BFGS\",\"alpha\":[0.3],\"lambda\":[0.002],\"lambda_search\":false,\"standardize\":false,\"non_negative\":false,\"score_each_iteration\":false,\"max_iterations\":-1,\"link\":\"family_default\",\"intercept\":true,\"objective_epsilon\":0.00001,\"beta_epsilon\":0.0001,\"gradient_epsilon\":0.0001,\"prior\":-1,\"max_active_predictors\":-1}"
     },
     {
       "type": "cs",
diff --git a/h2o-docs/src/product/flow/packs/test-small/prostate.flow b/h2o-docs/src/product/flow/packs/test-small/prostate.flow
index 840e56d..903f915 100644
--- a/h2o-docs/src/product/flow/packs/test-small/prostate.flow
+++ b/h2o-docs/src/product/flow/packs/test-small/prostate.flow
@@ -31,7 +31,7 @@
     },
     {
       "type": "cs",
-      "input": "buildModel 'glm', {\"model_id\":\"glm-f3096748-1ed4-402e-8599-4579458ee71a\",\"training_frame\":\"prostate.hex\",\"response_column\":\"CAPSULE\",\"solver\":\"IRLSM\",\"max_iterations\":-1,\"beta_epsilon\":0,\"standardize\":true,\"family\":\"gaussian\",\"balance_classes\":false,\"link\":\"family_default\",\"alpha\":[0.5],\"lambda\":[],\"lambda_search\":false,\"class_sampling_factors\":[],\"max_after_balance_size\":5,\"prior\":0,\"nlambdas\":-1,\"lambda_min_ratio\":-1}"
+      "input": "buildModel 'glm', {\"model_id\":\"glm-f3096748-1ed4-402e-8599-4579458ee71a\",\"training_frame\":\"prostate.hex\",\"response_column\":\"CAPSULE\",\"solver\":\"IRLSM\",\"max_iterations\":-1,\"beta_epsilon\":0,\"standardize\":true,\"family\":\"gaussian\",\"balance_classes\":false,\"link\":\"family_default\",\"alpha\":[0.5],\"lambda\":[],\"lambda_search\":false,\"class_sampling_factors\":[],\"max_after_balance_size\":5,\"prior\":-1,\"nlambdas\":-1,\"lambda_min_ratio\":-1}"
     },
     {
       "type": "cs",
diff --git a/h2o-py/README.rst b/h2o-py/README.rst
index 91d1b17..451a09b 100644
--- a/h2o-py/README.rst
+++ b/h2o-py/README.rst
@@ -16,6 +16,12 @@ This module depends on *requests*, *tabulate*, and *scikit-learn* modules, all o
 
     $ pip install scikit-learn
 
+    $ pip install future
+    
+    $ pip install six
+ 
+    $ pip install wheel
+
 The H2O Python Module
 =====================
 
diff --git a/h2o-py/build.gradle b/h2o-py/build.gradle
index 2315019..cb41178 100644
--- a/h2o-py/build.gradle
+++ b/h2o-py/build.gradle
@@ -63,13 +63,21 @@ task smokeTest(type: Exec) {
     println "PyUnit smoke test (run.py --wipeall --testsize s)..."
     workingDir testsPath
     commandLine 'pwd'
-    commandLine 'python', '../../scripts/run.py', '--wipeall', '--testsize', 's'
+	def args = ['python', '../../scripts/run.py', '--wipeall', '--testsize', 's']
+    if (project.hasProperty("jacocoCoverage")) {
+        args << '--jacoco'
+    }
+    commandLine args
 }
 
 task cleanUpSmokeTest << {
     new File([T, "tests/results"].join(File.separator)).deleteDir()
 }
 
+task cleanCoverageData(type: Delete) {
+    delete files("${testsPath}/results/jacoco")
+}
+
 task cleaner << {
     println "Cleaning..."
     getProjectDir().toString()
diff --git a/h2o-py/h2o/connection.py b/h2o-py/h2o/connection.py
index 8f24fb5..bcce953 100644
--- a/h2o-py/h2o/connection.py
+++ b/h2o-py/h2o/connection.py
@@ -46,7 +46,7 @@ class H2OConnection(object):
   __ENCODING_ERROR__ = "replace"
 
   def __init__(self, ip, port, start_h2o, enable_assertions, license, nthreads, max_mem_size, min_mem_size, ice_root,
-               strict_version_check, proxy, https, insecure, username, password, max_mem_size_GB, min_mem_size_GB):
+               strict_version_check, proxy, https, insecure, username, password, force_connect, max_mem_size_GB, min_mem_size_GB):
     """
     Instantiate the package handle to the H2O cluster.
     :param ip: An IP address, default is "localhost"
@@ -64,7 +64,8 @@ class H2OConnection(object):
     :param https: Set this to True to use https instead of http.
     :param insecure: Set this to True to disable SSL certificate checking.
     :param username: Username to login with.
-    :param password: Password to login with. 
+    :param password: Password to login with.
+    :param force_connect: Set this to True to attempt to establish a connection even if the cluster reports as unhealthy
     :param max_mem_size_GB: DEPRECATED. Use max_mem_size.
     :param min_mem_size_GB: DEPRECATED. Use min_mem_size.
     :return: None
@@ -92,6 +93,7 @@ class H2OConnection(object):
     self._insecure = insecure
     self._username = username
     self._password = password
+    self.force_connect = force_connect
     self._session_id = None
     self._rest_version = __H2O_REST_API_VERSION__
     self._child = getattr(__H2OCONN__, "_child") if hasattr(__H2OCONN__, "_child") else None
@@ -229,7 +231,10 @@ class H2OConnection(object):
       try:
         cld = H2OConnection.get_json(url_suffix="Cloud")
         if not cld['cloud_healthy']:
-          raise ValueError("Cluster reports unhealthy status", cld)
+          if __H2OCONN__.force_connect:
+            print("WARNING: Cluster reports unhealthy status")
+          else:
+            raise ValueError("Cluster reports unhealthy status", cld)
         if cld['cloud_size'] >= size and cld['consensus']:
           if print_dots: print(" Connection successful!")
           return cld
diff --git a/h2o-py/h2o/estimators/deeplearning.py b/h2o-py/h2o/estimators/deeplearning.py
index 0c048e4..2ca580c 100644
--- a/h2o-py/h2o/estimators/deeplearning.py
+++ b/h2o-py/h2o/estimators/deeplearning.py
@@ -10,7 +10,7 @@ class H2ODeepLearningEstimator(H2OEstimator):
                nesterov_accelerated_gradient=None, input_dropout_ratio=None,
                hidden_dropout_ratios=None, l1=None, l2=None, max_w2=None,
                initial_weight_distribution=None, initial_weight_scale=None, loss=None,
-               distribution=None, tweedie_power=None, score_interval=None,
+               distribution=None, quantile_alpha=None, tweedie_power=None, score_interval=None,
                score_training_samples=None, score_validation_samples=None,
                score_duty_cycle=None, classification_stop=None, regression_stop=None,
                quiet_mode=None, max_confusion_matrix_size=None, max_hit_ratio_k=None,
@@ -105,7 +105,9 @@ class H2ODeepLearningEstimator(H2OEstimator):
     distribution : str
        A character string. The distribution function of the response.
        Must be "AUTO", "bernoulli", "multinomial", "poisson", "gamma",
-       "tweedie", "laplace", "huber" or "gaussian"
+       "tweedie", "laplace", "huber", "quantile" or "gaussian"
+    quantile_alpha : float
+      Quantile (only for Quantile regression, must be between 0 and 1)
     tweedie_power : float
       Tweedie power (only for Tweedie distribution, must be between 1 and 2)
     score_interval : int
@@ -433,6 +435,14 @@ class H2ODeepLearningEstimator(H2OEstimator):
     self._parms["distribution"] = value
 
   @property
+  def quantile_alpha(self):
+      return self._parms["quantile_alpha"]
+
+  @quantile_alpha.setter
+  def quantile_alpha(self, value):
+      self._parms["quantile_alpha"] = value
+
+  @property
   def tweedie_power(self):
     return self._parms["tweedie_power"]
 
diff --git a/h2o-py/h2o/estimators/gbm.py b/h2o-py/h2o/estimators/gbm.py
index bbaee62..2054255 100644
--- a/h2o-py/h2o/estimators/gbm.py
+++ b/h2o-py/h2o/estimators/gbm.py
@@ -15,7 +15,9 @@ class H2OGradientBoostingEstimator(H2OEstimator):
     automatically be generated.
   distribution : str
      The distribution function of the response. Must be "AUTO", "bernoulli",
-     "multinomial", "poisson", "gamma", "tweedie", "laplace" or "gaussian"
+     "multinomial", "poisson", "gamma", "tweedie", "laplace", "quantile" or "gaussian"
+  quantile_alpha : float
+    Quantile (only for Quantile regression, must be between 0 and 1)
   tweedie_power : float
     Tweedie power (only for Tweedie distribution, must be between 1 and 2)
   ntrees : int
@@ -63,6 +65,8 @@ class H2OGradientBoostingEstimator(H2OEstimator):
     Whether to keep the predictions of the cross-validation models
   score_each_iteration : bool
     Attempts to score each tree.
+  score_tree_interval : int
+    Score the model after every so many trees. Disabled if set to 0.
   stopping_rounds : int
     Early stopping based on convergence of stopping_metric.
     Stop if simple moving average of length k of the stopping_metric does not improve
@@ -78,14 +82,14 @@ class H2OGradientBoostingEstimator(H2OEstimator):
   -------
     A new H2OGradientBoostedEstimator object.
   """
-  def __init__(self, model_id=None, distribution=None, tweedie_power=None, ntrees=None,
+  def __init__(self, model_id=None, distribution=None, quantile_alpha=None, tweedie_power=None, ntrees=None,
                max_depth=None, min_rows=None, learn_rate=None, nbins=None,
                sample_rate=None, col_sample_rate=None, col_sample_rate_per_tree=None,
                nbins_top_level=None, nbins_cats=None, balance_classes=None,
                max_after_balance_size=None, seed=None, build_tree_one_node=None,
                nfolds=None, fold_assignment=None, keep_cross_validation_predictions=None,
                stopping_rounds=None, stopping_metric=None, stopping_tolerance=None,
-               score_each_iteration=None, checkpoint=None):
+               score_each_iteration=None, score_tree_interval=None, checkpoint=None):
     super(H2OGradientBoostingEstimator, self).__init__()
     self._parms = locals()
     self._parms = {k:v for k,v in self._parms.items() if k!="self"}
@@ -99,6 +103,14 @@ class H2OGradientBoostingEstimator(H2OEstimator):
     self._parms["distribution"] = value
 
   @property
+  def quantile_alpha(self):
+      return self._parms["quantile_alpha"]
+
+  @quantile_alpha.setter
+  def quantile_alpha(self, value):
+      self._parms["quantile_alpha"] = value
+
+  @property
   def tweedie_power(self):
     return self._parms["tweedie_power"]
 
@@ -251,6 +263,14 @@ class H2OGradientBoostingEstimator(H2OEstimator):
     self._parms["score_each_iteration"] = value
 
   @property
+  def score_tree_interval(self):
+      return self._parms["score_tree_interval"]
+
+  @score_tree_interval.setter
+  def score_tree_interval(self, value):
+      self._parms["score_tree_interval"] = value
+
+  @property
   def stopping_rounds(self):
     return self._parms["stopping_rounds"]
 
diff --git a/h2o-py/h2o/estimators/random_forest.py b/h2o-py/h2o/estimators/random_forest.py
index d18ed7d..742440c 100644
--- a/h2o-py/h2o/estimators/random_forest.py
+++ b/h2o-py/h2o/estimators/random_forest.py
@@ -7,7 +7,8 @@ class H2ORandomForestEstimator(H2OEstimator):
                nbins_cats=None, binomial_double_trees=None, balance_classes=None,
                max_after_balance_size=None, seed=None, nfolds=None, fold_assignment=None,
                stopping_rounds=None, stopping_metric=None, stopping_tolerance=None,
-               score_each_iteration=None, keep_cross_validation_predictions=None, checkpoint=None):
+               score_each_iteration=None, score_tree_interval=None,
+               keep_cross_validation_predictions=None, checkpoint=None):
     """Builds a Random Forest Model on an H2OFrame
 
     Parameters
@@ -64,6 +65,8 @@ class H2ORandomForestEstimator(H2OEstimator):
       Whether to keep the predictions of the cross-validation models
     score_each_iteration : bool
       Attempts to score each tree.
+    score_tree_interval : int
+      Score the model after every so many trees. Disabled if set to 0.
     stopping_rounds : int
       Early stopping based on convergence of stopping_metric.
       Stop if simple moving average of length k of the stopping_metric does not improve
@@ -211,11 +214,19 @@ class H2ORandomForestEstimator(H2OEstimator):
 
   @property
   def score_each_iteration(self):
-    return self._parms["score_each_iteration"]
+      return self._parms["score_each_iteration"]
 
   @score_each_iteration.setter
   def score_each_iteration(self, value):
-    self._parms["score_each_iteration"] = value
+      self._parms["score_each_iteration"] = value
+
+  @property
+  def score_tree_interval(self):
+    return self._parms["score_tree_interval"]
+
+  @score_tree_interval.setter
+  def score_tree_interval(self, value):
+    self._parms["score_tree_interval"] = value
 
   @property
   def stopping_rounds(self):
diff --git a/h2o-py/h2o/frame.py b/h2o-py/h2o/frame.py
index 92cf475..7b28a5e 100644
--- a/h2o-py/h2o/frame.py
+++ b/h2o-py/h2o/frame.py
@@ -1479,27 +1479,36 @@ class H2OFrame(object):
     """
     return ExprNode("median", self, na_rm)._eager_scalar()
 
-  def var(self,y=None,use="everything"):
-    """Compute the variance, or co-variance matrix.
+  def var(self,y=None,na_rm=False, use=None):
+    """Compute the variance or covariance matrix of one or two H2OFrames.
 
     Parameters
     ----------
     y : H2OFrame, default=None
-      If y is None, then the variance is computed for self. If self has more than one
-      column, then the covariance matrix is returned.
-      If y is not None, then the covariance between self and y is computed (self and y
-      must therefore both be single columns).
-    use : str
-      One of "everything", "complete.obs", or "all.obs"
+      If y is None and self is a single column, then the variance is computed for self. If self has 
+      multiple columns, then its covariance matrix is returned. Single rows are treated as single columns. 
+      If y is not None, then a covariance matrix between the columns of self and the columns of y is computed. 
+    na_rm : bool, default=False
+      Remove NAs from the computation.
+    use : str, default=None, which acts as "everything" if na_rm is False, and "complete.obs" if na_rm is True
+      A string indicating how to handle missing values. This must be one of the following: 
+        "everything"            - outputs NaNs whenever one of its contributing observations is missing
+        "all.obs"               - presence of missing observations will throw an error
+        "complete.obs"          - discards missing values along with all observations in their rows so that only complete observations are used
+        "pairwise.complete.obs" - uses all complete pairs of observations
 
     Returns
     -------
-      The covariance matrix of the columns in this H2OFrame if y is given, or a eagerly
-      computed scalar if y is not given.
+      An H2OFrame of the covariance matrix of the columns of this H2OFrame with itself (if y is not given), or with the columns of y 
+      (if y is given). If self and y are single rows or single columns, the variance or covariance is given as a scalar.
     """
-    if y is None: y = self
-    if self.nrow==1 or (self.ncol==1 and y.ncol==1): return ExprNode("var",self,y,use)._eager_scalar()
-    return H2OFrame._expr(expr=ExprNode("var",self,y,use))._frame()
+    symmetric = False
+    if y is None: 
+      y = self
+      if self.ncol > 1 and self.nrow > 1: symmetric = True
+    if use is None: use = "complete.obs" if na_rm else "everything"
+    if self.nrow==1 or (self.ncol==1 and y.ncol==1): return ExprNode("var",self,y,use,symmetric)._eager_scalar()
+    return H2OFrame._expr(expr=ExprNode("var",self,y,use,symmetric))._frame()
 
   def sd(self, na_rm=False):
     """Compute the standard deviation.
@@ -1840,12 +1849,15 @@ class H2OFrame(object):
     return H2OFrame._expr(expr=ExprNode("signif", self, digits), cache=self._ex._cache)
 
   def round(self, digits=0):
-    """Round doubles/floats to the given number of digits.
+    """Round doubles/floats to the given number of decimal places.
 
     Parameters
     ----------
-    digits : int
-
+    digits : int, default=0
+      Number of decimal places to round doubles/floats. Rounding to a negative number of decimal places is not 
+      supported. For rounding off a 5, the IEC 60559 standard is used, go to the even digit. Therefore rounding 2.5 
+      gives 2 and rounding 3.5 gives 4.
+      
     Returns
     -------
       H2OFrame
@@ -1886,7 +1898,7 @@ class H2OFrame(object):
     fr = H2OFrame._expr(expr=ExprNode("na.omit", self), cache=self._ex._cache)
     fr._ex._cache.nrows=-1
     return fr
-
+ 
   def isna(self):
     """For each element in an H2OFrame, determine if it is NA or not.
 
diff --git a/h2o-py/h2o/grid/grid_search.py b/h2o-py/h2o/grid/grid_search.py
index 2ee6112..9480209 100644
--- a/h2o-py/h2o/grid/grid_search.py
+++ b/h2o-py/h2o/grid/grid_search.py
@@ -36,6 +36,8 @@ class H2OGridSearch(object):
     grid_id : str, optional
       The unique id assigned to the resulting grid object. If none is given, an id will
       automatically be generated.
+    search_criteria: dict, optional
+      A dictionary of directives which direct the search of the hyperparameter space.
      
     Returns
     -------
@@ -54,7 +56,7 @@ class H2OGridSearch(object):
     self._id = grid_id
     self.model = model() if model.__class__.__name__ == 'type' else model  # H2O Estimator child class
     self.hyper_params = dict(hyper_params)
-    self.search_criteria = search_criteria
+    self.search_criteria = None if None == search_criteria else dict(search_criteria)
     self._grid_json = None
     self.models = None # list of H2O Estimator instances
     self._parms = {} # internal, for object recycle #
@@ -148,7 +150,7 @@ class H2OGridSearch(object):
 
     parms = self._parms.copy()
     parms.update({k:v for k, v in algo_params.items() if k not in ["self","params", "algo_params", "parms"] })
-    if self.search_criteria: parms.update({k:v for k, v in self.search_criteria.items() })
+    parms["search_criteria"] = self.search_criteria
     parms["hyper_parameters"] = self.hyper_params  # unique to grid search
     parms.update({k:v for k,v in list(self.model._parms.items()) if v is not None})  # unique to grid search
     if '__class__' in parms:  # FIXME: hackt for PY3
diff --git a/h2o-py/h2o/h2o.py b/h2o-py/h2o/h2o.py
index b971f20..04afafe 100644
--- a/h2o-py/h2o/h2o.py
+++ b/h2o-py/h2o/h2o.py
@@ -668,7 +668,7 @@ def cluster_status():
 def init(ip="localhost", port=54321, start_h2o=True, enable_assertions=True,
          license=None, nthreads=-1, max_mem_size=None, min_mem_size=None, ice_root=None, 
          strict_version_check=True, proxy=None, https=False, insecure=False, username=None, 
-         password=None, max_mem_size_GB=None, min_mem_size_GB=None):
+         password=None, force_connect=False, max_mem_size_GB=None, min_mem_size_GB=None):
   """Initiate an H2O connection to the specified ip and port.
 
   Parameters
@@ -707,6 +707,8 @@ def init(ip="localhost", port=54321, start_h2o=True, enable_assertions=True,
     Username to login with.
   password : str
     Password to login with.
+  force_connect : bool
+    When set to True, a connection to the cluster will attempt to be established regardless of the its health.
   max_mem_size_GB: DEPRECATED
     Use max_mem_size instead.
   min_mem_size_GB: DEPRECATED
@@ -727,7 +729,7 @@ def init(ip="localhost", port=54321, start_h2o=True, enable_assertions=True,
   H2OConnection(ip=ip, port=port,start_h2o=start_h2o,enable_assertions=enable_assertions,license=license,
                 nthreads=nthreads,max_mem_size=max_mem_size,min_mem_size=min_mem_size,ice_root=ice_root,
                 strict_version_check=strict_version_check,proxy=proxy,https=https,insecure=insecure,username=username,
-                password=password,max_mem_size_GB=max_mem_size_GB,min_mem_size_GB=min_mem_size_GB)
+                password=password,force_connect=force_connect,max_mem_size_GB=max_mem_size_GB,min_mem_size_GB=min_mem_size_GB)
   return None
 
 
diff --git a/h2o-py/scripts/h2o-py-test-setup.py b/h2o-py/scripts/h2o-py-test-setup.py
index 88d6ff0..b9b3153 100644
--- a/h2o-py/scripts/h2o-py-test-setup.py
+++ b/h2o-py/scripts/h2o-py-test-setup.py
@@ -11,6 +11,7 @@ _IS_PYUNIT_                   = False
 _IS_PYBOOKLET_                = False
 _RESULTS_DIR_                 = False
 _TEST_NAME_                   = None
+_FORCE_CONNECT_               = False
 
 def parse_args(args):
     global _H2O_IP_
@@ -23,6 +24,7 @@ def parse_args(args):
     global _IS_PYBOOKLET_
     global _RESULTS_DIR_
     global _TEST_NAME_
+    global _FORCE_CONNECT_
 
     i = 1
     while (i < len(args)):
@@ -55,6 +57,8 @@ def parse_args(args):
             i = i + 1
             if (i > len(args)): usage()
             _TEST_NAME_ = args[i]
+        elif (s == "--forceConnect"):
+            _FORCE_CONNECT_ = True
         else:
             unknownArg(s)
         i = i + 1
@@ -84,6 +88,8 @@ def usage():
     print("")
     print("    --testName        name of the pydemo, pyunit, or pybooklet.")
     print("")
+    print("    --forceConnect    h2o will attempt to connect to cluster regardless of cluster's health.")
+    print("")
     sys.exit(1) #exit with nonzero exit code
 
 def unknownArg(arg):
@@ -120,7 +126,7 @@ def h2o_test_setup(sys_args):
                                 "{0}".format(_TEST_NAME_))
 
     print("[{0}] {1}\n".format(strftime("%Y-%m-%d %H:%M:%S", gmtime()), "Connect to h2o on IP: {0} PORT: {1}".format(_H2O_IP_, _H2O_PORT_)))
-    h2o.init(ip=_H2O_IP_, port=_H2O_PORT_, strict_version_check=False)
+    h2o.init(ip=_H2O_IP_, port=_H2O_PORT_, strict_version_check=False, force_connect=_FORCE_CONNECT_)
 
     #rest_log = os.path.join(_RESULTS_DIR_, "rest.log")
     #h2o.start_logging(rest_log)
@@ -140,4 +146,4 @@ def h2o_test_setup(sys_args):
     elif _IS_PYDEMO_:    pydemo_utils.pydemo_exec(_TEST_NAME_)
 
 if __name__ == "__main__":
-    h2o_test_setup(sys.argv)
\ No newline at end of file
+    h2o_test_setup(sys.argv)
diff --git a/h2o-py/tests/testdir_algos/glm/pyunit_benign_glm_grid.py b/h2o-py/tests/testdir_algos/glm/pyunit_benign_glm_grid.py
index 18c74b8..3843faa 100644
--- a/h2o-py/tests/testdir_algos/glm/pyunit_benign_glm_grid.py
+++ b/h2o-py/tests/testdir_algos/glm/pyunit_benign_glm_grid.py
@@ -37,7 +37,7 @@ def benign_grid():
   assert best_model.params['family']['actual'] == 'binomial'
 
   # test search_criteria plumbing
-  search_criteria = { 'strategy': "Random", 'max_models': 3 }
+  search_criteria = { 'strategy': "RandomDiscrete", 'max_models': 3 }
   max_models_g = H2OGridSearch(H2OGeneralizedLinearEstimator(family='binomial'), hyper_parameters, search_criteria=search_criteria)
   max_models_g.train(x=X,y=Y, training_frame=training_data)
 
@@ -45,8 +45,6 @@ def benign_grid():
   print(max_models_g.grid_id)
   print(max_models_g.sort_by('F1', False))
 
-  ##### TODO: remove:
-  print("before assert")
   assert len(max_models_g.models) == 3, "expected 3 models, got: {}".format(len(max_models_g.models))
 
 if __name__ == "__main__":
diff --git a/h2o-r/H2O_Load.R b/h2o-r/H2O_Load.R
index e07b39c..39442fd 100755
--- a/h2o-r/H2O_Load.R
+++ b/h2o-r/H2O_Load.R
@@ -2,7 +2,7 @@
 CLIFF.ROOT.PATH <- "C:/Users/cliffc/Desktop/"
 SPENCER.ROOT.PATH <- "/Users/spencer/0xdata/"
 LUDI.ROOT.PATH <- "/Users/ludirehak/"
-ROOT.PATH <- SPENCER.ROOT.PATH
+ROOT.PATH <- LUDI.ROOT.PATH
 DEV.PATH  <- "h2o-3/h2o-r/h2o-package/R/"
 FULL.PATH <- paste(ROOT.PATH, DEV.PATH, sep="")
 
diff --git a/h2o-r/build.gradle b/h2o-r/build.gradle
index 5489e7a..82d8afd 100644
--- a/h2o-r/build.gradle
+++ b/h2o-r/build.gradle
@@ -183,7 +183,11 @@ def testsPath = new File("./tests")
 task smokeTest(type: Exec) {
     println "R smoke test (run.py --wipeall --testsize s)..."
     workingDir testsPath
-    commandLine 'python', 'run.py', '--wipeall', '--testsize', 's'
+	def args = ['python', 'run.py', '--wipeall', '--testsize', 's']
+    if (project.hasProperty("jacocoCoverage")) {
+        args << '--jacoco'
+    }
+    commandLine args
 }
 
 task cleanUpSmokeTest << {
diff --git a/h2o-r/ensemble/h2oEnsemble-package/DESCRIPTION b/h2o-r/ensemble/h2oEnsemble-package/DESCRIPTION
index 680603b..50d4566 100644
--- a/h2o-r/ensemble/h2oEnsemble-package/DESCRIPTION
+++ b/h2o-r/ensemble/h2oEnsemble-package/DESCRIPTION
@@ -1,13 +1,13 @@
 Package: h2oEnsemble
 Type: Package
 Title: H2O Ensemble Learning
-Version: 0.1.5
-Date: 2015-12-16
+Version: 0.1.5.9000
+Date: 2016-02-08
 Author: Erin LeDell
 Maintainer: Erin LeDell <erin@h2o.ai>
 Description: H2O Ensemble implements the Super Learner ensemble (stacking) algorithm using the H2O R interface to provide base learning algorithms.   
 License: Apache License (== 2.0)
-Depends: R (>= 2.13.0), h2o (>= 3.2.0.9)
+Depends: R (>= 2.13.0), h2o (>= 3.2.0.8)
 NeedsCompilation: no
 SystemRequirements: Java (>= 1.6)
 Suggests: SuperLearner, cvAUC, testthat
diff --git a/h2o-r/ensemble/h2oEnsemble-package/NEWS b/h2o-r/ensemble/h2oEnsemble-package/NEWS
index 29dc183..270eb70 100644
--- a/h2o-r/ensemble/h2oEnsemble-package/NEWS
+++ b/h2o-r/ensemble/h2oEnsemble-package/NEWS
@@ -1,14 +1,16 @@
 # News for the h2oEnsemble package. #
 
-h2oEnsemble 0.1.6 (In progress)
+h2oEnsemble 0.1.5.9000 (devel)
 -----------------
-* TO DO: Add support for multi-class classification.
-* TO DO: Add utility function, `h2o.ensemble_group` which takes as input a list of "H2OModel" objects and returns an object of class, "h2o.ensemble".  
+* Updated default value for `model_id` in wrapper functions from `""` to `NULL`.  Was causing an NPE on the master branch of h2o-3. Issue documented here: https://0xdata.atlassian.net/browse/PUBDEV-2623
+* Updated default `prior` value in `h2o.glm.wrapper` from 0 (no longer supported in `h2o.glm`) to `NULL`.  Issue documented here: https://0xdata.atlassian.net/browse/PUBDEV-2624
+* TO DO: Add utility function, `h2o.stack` which takes as input a list of "H2OModel" objects and returns an object of class, "h2o.ensemble".  
 * TO DO: Remove a learner from the level-one matrix, Z, if the algorithm fails to run.
 * TO DO: Add a check that CV folds produce training sets that have both pos/neg examples instead of a constant response col (in the case of imbalanced data)
 * TO DO: Update the `runtime` list to include times for `cv` and `baselearning`.  These were dropped in version 0.0.5.
 * TO DO: Add ability to generate predicted values using the `validation_frame` argument; currently this arg is ignored and doesn't do anything, and predictions must be made with the `predict.h2o.ensemble` function.  This should also include setting a new output object to store metrics.
 * TO DO: Add a utility function, `h2o.ensemble_metrics` to extract default performance metrics for binomial and gaussian family (AUC and MSE).  Also need functions to compute AUC and MSE from predicted y and true y (need use Java AUC and MSE functions).
+* TO DO: Add support for multi-class classification.
 
 
 h2oEnsemble 0.1.5 (2015-12-16)
diff --git a/h2o-r/ensemble/h2oEnsemble-package/R/wrappers.R b/h2o-r/ensemble/h2oEnsemble-package/R/wrappers.R
index 0fed895..c89284e 100644
--- a/h2o-r/ensemble/h2oEnsemble-package/R/wrappers.R
+++ b/h2o-r/ensemble/h2oEnsemble-package/R/wrappers.R
@@ -1,7 +1,9 @@
 # Set of default wrappers to create a uniform interface for h2o supervised ML functions (H2O 3.0 and above)
+# These wrapper functions should always be compatible with the master branch of: https://github.com/h2oai/h2o-3
+# See the ensemble README for a full wrapper compatibility chart
 
 # Example of a wrapper function:
-h2o.example.wrapper <- function(x, y, training_frame, model_id = "", family = c("gaussian", "binomial"), ...) {
+h2o.example.wrapper <- function(x, y, training_frame, model_id = NULL, family = c("gaussian", "binomial"), ...) {
   # This function is just an example.  
   # You can wrap any H2O learner inside a wrapper function, example: h2o.glm
   h2o.glm(x = x, y = y, training_frame = training_frame, family = family)
@@ -15,16 +17,20 @@ h2o.example.wrapper <- function(x, y, training_frame, model_id = "", family = c(
 # This is a version of the h2o.glm.wrapper which doesn't pass along all the args
 # Use this version until this is resolved: https://0xdata.atlassian.net/browse/PUBDEV-1558
 # beta_constraints currently causing a bug: https://0xdata.atlassian.net/browse/PUBDEV-1556
-h2o.glm.wrapper <- function(x, y, training_frame, model_id = "", validation_frame = NULL, max_iterations = 50,
+h2o.glm.wrapper <- function(x, y, training_frame, model_id = NULL, validation_frame = NULL, max_iterations = 50,
                             beta_epsilon = 0, solver = c("IRLSM", "L_BFGS"), standardize = TRUE,
                             family = c("gaussian", "binomial", "poisson", "gamma", "tweedie"),
                             link = c("family_default", "identity", "logit", "log", "inverse", "tweedie"), 
                             tweedie_variance_power = NaN, tweedie_link_power = NaN,
-                            alpha = 0.5, prior = 0, lambda = 1e-05, lambda_search = FALSE,
+                            alpha = 0.5, prior = NULL, lambda = 1e-05, lambda_search = FALSE,
                             nlambdas = -1, lambda_min_ratio = -1, nfolds = 0, fold_column = NULL,
                             fold_assignment = c("AUTO", "Random", "Modulo"),
                             keep_cross_validation_predictions = TRUE, beta_constraints = NULL,
-                            offset_column = NULL, weights_column = NULL, intercept = TRUE, ...) {
+                            offset_column = NULL, weights_column = NULL, intercept = TRUE, 
+                            max_active_predictors = -1, objective_epsilon = -1,
+                            gradient_epsilon = -1, non_negative = FALSE, compute_p_values = FALSE,
+                            remove_collinear_columns = FALSE, max_runtime_secs = 0,
+                            missing_values_handling = c("Skip", "MeanImputation"), ...) {
   
   # Also, offset_column, weights_column, intercept not implemented at the moment
   h2o.glm(x = x, y = y, training_frame = training_frame, model_id = model_id, 
@@ -41,7 +47,7 @@ h2o.glm.wrapper <- function(x, y, training_frame, model_id = "", validation_fram
 
 
 
-h2o.gbm.wrapper <- function(x, y, training_frame, model_id = "", #checkpoint
+h2o.gbm.wrapper <- function(x, y, training_frame, model_id = NULL, #checkpoint
                             family = c("AUTO", "gaussian", "bernoulli", "binomial", "multinomial", "poisson", "gamma", "tweedie"),
                             tweedie_power = 1.5, ntrees = 50, max_depth = 5, min_rows = 10,
                             learn_rate = 0.1, sample_rate = 1, col_sample_rate = 1,
@@ -75,7 +81,7 @@ h2o.gbm.wrapper <- function(x, y, training_frame, model_id = "", #checkpoint
 }
 
 
-h2o.randomForest.wrapper <- function(x, y, training_frame, model_id = "",
+h2o.randomForest.wrapper <- function(x, y, training_frame, model_id = NULL,
                                      family = c("binomial", "multinomial", "gaussian"), 
                                      validation_frame = NULL, #checkpoint 
                                      mtries = -1, sample_rate = 0.632, build_tree_one_node = FALSE,
@@ -102,7 +108,7 @@ h2o.randomForest.wrapper <- function(x, y, training_frame, model_id = "",
 }
 
 
-h2o.deeplearning.wrapper <- function(x, y, training_frame, model_id = "",
+h2o.deeplearning.wrapper <- function(x, y, training_frame, model_id = NULL,
                                      family = c("binomial", "multinomial", "gaussian"), 
                                      overwrite_with_best_model, validation_frame = NULL, checkpoint,
                                      autoencoder = FALSE, use_all_factor_levels = TRUE,
diff --git a/h2o-r/ensemble/h2oEnsemble-package/man/h2o.ensemble.Rd b/h2o-r/ensemble/h2oEnsemble-package/man/h2o.ensemble.Rd
index e543bcf..3364c27 100644
--- a/h2o-r/ensemble/h2oEnsemble-package/man/h2o.ensemble.Rd
+++ b/h2o-r/ensemble/h2oEnsemble-package/man/h2o.ensemble.Rd
@@ -101,6 +101,9 @@ The version of the h2oEnsemble R package.
 }
 }
 \references{
+\href{https://github.com/h2oai/h2o-3/tree/master/h2o-r/ensemble}{H2O Ensemble Homepage}	\cr
+\href{http://learn.h2o.ai/content/tutorials/ensembles-stacking/index.html}{H2O Ensemble Tutorial}	\cr
+\cr	
 LeDell, E. (2015) Scalable Ensemble Learning and Computationally Efficient Variance Estimation (Doctoral Dissertation).  University of California, Berkeley, USA.\cr
 \url{http://www.stat.berkeley.edu/~ledell/papers/ledell-phd-thesis.pdf}\cr
 \cr
diff --git a/h2o-r/ensemble/h2oEnsemble-package/man/h2oEnsemble-package.Rd b/h2o-r/ensemble/h2oEnsemble-package/man/h2oEnsemble-package.Rd
index f2669f4..d423017 100644
--- a/h2o-r/ensemble/h2oEnsemble-package/man/h2oEnsemble-package.Rd
+++ b/h2o-r/ensemble/h2oEnsemble-package/man/h2oEnsemble-package.Rd
@@ -12,8 +12,8 @@ H2O Ensemble implements the Super Learner ensemble (stacking) algorithm using th
 \tabular{ll}{
 Package: \tab h2oEnsemble\cr
 Type: \tab Package\cr
-Version: \tab 0.1.5\cr
-Date: \tab 2015-12-16\cr
+Version: \tab 0.1.5.9000\cr
+Date: \tab 2016-02-08\cr
 License: \tab Apache License (== 2.0)\cr
 }
 }
diff --git a/h2o-r/h2o-package/R/classes.R b/h2o-r/h2o-package/R/classes.R
index 140d5f6..66630d6 100755
--- a/h2o-r/h2o-package/R/classes.R
+++ b/h2o-r/h2o-package/R/classes.R
@@ -207,7 +207,7 @@ setMethod("summary", "H2OModel", function(object, ...) {
   } else {
     if( !is.null(tm$cm)                                              )  { if ( arg != "xval" ) { cat(paste0("\nConfusion Matrix: Extract with `h2o.confusionMatrix(<model>, <data>)`)\n")); } }
   }
-  if( !is.null(tm$cm)                                              )  { if ( arg != "xval" ) { cat("=========================================================================\n"); print(data.frame(tm$cm$table)) } }
+  if( !is.null(tm$cm)                                              )  { if ( arg != "xval" ) { cat("=========================================================================\n"); print(tm$cm$table) } }
   if (arg != "test") {
     if( !is.null(tm$hit_ratio_table)                                 )  cat(paste0("\nHit Ratio Table: Extract with `h2o.hit_ratio_table(<model>,", arg, " = TRUE)`\n"))
   } else {
diff --git a/h2o-r/h2o-package/R/communication.R b/h2o-r/h2o-package/R/communication.R
index ae6af4b..15d8455 100755
--- a/h2o-r/h2o-package/R/communication.R
+++ b/h2o-r/h2o-package/R/communication.R
@@ -430,8 +430,7 @@
                  string = {},
                  {})
         }
-        if( x$name == "Confusion Matrix") attr(tbl, "header") <- paste0(x$name, " - (", x$description, ")")
-        else                              attr(tbl, "header")  <- x$name
+        attr(tbl, "header")  <- x$name
         attr(tbl, "formats") <- fmts
         attr(tbl, "description")   <- descr
         oldClass(tbl) <- c("H2OTable", "data.frame")
diff --git a/h2o-r/h2o-package/R/connection.R b/h2o-r/h2o-package/R/connection.R
index 1067668..1ed52f7 100755
--- a/h2o-r/h2o-package/R/connection.R
+++ b/h2o-r/h2o-package/R/connection.R
@@ -95,7 +95,6 @@ h2o.init <- function(ip = "localhost", port = 54321, startH2O = TRUE, forceDL =
     stop("`password` must be a character string or NA_character_")
   if (is.na(username) != is.na(password))
     stop("Must provide both `username` and `password`")
-
   if ((R.Version()$major == "3") && (R.Version()$minor == "1.0")) {
     stop("H2O is not compatible with R 3.1.0\n",
          "Please change to a newer or older version of R.\n",
@@ -421,7 +420,16 @@ h2o.clusterStatus <- function() {
   if (.Platform$OS.type == "windows") {
     command <- normalizePath(gsub("\"","",command))
   }
-  jver <- system2(command, "-version", stdout = TRUE, stderr = TRUE)
+  
+  jver <- tryCatch({system2(command, "-version", stdout = TRUE, stderr = TRUE)}, 
+      error = function(err) {
+        print(err)
+        stop("You have a 32-bit version of Java. H2O works best with 64-bit Java.\n",
+        "Please download the latest Java SE JDK 7 from the following URL:\n",
+        "http://www.oracle.com/technetwork/java/javase/downloads/jdk7-downloads-1880260.html")
+      }
+    )
+    
   if(any(grepl("GNU libgcj", jver))) {
     stop("Sorry, GNU Java is not supported for H2O.\n",
          "Please download the latest Java SE JDK 7 from the following URL:\n",
diff --git a/h2o-r/h2o-package/R/deeplearning.R b/h2o-r/h2o-package/R/deeplearning.R
index 6b09d0e..f19a982 100755
--- a/h2o-r/h2o-package/R/deeplearning.R
+++ b/h2o-r/h2o-package/R/deeplearning.R
@@ -55,7 +55,8 @@
 #'        (experimental) or "Huber" (experimental)
 #' @param distribution A \code{character} string. The distribution function of the response.
 #'        Must be "AUTO", "bernoulli", "multinomial", "poisson", "gamma", "tweedie",
-#'        "laplace", "huber" or "gaussian"
+#'        "laplace", "huber", "quantile" or "gaussian"
+#' @param quantile_alpha Quantile (only for Quantile regression, must be between 0 and 1)
 #' @param tweedie_power Tweedie power (only for Tweedie distribution, must be between 1 and 2).
 #' @param score_interval Shortest time interval (in secs) between model scoring.
 #' @param score_training_samples Number of training set samples for scoring (0 for all).
@@ -162,7 +163,8 @@ h2o.deeplearning <- function(x, y, training_frame,
                              initial_weight_distribution = c("UniformAdaptive", "Uniform", "Normal"),
                              initial_weight_scale = 1,
                              loss = c("Automatic", "CrossEntropy", "Quadratic", "Absolute", "Huber"),
-                             distribution = c("AUTO","gaussian", "bernoulli", "multinomial", "poisson", "gamma", "tweedie", "laplace", "huber"),
+                             distribution = c("AUTO","gaussian", "bernoulli", "multinomial", "poisson", "gamma", "tweedie", "laplace", "huber", "quantile"),
+                             quantile_alpha = 0.5,
                              tweedie_power = 1.5,
                              score_interval = 5,
                              score_training_samples,
@@ -297,6 +299,8 @@ h2o.deeplearning <- function(x, y, training_frame,
   }
   if (!missing(distribution))
     parms$distribution <- distribution
+  if (!missing(quantile_alpha))
+    parms$quantile_alpha <- quantile_alpha
   if (!missing(tweedie_power))
     parms$tweedie_power <- tweedie_power
   if(!missing(score_interval))
diff --git a/h2o-r/h2o-package/R/frame.R b/h2o-r/h2o-package/R/frame.R
index a47bd4d..c2838f2 100644
--- a/h2o-r/h2o-package/R/frame.R
+++ b/h2o-r/h2o-package/R/frame.R
@@ -199,7 +199,7 @@ pfr <- function(x) { chk.H2OFrame(x); .pfr(x) }
       if( y=="TRUE" )       y <- TRUE
       else if( y=="FALSE" ) y <- FALSE
     }
-    .set(x,"data",y)
+    .set(x,"data",as.numeric(y))
   } else if( !is.null(res$funstr) ) {
     stop("Unimplemented: handling of function returns")
   } else if( !is.null(res$string) ) {
@@ -264,10 +264,10 @@ pfr <- function(x) { chk.H2OFrame(x); .pfr(x) }
 
 #` Flush any cached data
 .flush.data <- function(x) {
-  if( !is.null(attr(x,"data")) ) rm("data" ,envir=x)
-  if( !is.null(attr(x,"data")) ) rm("types",envir=x)
-  if( !is.null(attr(x,"data")) ) rm("nrow" ,envir=x)
-  if( !is.null(attr(x,"data")) ) rm("ncol" ,envir=x)
+  if( !is.null(attr(x,"data")) ) attr(x, "data")  <- NULL
+  if( !is.null(attr(x,"nrow")) ) attr(x, "nrow")  <- NULL
+  if( !is.null(attr(x,"ncol")) ) attr(x, "ncol")  <- NULL
+  if( !is.null(attr(x,"types"))) attr(x, "types") <- NULL
   x
 }
 
@@ -287,6 +287,7 @@ h2o.assign <- function(data, key) {
   x = .eval.driver(.newExpr("assign", key, id)) # Eager eval, so can see it in cluster
   .set(x,"id",key)
   .set(x,"eval",NULL)
+  gc()
   x
 }
 
@@ -1184,14 +1185,43 @@ t.H2OFrame <- function(x) .newExpr("t",x)
 #' @rdname H2OFrame
 #' @export
 log <- function(x, ...) {
-  if( !is.H2OFrame(x) ) .Primitive("log")(x)
-  else .newExpr("log",x)
+  if( !is.H2OFrame(x) ) .Primitive("log")(x,...)
+  else {
+    dots <- list(...)
+    base <- if (length(dots) > 0) dots[[1]] else exp(1) 
+    if (base == exp(1)) .newExpr("log",x)
+    else if (base == 10) .newExpr("log10",x)
+    else if (base == 2) .newExpr("log2",x)
+    else .newExpr("log",x) / .newExpr("log",base)
+  }
+}
+
+#' @rdname H2OFrame
+#' @export
+log10 <- function(x) {
+  if( !is.H2OFrame(x) ) .Primitive("log10")(x)
+  else .newExpr("log10",x)
 }
 
 #' @rdname H2OFrame
 #' @export
+log2 <- function(x) {
+  if( !is.H2OFrame(x) ) .Primitive("log2")(x)
+  else .newExpr("log2",x)
+}
+
+#' @rdname H2OFrame
+#' @export
+log1p <- function(x) {
+  if( !is.H2OFrame(x) ) .Primitive("log1p")(x)
+  else .newExpr("log1p",x)
+}
+
+
+#' @rdname H2OFrame
+#' @export
 trunc <- function(x, ...) {
-  if( !is.H2OFrame(x) ) .Primitive("trunc")(x)
+  if( !is.H2OFrame(x) ) .Primitive("trunc")(x, ...)
   else .newExpr("trunc",x)
 }
 
@@ -1217,7 +1247,7 @@ trunc <- function(x, ...) {
 #' @export
 h2o.which <- function(x) {
   if( !is.H2OFrame(x) ) stop("must be an H2OFrame")
-  else .newExpr("which",x)
+  else .newExpr("which",x) + 1
 }
 
 #' Count of NAs per column
@@ -1293,7 +1323,7 @@ h2o.length <- length.H2OFrame
 #' Return the levels from the column requested column.
 #'
 #' @param x An H2OFrame object.
-#' @param i The index of the column whose domain is to be returned.
+#' @param i Optional, the index of the column whose domain is to be returned.
 #' @seealso \code{\link[base]{levels}} for the base R method.
 #' @examples
 #' \donttest{
@@ -1303,10 +1333,29 @@ h2o.length <- length.H2OFrame
 #' @export
 h2o.levels <- function(x, i) {
   df <- .fetch.data(x,1L)
-  if( missing(i) ) levels(df[[1]])
-  else levels(df[[i]])
+  res <- list()
+  if( missing(i) ) {
+    for (col in 1:ncol(df)) {
+      res <- c(res, list(levels(df[[col]])))
+    }
+    if (length(res) == 1) res <- res[[1]]
+  }
+  else res <- levels(df[[i]])
+  res
 }
 
+
+#'
+#' Get the number of factor levels for this frame.
+#'
+#' @param x An H2OFrame object.
+#' @seealso \code{\link[base]{nlevels}} for the base R method.
+#' @export
+h2o.nlevels <- function(x) {
+  levels <- h2o.levels(x)
+  if (!is.list(levels)) levels <- list(levels)
+  lapply(levels,length)
+}
 #'
 #' Set Levels of H2O Factor Column
 #'
@@ -1849,14 +1898,18 @@ mean.H2OFrame <- h2o.mean
 #}
 
 #'
-#' Variance of a column.
+#' Variance of a column or covariance of columns.
 #'
-#' Obtain the variance of a column of a parsed H2O data object.
+#' Compute the variance or covariance matrix of one or two H2OFrames.
 #'
 #' @param x An H2OFrame object.
-#' @param y \code{NULL} (default) or a column of an H2OFrame object. The default is equivalent to y = x (but more efficient).
+#' @param y \code{NULL} (default) or an H2OFrame. The default is equivalent to y = x.
 #' @param na.rm \code{logical}. Should missing values be removed?
-#' @param use An optional character string to be used in the presence of missing values. This must be one of the following strings. "everything", "all.obs", or "complete.obs".
+#' @param use An optional character string indicating how to handle missing values. This must be one of the following: 
+#   "everything"            - outputs NaNs whenever one of its contributing observations is missing
+#   "all.obs"               - presence of missing observations will throw an error
+#   "complete.obs"          - discards missing values along with all observations in their rows so that only complete observations are used
+#   "pairwise.complete.obs" - uses all complete pairs of observations
 #' @seealso \code{\link[stats]{var}} for the base R implementation. \code{\link{h2o.sd}} for standard deviation.
 #' @examples
 #' \donttest{
@@ -1867,16 +1920,20 @@ mean.H2OFrame <- h2o.mean
 #' }
 #' @export
 h2o.var <- function(x, y = NULL, na.rm = FALSE, use) {
-  if( na.rm ) stop("na.rm versions not impl")
-  if( is.null(y) ) y <- x
+  symmetric <- FALSE
+  if( is.null(y) ) {
+    y <- x
+    if( ncol(x) > 1 && nrow(x) > 1) symmetric <- TRUE
+  }
   if(!missing(use)) {
-    if (use %in% c("pairwise.complete.obs", "na.or.complete"))
-      stop("Unimplemented : `use` may be either \"everything\", \"all.obs\", or \"complete.obs\"")
-  } else
-    use <- "everything"
+    if (use == "na.or.complete")
+      stop("Unimplemented : `use` may be either \"everything\", \"all.obs\", \"complete.obs\", or \"pairwise.complete.obs\"")
+  } else {
+    if (na.rm) use <- "complete.obs" else use <- "everything"
+  }
   # Eager, mostly to match prior semantics but no real reason it need to be
-  expr <- .newExpr("var",x,y,.quote(use))
-  if( (nrow(x)==1L || ncol(x)==1L) ) .eval.scalar(expr)
+  expr <- .newExpr("var",x,y,.quote(use),symmetric)
+  if( (nrow(x)==1L || (ncol(x)==1L && ncol(y)==1L)) ) .eval.scalar(expr)
   else .fetch.data(expr,ncol(x))
 }
 
@@ -1917,6 +1974,45 @@ sd <- function(x, na.rm=FALSE) {
 }
 
 #'
+#' Round doubles/floats to the given number of significant digits.
+#'
+#' @name h2o.signif
+#' @param x An H2OFrame object.
+#' @param digits Number of significant digits to round doubles/floats.
+#' @seealso \code{\link[base]{signif}} for the base R implementation.
+#' @export
+h2o.signif <- function(x, digits=6) .newExpr("signif",chk.H2OFrame(x),digits)
+
+#' @rdname h2o.signif
+#' @export
+signif <- function(x, digits=6) {
+  if( is.H2OFrame(x) ) h2o.signif(x,digits)
+  else base::signif(x,digits)
+}
+
+#'
+#' Round doubles/floats to the given number of decimal places.
+#'
+#' @name h2o.round
+#' @param x An H2OFrame object.
+#' @param digits Number of decimal places to round doubles/floats. Rounding to a negative number of decimal places is 
+#         not supported. For rounding off a 5, the IEC 60559 standard is used, 'go to the even digit'. Therefore 
+#         rounding 2.5 gives 2 and rounding 3.5 gives 4.
+#' @seealso \code{\link[base]{round}} for the base R implementation.
+#' @export
+h2o.round <- function(x, digits=0) .newExpr("round",chk.H2OFrame(x),digits)
+
+
+#' @rdname h2o.round
+#' @export
+round <- function(x, digits=0) {
+  if( is.H2OFrame(x) ) h2o.round(x,digits)
+  else base::round(x,digits)
+}
+
+
+
+#'
 #' Scaling and Centering of an H2OFrame
 #'
 #' Centers and/or scales the columns of an H2O dataset.
@@ -1958,6 +2054,10 @@ scale.H2OFrame <- h2o.scale
 as.h2o <- function(x, destination_frame= "") {
   .key.validate(destination_frame)
 
+  dest_name <- if( destination_frame=="") deparse(substitute(x)) else destination_frame
+  if( nzchar(dest_name) && regexpr("^[a-zA-Z_][a-zA-Z0-9_.]*$", dest_name)[1L] == -1L )
+    dest_name <- destination_frame
+
   # TODO: Be careful, there might be a limit on how long a vector you can define in console
   if(!is.data.frame(x))
     if( length(x)==1L ) x <- data.frame(C1=x)
@@ -1973,7 +2073,7 @@ as.h2o <- function(x, destination_frame= "") {
   types <- gsub("Date", "Time", types)
   tmpf <- tempfile(fileext = ".csv")
   write.csv(x, file = tmpf, row.names = FALSE, na="NA_h2o")
-  h2f <- h2o.uploadFile(tmpf, destination_frame = destination_frame, header = TRUE, col.types=types,
+  h2f <- h2o.uploadFile(tmpf, destination_frame = dest_name, header = TRUE, col.types=types,
                         col.names=colnames(x, do.NULL=FALSE, prefix="C"), na.strings=rep(c("NA_h2o"),ncol(x)))
   file.remove(tmpf)
   h2f
diff --git a/h2o-r/h2o-package/R/gbm.R b/h2o-r/h2o-package/R/gbm.R
index 386640d..59a0f5f 100755
--- a/h2o-r/h2o-package/R/gbm.R
+++ b/h2o-r/h2o-package/R/gbm.R
@@ -16,7 +16,8 @@
 #' @param checkpoint "Model checkpoint (either key or H2ODeepLearningModel) to resume training with."
 #' @param ignore_const_cols A logical value indicating whether or not to ignore all the constant columns in the training frame.
 #' @param distribution A \code{character} string. The distribution function of the response.
-#'        Must be "AUTO", "bernoulli", "multinomial", "poisson", "gamma", "tweedie", "laplace" or "gaussian"
+#'        Must be "AUTO", "bernoulli", "multinomial", "poisson", "gamma", "tweedie", "laplace", "quantile" or "gaussian"
+#' @param quantile_alpha Quantile (only for Quantile regression, must be between 0 and 1)
 #' @param tweedie_power Tweedie power (only for Tweedie distribution, must be between 1 and 2)
 #' @param ntrees A nonnegative integer that determines the number of trees to grow.
 #' @param max_depth Maximum depth to grow the tree.
@@ -45,6 +46,7 @@
 #'        Must be "AUTO", "Random" or "Modulo".
 #' @param keep_cross_validation_predictions Whether to keep the predictions of the cross-validation models
 #' @param score_each_iteration Attempts to score each tree.
+#' @param score_tree_interval Score the model after every so many trees. Disabled if set to 0.
 #' @param stopping_rounds Early stopping based on convergence of stopping_metric.
 #'        Stop if simple moving average of length k of the stopping_metric does not improve
 #'        (by stopping_tolerance) for k=stopping_rounds scoring events.
@@ -76,7 +78,8 @@ h2o.gbm <- function(x, y, training_frame,
                     model_id,
                     checkpoint,
                     ignore_const_cols = TRUE,
-                    distribution = c("AUTO","gaussian", "bernoulli", "multinomial", "poisson", "gamma", "tweedie", "laplace"),
+                    distribution = c("AUTO","gaussian", "bernoulli", "multinomial", "poisson", "gamma", "tweedie", "laplace", "quantile"),
+                    quantile_alpha = 0.5,
                     tweedie_power = 1.5,
                     ntrees = 50,
                     max_depth = 5,
@@ -98,6 +101,7 @@ h2o.gbm <- function(x, y, training_frame,
                     fold_assignment = c("AUTO","Random","Modulo"),
                     keep_cross_validation_predictions = FALSE,
                     score_each_iteration = FALSE,
+                    score_tree_interval = 0,
                     stopping_rounds=0,
                     stopping_metric=c("AUTO", "deviance", "logloss", "MSE", "AUC", "r2", "misclassification"),
                     stopping_tolerance=1e-3,
@@ -140,6 +144,8 @@ h2o.gbm <- function(x, y, training_frame,
     parms$ignore_const_cols <- ignore_const_cols
   if (!missing(distribution))
     parms$distribution <- distribution
+  if (!missing(quantile_alpha))
+    parms$quantile_alpha <- quantile_alpha
   if (!missing(tweedie_power))
     parms$tweedie_power <- tweedie_power
   if (!missing(ntrees))
@@ -174,8 +180,8 @@ h2o.gbm <- function(x, y, training_frame,
     parms$build_tree_one_node <- build_tree_one_node
   if (!missing(nfolds))
     parms$nfolds <- nfolds
-  if (!missing(score_each_iteration))
-    parms$score_each_iteration <- score_each_iteration
+  if (!missing(score_each_iteration)) parms$score_each_iteration <- score_each_iteration
+  if (!missing(score_tree_interval)) parms$score_tree_interval <- score_tree_interval
   if( !missing(offset_column) )             parms$offset_column          <- offset_column
   if( !missing(weights_column) )            parms$weights_column         <- weights_column
   if( !missing(fold_column) )               parms$fold_column            <- fold_column
diff --git a/h2o-r/h2o-package/R/glm.R b/h2o-r/h2o-package/R/glm.R
index 2b9240f..00a3646 100755
--- a/h2o-r/h2o-package/R/glm.R
+++ b/h2o-r/h2o-package/R/glm.R
@@ -31,7 +31,7 @@
 #' @param lambda A non-negative shrinkage parameter for the elastic-net, which multiplies \eqn{P(\alpha,\beta)} in the objective function.
 #'               When \code{lambda = 0}, no elastic-net penalty is applied and ordinary generalized linear models are fit.
 #' @param prior (Optional) A numeric specifying the prior probability of class 1 in the response when \code{family = "binomial"}.
-#'               The default prior is the observational frequency of class 1.
+#'               The default prior is the observational frequency of class 1. Must be from (0,1) exclusive range or NULL (no prior).
 #' @param lambda_search A logical value indicating whether to conduct a search over the space of lambda values starting from the lambda max, given
 #'                      \code{lambda} is interpreted as lambda min.
 #' @param nlambdas The number of lambda values to use when \code{lambda_search = TRUE}.
@@ -58,6 +58,7 @@
 #' @param non_negative Logical, allow only positive coefficients.
 #' @param compute_p_values (Optional)  Logical, compute p-values, only allowed with IRLSM solver and no regularization. May fail if there are collinear predictors.
 #' @param remove_collinear_columns (Optional)  Logical, valid only with no regularization. If set, co-linear columns will be automatically ignored (coefficient will be 0).
+#' @param missing_values_handling (Optional) Controls handling of missing values. Can be either "MeanImputation" or "Skip". MeanImputation replaces missing values with mean for numeric and most frequent level for categorical,  Skip ignores observations with any missing value. Applied both during model training *AND* scoring.
 #' @param max_runtime_secs Maximum allowed runtime in seconds for model training. Use 0 to disable.
 #' @param ... (Currently Unimplemented)
 #'        coefficients.
@@ -109,12 +110,12 @@ h2o.glm <- function(x, y, training_frame, model_id,
                     beta_epsilon = 0,
                     solver = c("IRLSM", "L_BFGS"),
                     standardize = TRUE,
-                    family = c("gaussian", "binomial", "poisson", "gamma", "tweedie"),
+                    family = c("gaussian", "binomial", "poisson", "gamma", "tweedie","multinomial"),
                     link = c("family_default", "identity", "logit", "log", "inverse", "tweedie"),
                     tweedie_variance_power = NaN,
                     tweedie_link_power = NaN,
                     alpha = 0.5,
-                    prior = 0.0,
+                    prior = NULL,
                     lambda = 1e-05,
                     lambda_search = FALSE,
                     nlambdas = -1,
@@ -133,7 +134,8 @@ h2o.glm <- function(x, y, training_frame, model_id,
                     non_negative = FALSE,
                     compute_p_values = FALSE,
                     remove_collinear_columns = FALSE,
-                    max_runtime_secs = 0)
+                    max_runtime_secs = 0,
+                    missing_values_handling = c("Skip", "MeanImputation"))
 {
   # if (!is.null(beta_constraints)) {
   #     if (!inherits(beta_constraints, "data.frame") && !is.H2OFrame(beta_constraints))
@@ -198,6 +200,8 @@ h2o.glm <- function(x, y, training_frame, model_id,
     parms$nfolds <- nfolds
   if(!missing(beta_constraints))
     parms$beta_constraints <- beta_constraints
+    if(!missing(missing_values_handling))
+        parms$missing_values_handling <- missing_values_handling
   m <- .h2o.modelJob('glm', parms)
   m@model$coefficients <- m@model$coefficients_table[,2]
   names(m@model$coefficients) <- m@model$coefficients_table[,1]
diff --git a/h2o-r/h2o-package/R/grid.R b/h2o-r/h2o-package/R/grid.R
index 2f85e42..3911939 100644
--- a/h2o-r/h2o-package/R/grid.R
+++ b/h2o-r/h2o-package/R/grid.R
@@ -23,7 +23,7 @@
 #' @param search_criteria  (Optional)  List of control parameters for smarter hyperparameter search.  Specify the
 #'        Random strategy to get random search of all the combinations of your hyperparameters.  Generally this
 #'        should be combined with an early stopping criterion,
-#'        max_models or max_time_ms, e.g. \code{list(strategy = "Random", max_models = 42)}.  
+#'        max_models or max_time_ms, e.g. \code{list(strategy = "RandomDiscrete", max_models = 42)}.  
 #' @importFrom jsonlite toJSON
 #' @examples
 #' \donttest{
@@ -46,7 +46,7 @@ h2o.grid <- function(algorithm,
                      hyper_params = list(),
                      is_supervised = NULL,
                      do_hyper_params_check = FALSE,
-                     search_criteria = list())
+                     search_criteria = NULL)
 {
   # Extract parameters
   dots <- list(...)
@@ -97,8 +97,15 @@ h2o.grid <- function(algorithm,
   # Append grid parameters in JSON form
   params$hyper_parameters <- toJSON(hyper_values, digits=99)
 
-  # pass along the search criteria among the other params
-  params <- c(params, search_criteria)
+  if( !is.null(search_criteria)) {
+      # Append grid search criteria in JSON form. 
+      # jsonlite unfortunately doesn't handle scalar values so we need to serialize ourselves.
+      keys = paste0("\"", names(search_criteria), "\"", "=")
+      vals <- lapply(search_criteria, function(val) { if(is.numeric(val)) val else paste0("\"", val, "\"") })
+      body <- paste0(paste0(keys, vals), collapse=",")
+      js <- paste0("{", body, "}", collapse="")
+      params$search_criteria <- js
+  }
 
   # Append grid_id if it is specified
   if (!missing(grid_id)) params$grid_id <- grid_id
diff --git a/h2o-r/h2o-package/R/import.R b/h2o-r/h2o-package/R/import.R
index 8f96f99..919e3a6 100755
--- a/h2o-r/h2o-package/R/import.R
+++ b/h2o-r/h2o-package/R/import.R
@@ -136,12 +136,13 @@ h2o.uploadFile <- function(path, destination_frame = "",
 
   .h2o.gc()  # Clear out H2O to make space for new file
   path <- normalizePath(path, winslash = "/")
-  srcKey <- .key.make(path)
+  srcKey <- .key.make( path )
   urlSuffix <- sprintf("PostFile?destination_frame=%s",  curlEscape(srcKey))
   fileUploadInfo <- fileUpload(path)
   .h2o.doSafePOST(h2oRestApiVersion = .h2o.__REST_API_VERSION, urlSuffix = urlSuffix, fileUploadInfo = fileUploadInfo)
 
-  rawData = .newH2OFrame(op="PostFile",id=srcKey,-1,-1)
+  rawData <- .newH2OFrame(op="PostFile",id=srcKey,-1,-1)
+  destination_frame <- if( destination_frame == "" ) .key.make(strsplit(basename(path), "\\.")[[1]][1]) else destination_frame
   if (parse) {
     h2o.parseRaw(data=rawData, destination_frame=destination_frame, header=header, sep=sep, col.names=col.names, col.types=col.types, na.strings=na.strings, blocking=!progressBar, parse_type = parse_type)
   } else {
diff --git a/h2o-r/h2o-package/R/kvstore.R b/h2o-r/h2o-package/R/kvstore.R
index 5321482..a04f62a 100644
--- a/h2o-r/h2o-package/R/kvstore.R
+++ b/h2o-r/h2o-package/R/kvstore.R
@@ -5,8 +5,8 @@
 .key.validate <- function(key) {
   if (!missing(key) && !is.null(key)) {
     stopifnot( is.character(key) && length(key) == 1L && !is.na(key) )
-    if( nzchar(key) && regexpr("^[a-zA-Z_][a-zA-Z0-9_.]*$", key)[1L] == -1L )
-      stop("`key` must match the regular expression '^[a-zA-Z_][a-zA-Z0-9_.]*$'")
+    if( nzchar(key) && regexpr("^[a-zA-Z_][a-zA-Z0-9_.-]*$", key)[1L] == -1L )
+      stop(paste0("`key` must match the regular expression '^[a-zA-Z_][a-zA-Z0-9_.]*$': ", key))
   }
   invisible(TRUE)
 }
@@ -62,6 +62,7 @@ h2o.ls <- function() {
 #' }
 #' @export
 h2o.removeAll <- function(timeout_secs=0) {
+  gc()
   tryCatch(
     invisible(.h2o.__remoteSend(.h2o.__DKV, method = "DELETE", timeout=timeout_secs)),
     error = function(e) {
@@ -81,6 +82,7 @@ h2o.removeAll <- function(timeout_secs=0) {
 #' @seealso \code{\link{h2o.assign}}, \code{\link{h2o.ls}}
 #' @export
 h2o.rm <- function(ids) {
+  gc()
   if( !is.vector(ids) ) x_list = c(ids) else x_list = ids
   for (xi in x_list) {
     if( is.null(xi) ) stop("h2o.rm with NULL object is not supported")
@@ -100,7 +102,6 @@ h2o.rm <- function(ids) {
   #remove object from R client if possible (not possible for input of strings)
   ids <- deparse(substitute(ids))
   if( exists(ids, envir=parent.frame()) ) rm(list=ids, envir=parent.frame())
-  
 }
 
 #'
diff --git a/h2o-r/h2o-package/R/randomforest.R b/h2o-r/h2o-package/R/randomforest.R
index 35aa520..91a84d5 100644
--- a/h2o-r/h2o-package/R/randomforest.R
+++ b/h2o-r/h2o-package/R/randomforest.R
@@ -46,6 +46,7 @@
 #'        Must be "AUTO", "Random" or "Modulo"
 #' @param keep_cross_validation_predictions Whether to keep the predictions of the cross-validation models
 #' @param score_each_iteration Attempts to score each tree.
+#' @param score_tree_interval Score the model after every so many trees. Disabled if set to 0.
 #' @param stopping_rounds Early stopping based on convergence of stopping_metric.
 #'        Stop if simple moving average of length k of the stopping_metric does not improve
 #'        (by stopping_tolerance) for k=stopping_rounds scoring events.
@@ -85,6 +86,7 @@ h2o.randomForest <- function(x, y, training_frame,
                              fold_assignment = c("AUTO","Random","Modulo"),
                              keep_cross_validation_predictions = FALSE,
                              score_each_iteration = FALSE,
+                             score_tree_interval = 0,
                              stopping_rounds=0,
                              stopping_metric=c("AUTO", "deviance", "logloss", "MSE", "AUC", "r2", "misclassification"),
                              stopping_tolerance=1e-3,
@@ -157,8 +159,8 @@ h2o.randomForest <- function(x, y, training_frame,
   if( !missing(fold_column) )               parms$fold_column            <- fold_column
   if( !missing(fold_assignment) )           parms$fold_assignment        <- fold_assignment
   if( !missing(keep_cross_validation_predictions) )  parms$keep_cross_validation_predictions  <- keep_cross_validation_predictions
-  if (!missing(score_each_iteration))
-    parms$score_each_iteration <- score_each_iteration
+  if (!missing(score_each_iteration)) parms$score_each_iteration <- score_each_iteration
+  if (!missing(score_tree_interval)) parms$score_tree_interval <- score_tree_interval
   if(!missing(stopping_rounds)) parms$stopping_rounds <- stopping_rounds
   if(!missing(stopping_metric)) parms$stopping_metric <- stopping_metric
   if(!missing(stopping_tolerance)) parms$stopping_tolerance <- stopping_tolerance
diff --git a/h2o-r/tests/runitUtils/utilsR.R b/h2o-r/tests/runitUtils/utilsR.R
index 8d01efd..9b43c51 100644
--- a/h2o-r/tests/runitUtils/utilsR.R
+++ b/h2o-r/tests/runitUtils/utilsR.R
@@ -401,3 +401,23 @@ function(seed = NULL, master_seed = FALSE) {
     Log.info(paste("USING SEED: ", SEED))
 }
 
+h2o_and_R_equal <- function(h2o_obj, r_obj, tolerance = 1e-6) {
+  df_h2o_obj <- as.data.frame(h2o_obj)
+  df_r_obj <- as.data.frame(r_obj)
+  expect_equal(length(df_h2o_obj), length(df_r_obj))
+  
+  #Check NAs are in same places 
+  df_h2o_nas <- if (length(df_h2o_obj) == 1) df_h2o_obj == "NaN" else is.na(df_h2o_obj)
+  if (length(df_h2o_nas) ==1 && is.na(df_h2o_nas)) df_h2o_nas <- is.na(df_h2o_obj)
+  df_r_nas <- is.na(df_r_obj)
+  expect_true(all(df_h2o_nas == df_r_nas))
+  
+  #Check non-NAs are same vals
+  df_h2o_obj_free <- df_h2o_obj[!df_h2o_nas]
+  df_r_na_free <- df_r_obj[!df_r_nas]
+  
+  expect_equal(length(df_h2o_obj_free), length(df_r_na_free))
+  if (length(df_r_na_free) > 0)
+    expect_true(all(abs(df_h2o_obj_free - df_r_na_free) < tolerance))
+  
+}
diff --git a/h2o-r/tests/testdir_algos/deeplearning/runit_deeplearning_poisson_weight.R b/h2o-r/tests/testdir_algos/deeplearning/runit_deeplearning_poisson_weight.R
index 2e77338..149fe65 100644
--- a/h2o-r/tests/testdir_algos/deeplearning/runit_deeplearning_poisson_weight.R
+++ b/h2o-r/tests/testdir_algos/deeplearning/runit_deeplearning_poisson_weight.R
@@ -31,7 +31,7 @@ test <- function() {
   print(max(ph[,1]))
 	expect_equal(1.996, mean_deviance, tolerance=1e-1)
 	expect_equal(1.05837, mean(ph[,1]), tolerance=1e-1 )
-	expect_equal(0.86598, min(ph[,1]), tolerance=1e-1 )
+	expect_equal(0.9155, min(ph[,1]), tolerance=1e-1 )
 	expect_equal(1.2629, max(ph[,1]), tolerance=1e-1 )
 		
 	
diff --git a/h2o-r/tests/testdir_algos/gbm/runit_GBMGrid_airlines.R b/h2o-r/tests/testdir_algos/gbm/runit_GBMGrid_airlines.R
index 65b915d..3f36dfc 100644
--- a/h2o-r/tests/testdir_algos/gbm/runit_GBMGrid_airlines.R
+++ b/h2o-r/tests/testdir_algos/gbm/runit_GBMGrid_airlines.R
@@ -34,7 +34,7 @@ gbm.grid.test <- function() {
     #
     # test random/max_models search criterion
     size_of_hyper_space <- 5
-    search_criteria = list(strategy = "Random", max_models = size_of_hyper_space)
+    search_criteria = list(strategy = "RandomDiscrete", max_models = size_of_hyper_space)
     air.grid <- h2o.grid("gbm", y = "IsDepDelayed", x = myX,
                          distribution="bernoulli",
                          training_frame = air.hex,
diff --git a/h2o-r/tests/testdir_jira/runit_pubdev_686_glm.R b/h2o-r/tests/testdir_jira/runit_pubdev_686_glm.R
index c920fc2..7649204 100644
--- a/h2o-r/tests/testdir_jira/runit_pubdev_686_glm.R
+++ b/h2o-r/tests/testdir_jira/runit_pubdev_686_glm.R
@@ -8,7 +8,6 @@ source("../../scripts/h2o-r-test-setup.R")
 
 
 test <- function() {
-
   print("Read allyears2k_headers.zip into R.")
   data.hex <-  h2o.importFile(locate("smalldata/airlines/allyears2k_headers.zip"), destination_frame="airlines.data")
 
@@ -21,10 +20,7 @@ test <- function() {
 
   # GLM - All columns are being filtered out due to NA content
   expect_error(h2o.glm(x = myX, y = myY, training_frame = data.hex, validation_frame = valid,
-    family = "gaussian"))
-
-
-  
+    family = "gaussian",missing_values_handling="Skip"))
 }
 
 doTest("GLM PUBDEV-686", test)
diff --git a/h2o-r/tests/testdir_misc/runit_NOPASS_revalue.R b/h2o-r/tests/testdir_misc/runit_NOPASS_revalue.R
deleted file mode 100644
index 045151a..0000000
--- a/h2o-r/tests/testdir_misc/runit_NOPASS_revalue.R
+++ /dev/null
@@ -1,30 +0,0 @@
-setwd(normalizePath(dirname(R.utils::commandArgs(asValues=TRUE)$"f")))
-source("../../scripts/h2o-r-test-setup.R")
-##
-# Generate lots of keys then remove them
-##
-
-
-
-
-test <- function() {
-
-  hex <- as.h2o(iris)
-
-  Log.info("Original factor column")
-  print(hex$Species)
-  print(h2o.levels(hex$Species))
-
-  h2o.setLevels(hex$Species, c(setosa = "NEW SETOSA ENUM", virginica = "NEW VIRG ENUM", versicolor = "NEW VERSI ENUM"))
-
-  Log.info("Revalued factor column")
-  print(hex$Species)
-  print(h2o.levels(hex$Species))
-  vals <- c("NEW SETOSA ENUM", "NEW VIRG ENUM", "NEW VERSI ENUM")
-  expect_equal(h2o.levels(hex$Species), vals)
-
-  
-}
-
-doTest("Many Keys Test: Removing", test)
-
diff --git a/h2o-r/tests/testdir_misc/runit_levels.R b/h2o-r/tests/testdir_misc/runit_levels.R
index c3523b1..8197864 100644
--- a/h2o-r/tests/testdir_misc/runit_levels.R
+++ b/h2o-r/tests/testdir_misc/runit_levels.R
@@ -8,7 +8,7 @@ test <- function(conn) {
   iris.hex = as.h2o(iris)
   
   Log.info("Find the factor levels h2o and R frame...")
-  levels1 <- sort(h2o.levels(iris.hex$Species))
+  levels1 <- sort(unlist(h2o.levels(iris.hex$Species)))
   levels2 <- sort(levels(iris$Species))
   print("Factor levels for Species column for H2OH2OFrame...")
   print(levels1)
@@ -22,13 +22,28 @@ test <- function(conn) {
   
   Log.info("Try printing the levels of a numeric column...")
   levels1 <- levels(iris$Sepal.Length)
-  levels2 <- h2o.levels(iris.hex$Sepal.Length)
+  levels2 <- unlist(h2o.levels(iris.hex$Sepal.Length))
   print("Factor levels for Sepal.Length column for H2OH2OFrame...")
   print(levels1)
   print("Factor levels for Sepal.Length column for dataframe...")
   print(levels2)  
   if(!is.null(levels2)) stop("Numeric Column should not have any factor levels...")
 
+  allLevels <- h2o.levels(iris.hex)
+  expect_true(is.list(allLevels))
+  expect_true(length(allLevels) == ncol(iris.hex))
+  numLevels <- h2o.nlevels(iris.hex)
+  expect_true(length(numLevels) == 5)
+  
+  oneLevel <- h2o.levels(iris.hex[,5])
+  expect_true(!is.list(oneLevel))
+  expect_true(length(oneLevel) == 3)
+  numLevels <- h2o.nlevels(iris.hex[,5])
+  expect_true(numLevels == 3)
+  
+  oneLevel <- h2o.levels(iris.hex,5)
+  expect_true(!is.list(oneLevel))
+  expect_true(length(oneLevel) == 3)
   
 }
 
diff --git a/h2o-r/tests/testdir_misc/runit_revalue.R b/h2o-r/tests/testdir_misc/runit_revalue.R
new file mode 100644
index 0000000..1645b54
--- /dev/null
+++ b/h2o-r/tests/testdir_misc/runit_revalue.R
@@ -0,0 +1,30 @@
+setwd(normalizePath(dirname(R.utils::commandArgs(asValues=TRUE)$"f")))
+source("../../scripts/h2o-r-test-setup.R")
+##
+# Generate lots of keys then remove them
+##
+
+
+
+
+test <- function() {
+
+  hex <- as.h2o(iris)
+
+  Log.info("Original factor column")
+  print(hex$Species)
+  print(h2o.levels(hex$Species))
+
+  zz <- h2o.setLevels(hex$Species, c(setosa = "NEW SETOSA ENUM", virginica = "NEW VIRG ENUM", versicolor = "NEW VERSI ENUM"))
+
+  Log.info("Revalued factor column")
+  print(zz)
+  print(h2o.levels(zz))
+  vals <- c("NEW SETOSA ENUM", "NEW VIRG ENUM", "NEW VERSI ENUM")
+  expect_equal(h2o.levels(zz), vals)
+
+  
+}
+
+doTest("Many Keys Test: Removing", test)
+
diff --git a/h2o-r/tests/testdir_misc/runit_scalarNaNIsNumeric.R b/h2o-r/tests/testdir_misc/runit_scalarNaNIsNumeric.R
new file mode 100644
index 0000000..b9b2de0
--- /dev/null
+++ b/h2o-r/tests/testdir_misc/runit_scalarNaNIsNumeric.R
@@ -0,0 +1,11 @@
+setwd(normalizePath(dirname(R.utils::commandArgs(asValues=TRUE)$"f")))
+source("../../scripts/h2o-r-test-setup.R")
+
+test.scalar.nan.is.numeric <- function() {
+  fr <- as.h2o(iris)
+  h2o.insertMissingValues(fr)
+  numeric_NaN <- sum(fr[,1])
+  expect_true(is.numeric(numeric_NaN))
+}
+
+doTest("NaN is numeric value", test.scalar.nan.is.numeric)
diff --git a/h2o-r/tests/testdir_munging/exec/runit_merge_no_shared_cols.R b/h2o-r/tests/testdir_munging/exec/runit_merge_no_shared_cols.R
index af42a2b..7c4c20b 100644
--- a/h2o-r/tests/testdir_munging/exec/runit_merge_no_shared_cols.R
+++ b/h2o-r/tests/testdir_munging/exec/runit_merge_no_shared_cols.R
@@ -1,6 +1,5 @@
 setwd(normalizePath(dirname(R.utils::commandArgs(asValues=TRUE)$"f")))
 source("../../../scripts/h2o-r-test-setup.R")
-setwd(normalizePath(dirname(R.utils::commandArgs(asValues= TRUE)$"f")))
 
 
 check.merge_no_shared_cols <- function() {
diff --git a/h2o-r/tests/testdir_munging/runit_merge.R b/h2o-r/tests/testdir_munging/runit_merge.R
new file mode 100644
index 0000000..d46086f
--- /dev/null
+++ b/h2o-r/tests/testdir_munging/runit_merge.R
@@ -0,0 +1,32 @@
+setwd(normalizePath(dirname(R.utils::commandArgs(asValues=TRUE)$"f")))
+source("../../scripts/h2o-r-test-setup.R")
+##
+# Test out the merge() functionality
+##
+
+
+test.merge <- function() {
+
+  #HEXDEV-536
+  g <- data.frame(A=c(1,1,2,2,3,3),B=rep(8,6))
+  h <- data.frame(A=c(1))
+  run.merge.tests(g,h)
+  h <- data.frame(A=c(2:4))
+  run.merge.tests(g,h)
+
+  #HEXDEV-538
+  g <- data.frame(A = c(1,1,2,2,3,3), B = c(0.1,0.3,0.6,1,1.5,2.1), index=1:6)
+  h <- data.frame(A = 1:3,mean_B = c(.2,.8,1.8), sdev_B = c(sd(c(.1,.3)), sd(c(.6,1)),sd(c(1.5,2.1))))
+  run.merge.tests(g,h)  
+}
+
+run.merge.tests <- function (g,h) {
+  h2o_g <- as.h2o(g)
+  h2o_h <- as.h2o(h)
+  h2o_merge <- h2o.merge(h2o_g, h2o_h)
+  R_merge <- merge(g,h)
+  h2o_and_R_equal(h2o_merge, R_merge[names(h2o_merge)])
+}
+
+
+doTest("Test out the merge() functionality", test.merge)
diff --git a/h2o-r/tests/testdir_munging/unop/runit_covar.R b/h2o-r/tests/testdir_munging/unop/runit_covar.R
new file mode 100644
index 0000000..7c40c7b
--- /dev/null
+++ b/h2o-r/tests/testdir_munging/unop/runit_covar.R
@@ -0,0 +1,62 @@
+setwd(normalizePath(dirname(R.utils::commandArgs(asValues=TRUE)$"f")))
+source("../../../scripts/h2o-r-test-setup.R")
+##
+# Test out the var() functionality
+##
+
+
+test.var <- function() {
+  #1 row case
+  n <- 20
+  g <- runif(n)
+  h <- runif(n)
+  run.var.tests(g,h,one_row=TRUE)
+  g[4] <- NA
+  run.var.tests(g,h,one_row=TRUE, has_nas=TRUE)
+  
+  #1 column case
+  run.var.tests(g,h, has_nas = TRUE)
+  g[4] <- runif(1)
+  run.var.tests(g,h)
+  
+  #Matrices
+  g <- matrix(runif(n),nrow=4)
+  h <- matrix(runif(12),nrow=4)
+  run.var.tests(g,h)
+  g[2,3] <- NA
+  g[1,4] <- NA
+  h[2,3] <- NA
+  run.var.tests(g,h,has_nas=TRUE)
+}
+
+run.var.tests <- function (g,h,one_row=FALSE,has_nas=FALSE) {
+  h2o_g <- as.h2o(g)
+  h2o_h <- as.h2o(h)
+  uses <- c("everything", "all.obs", "complete.obs", "pairwise.complete.obs")
+  if (has_nas) uses <- uses[-2]
+  for (na.rm in c(FALSE, TRUE)) {
+    for (use in uses) {
+      #2 inputs
+      if (one_row) {
+        h2o_var <- var(as.h2o(t(g)),as.h2o(t(h)), na.rm = na.rm, use = use)
+      } else {
+        h2o_var <- var(h2o_g, h2o_h, na.rm = na.rm, use = use)
+      }
+      R_var <- var(g,h, na.rm = na.rm, use = use)
+      h2o_and_R_equal(h2o_var, R_var)
+
+      #1 input
+      h2o_var <- var(h2o_g, na.rm=na.rm, use=use)
+      R_var <- var(g, na.rm=na.rm, use=use)
+      h2o_and_R_equal(h2o_var, R_var)
+    
+      #other input
+      h2o_var <- var(h2o_h, na.rm=na.rm, use=use)
+      R_var <- var(h, na.rm=na.rm, use=use)
+      h2o_and_R_equal(h2o_var, R_var)    
+    }
+  }
+}
+
+
+doTest("Test out the var() functionality", test.var)
diff --git a/h2o-r/tests/testdir_munging/unop/runit_round_signif.R b/h2o-r/tests/testdir_munging/unop/runit_round_signif.R
new file mode 100644
index 0000000..f8035be
--- /dev/null
+++ b/h2o-r/tests/testdir_munging/unop/runit_round_signif.R
@@ -0,0 +1,36 @@
+setwd(normalizePath(dirname(R.utils::commandArgs(asValues=TRUE)$"f")))
+source("../../../scripts/h2o-r-test-setup.R")
+##
+# Test out the round() and signif() functionality
+##
+
+
+test.round.signif <- function() {
+
+  for (g in c(-231.38523412,-3.5,3.5,0,NA,4.5,-4.5,4.6)) { 
+    run.round.tests(g)
+    run.signif.tests(g)
+  }
+}
+
+run.signif.tests <- function (g) {
+  h2o_g <- as.h2o(g)
+  #test default
+  h2o_and_R_equal(signif(h2o_g), signif(g))
+  for (digits in c(-3.5,-7:7,3.5)) {
+    h2o_and_R_equal(signif(h2o_g,digits=digits), signif(g, digits=digits))
+  }
+}
+
+run.round.tests <- function (g) {
+  h2o_g <- as.h2o(g)
+  #test default
+  h2o_and_R_equal(round(h2o_g), round(g))
+  for (digits in seq(0,8,by=.5)) {
+    h2o_and_R_equal(round(h2o_g,digits=digits), round(g, digits=digits))
+  }
+}
+
+
+
+doTest("Test out the round() and signif() functionality", test.round.signif)
diff --git a/h2o-scala/testMultiNode.sh b/h2o-scala/testMultiNode.sh
index 09bebca..55149f3 100755
--- a/h2o-scala/testMultiNode.sh
+++ b/h2o-scala/testMultiNode.sh
@@ -1,5 +1,13 @@
 #!/bin/bash
 
+# Argument parsing
+if [ "$1" = "jacoco" ]
+then
+    JACOCO_ENABLED=true
+else
+    JACOCO_ENABLED=false
+fi
+
 # Clean out any old sandbox, make a new one
 OUTDIR=sandbox
 rm -fr $OUTDIR; mkdir -p $OUTDIR
@@ -27,6 +35,18 @@ function cleanup () {
 
 trap cleanup SIGTERM SIGINT
 
+
+MAX_MEM="-Xmx2g"
+# Check if coverage should be run
+if [ $JACOCO_ENABLED = true ]
+then
+    AGENT="../jacoco/jacocoagent.jar"
+    COVERAGE="-javaagent:$AGENT=destfile=build/jacoco/h2o-scala.exec"
+    MAX_MEM="-Xmx8g"
+else
+    COVERAGE=""
+fi
+
 # Find java command
 if [ -z "$TEST_JAVA_HOME" ]; then
   # Use default
@@ -37,7 +57,7 @@ else
   # Increase XMX since JAVA_HOME can point to java6
   JAVA6_REGEXP=".*1\.6.*"
   if [[ $TEST_JAVA_HOME =~ $JAVA6_REGEXP ]]; then
-    JAVA_CMD="${JAVA_CMD} -Xmx2g"
+    JAVA_CMD="${JAVA_CMD} $MAX_MEM"
   fi
 fi
 
@@ -45,8 +65,10 @@ fi
 #   build/classes/main - Main h2o core classes
 #   build/classes/test - Test h2o core classes
 #   build/resources/main - Main resources (e.g. page.html)
+
+
 # Command to invoke test
-JVM="nice $JAVA_CMD -ea -cp build/libs/h2o-scala_2.10.jar${SEP}build/libs/h2o-scala_2.10-test.jar${SEP}../h2o-core/build/libs/h2o-core.jar${SEP}../h2o-core/build/libs/h2o-core-test.jar${SEP}../h2o-genmodel/build/libs/h2o-genmodel.jar${SEP}../lib/*"
+JVM="nice $JAVA_CMD $COVERAGE -ea -cp build/libs/h2o-scala_2.10.jar${SEP}build/libs/h2o-scala_2.10-test.jar${SEP}../h2o-core/build/libs/h2o-core.jar${SEP}../h2o-core/build/libs/h2o-core-test.jar${SEP}../h2o-genmodel/build/libs/h2o-genmodel.jar${SEP}../lib/*"
 echo "$JVM" > $OUTDIR/jvm_cmd.txt
 
 # Runner
@@ -67,8 +89,16 @@ $JVM water.H2O -name $CLUSTER_NAME -baseport $CLUSTER_BASEPORT --ga_opt_out 1> $
 $JVM water.H2O -name $CLUSTER_NAME -baseport $CLUSTER_BASEPORT --ga_opt_out 1> $OUTDIR/out.3 2>&1 & PID_3=$!
 $JVM water.H2O -name $CLUSTER_NAME -baseport $CLUSTER_BASEPORT --ga_opt_out 1> $OUTDIR/out.4 2>&1 & PID_4=$!
 
+# If coverage is being run, then pass a system variable flag so that timeout limits are increased.
+if [ $JACOCO_ENABLED = true ]
+then
+    JACOCO_FLAG="-Dtest.jacocoEnabled=true"
+else
+    JACOCO_FLAG=""
+fi
+
 # Launch last driver JVM.  All output redir'd at the OS level to sandbox files.
 echo Running h2o-scala junit tests...
-($JVM -Dai.h2o.name=$CLUSTER_NAME -Dai.h2o.baseport=$CLUSTER_BASEPORT -Dai.h2o.ga_opt_out=yes $JUNIT_RUNNER `cat $OUTDIR/tests.txt` 2>&1 ; echo $? > $OUTDIR/status.0) 1> $OUTDIR/out.0 2>&1
+($JVM -Dai.h2o.name=$CLUSTER_NAME -Dai.h2o.baseport=$CLUSTER_BASEPORT -Dai.h2o.ga_opt_out=yes $JACOCO_FLAG $JUNIT_RUNNER `cat $OUTDIR/tests.txt` 2>&1 ; echo $? > $OUTDIR/status.0) 1> $OUTDIR/out.0 2>&1
 
 cleanup
diff --git a/h2o-test-integ/build.gradle b/h2o-test-integ/build.gradle
index d391f33..b640254 100644
--- a/h2o-test-integ/build.gradle
+++ b/h2o-test-integ/build.gradle
@@ -14,13 +14,21 @@ def pythonMultiJVMTestsPath = new File("$rootDir/py/testdir_multi_jvm")
 
 task runPythonMultiJVMTests(type: Exec) {
     workingDir pythonMultiJVMTestsPath
-    commandLine 'python', runner, '--wipeall', '--numclouds', '2', '--jvm.xmx', '2g'
+	def args = ['python', runner, '--wipeall', '--numclouds', '2', '--jvm.xmx', '2g']
+    if (project.hasProperty("jacocoCoverage")) {
+        args << '--jacoco'
+    }
+    commandLine args
 }
 
 task cleanPythonMultiJVMTests(type: Delete) {
     delete "$pythonMultiJVMTestsPath/results"
 }
 
+task cleanPythonMultiJVMTestsCoverageData(type: Delete) {
+    delete "$pythonMultiJVMTestsPath/results/jacoco"
+}
+
 apply from: '../gradle/dataCheck.gradle'
 
 test.dependsOn smalldataCheck
@@ -35,14 +43,27 @@ def nodeJsSingleJVMTestsPath = new File("$rootDir/h2o-web")
 
 task runNodeJsSingleJVMTests(type: Exec) {
     workingDir nodeJsSingleJVMTestsPath
-    commandLine 'python', runner, '--wipeall', '--numclouds', '1', '--test', 'src/main/resources/www/steam/js/steam-tests.js', '--jvm.xmx', '4g'
+    def args = ['python', runner, '--wipeall', '--numclouds', '1', '--test', 'src/main/resources/www/steam/js/steam-tests.js', '--jvm.xmx', '4g'
+]
+    if (project.hasProperty("jacocoCoverage")) {
+        args << '--jacoco'
+    }
+    commandLine args
 }
 
 task cleanNodeJsSingleJVMTests(type: Delete) {
     delete "$nodeJsSingleJVMTestsPath/results"
 }
 
+task cleanNodeJsSingleJVMTestsCoverageData(type: Delete) {
+    delete "$nodeJsSingleJVMTestsPath/results/jacoco"
+}
+
 // Commented out on 12/15/2014
 // test.dependsOn runNodeJsSingleJVMTests
 clean.dependsOn cleanNodeJsSingleJVMTests
 
+//task cleanCoverageData {
+//    dependsOn cleanNodeJsSingleJVMTestsCoverageData, cleanPythonMultiJVMTestsCoverageData
+//}
+
diff --git a/h2o-web/bower.json b/h2o-web/bower.json
index 5e2bd55..cbaad89 100644
--- a/h2o-web/bower.json
+++ b/h2o-web/bower.json
@@ -24,7 +24,7 @@
     "tests"
   ],
   "dependencies": {
-    "h2o-flow": "0.4.10"
+    "h2o-flow": "0.4.14"
   },
   "devDependencies": {}
 }
diff --git a/jacoco/coverage_tool/#CoverageTool.java# b/jacoco/coverage_tool/#CoverageTool.java#
new file mode 100644
index 0000000..fe75a14
--- /dev/null
+++ b/jacoco/coverage_tool/#CoverageTool.java#
@@ -0,0 +1,6 @@
+
+public class CoverageTool {
+
+    public static int main(String args[]) {
+    }
+}
\ No newline at end of file
diff --git a/jacoco/coverage_tool/DiffScanner.java b/jacoco/coverage_tool/DiffScanner.java
new file mode 100644
index 0000000..a416e1f
--- /dev/null
+++ b/jacoco/coverage_tool/DiffScanner.java
@@ -0,0 +1,361 @@
+//package diff;
+
+import java.io.*;
+import java.security.InvalidParameterException;
+import java.util.Scanner;
+import java.util.NoSuchElementException;
+import java.util.InputMismatchException;
+import java.util.regex.Pattern;
+import java.util.regex.Matcher;
+import java.util.regex.MatchResult;
+import java.util.List;
+import java.util.ArrayList;
+import java.util.Comparator;
+import java.util.Collections;
+import java.util.Iterator;
+
+/**
+ * Wrapper class for Java's Scanner. It is designed to parse the output of a diff provided by git.
+ **/
+public class DiffScanner {
+    private static final Pattern _BLOCK_HEADER_REGEX =
+            Pattern.compile("^@@\\s-(\\d+),(\\d+)\\s\\+(\\d+),(\\d+)\\s@@\\s.*$\\n", Pattern.MULTILINE);
+    private static final Pattern _BLOCK_REGEX =
+            Pattern.compile("(?=^[+\\-\\s])((?:^\\s.*$\\n)*)((?:^-.*$\\n)*)((?:^\\+.*$\\n)*)", Pattern.MULTILINE);
+    private static final Pattern _FILE_REGEX =
+            Pattern.compile("^diff\\s.*$\\n^index\\s\\w+\\.\\.\\w+\\s\\w+$\\n^---\\s(?:[abciow]/)?(.*)$\\n^\\+\\+\\+\\s(?:[abciow]/)?(.*)$\\n", Pattern.MULTILINE);
+
+    private Scanner _sc;
+    private MatchResult _next_match = null;
+    private NextType _next_type = NextType.NONE;
+
+    private enum NextType {
+        FILE, BLOCK_HEADER, BLOCK, NONE
+    }
+
+    public DiffScanner(InputStream source) {
+        _sc = new Scanner(source);
+    }
+
+    public DiffScanner(File source) throws FileNotFoundException {
+        _sc = new Scanner(source);
+    }
+
+    public boolean hasNextFile() {
+        if (_next_type == NextType.FILE) {
+            return true;
+        } else if (_next_type == NextType.NONE) {
+            try {
+                _sc.skip(_FILE_REGEX);
+            } catch (NoSuchElementException nsee) {
+                return false;
+            }
+            _next_match = _sc.match();
+            _next_type = NextType.FILE;
+            return true;
+        } else {
+            return false;
+        }
+    }
+
+    public DiffFile nextFile() {
+        if (_next_type == NextType.NONE) hasNextFile();
+        if (_next_type == NextType.FILE) {
+            DiffFile df = new DiffFile(new File(_next_match.group(1)), new File(_next_match.group(2)));
+            resetNext();
+            return df;
+        } else {
+            throw new InputMismatchException();
+        }
+    }
+
+    public boolean hasNextBlockHeader() {
+        if (_next_type == NextType.BLOCK_HEADER) {
+            return true;
+        } else if (_next_type == NextType.NONE) {
+            try {
+                _sc.skip(_BLOCK_HEADER_REGEX);
+            } catch (NoSuchElementException nsee) {
+                return false;
+            }
+            _next_match = _sc.match();
+            _next_type = NextType.BLOCK_HEADER;
+            return true;
+        } else {
+            return false;
+        }
+    }
+
+    public DiffBlockHeader nextBlockHeader() {
+        int r_start, r_length, i_start, i_length;
+
+        if (_next_type == NextType.NONE) hasNextBlockHeader();
+        if (_next_type == NextType.BLOCK_HEADER) {
+            r_start = Integer.parseInt(_next_match.group(1));
+            r_length = Integer.parseInt(_next_match.group(2));
+            i_start = Integer.parseInt(_next_match.group(3));
+            i_length = Integer.parseInt(_next_match.group(4));
+            if (r_start > 0) r_start -= 1;
+            if (i_start > 0) i_start -= 1;
+            DiffBlockHeader dbh = new DiffBlockHeader(r_start, r_length, i_start, i_length);
+            resetNext();
+            return dbh;
+        } else {
+            throw new InputMismatchException();
+        }
+    }
+
+    public boolean hasNextBlock() {
+        if (_next_type == NextType.BLOCK) {
+            return true;
+        } else if (_next_type == NextType.NONE) {
+            try {
+                _sc.skip(_BLOCK_REGEX);
+            } catch (NoSuchElementException nsee) {
+                return false;
+            }
+            _next_match = _sc.match();
+            _next_type = NextType.BLOCK;
+            return true;
+        } else {
+            return false;
+        }
+    }
+
+    public DiffBlock nextBlock() {
+        int blank_count, remove_count, insert_count;
+        if (_next_type == NextType.NONE) hasNextBlockHeader();
+        if (_next_type == NextType.BLOCK) {
+            blank_count = getLineCount(_next_match.group(1));
+            remove_count = getLineCount(_next_match.group(2));
+            insert_count = getLineCount(_next_match.group(3));
+            resetNext();
+            return new DiffBlock(blank_count - 1, remove_count, blank_count - 1, insert_count);
+        } else {
+            throw new InputMismatchException();
+        }
+    }
+
+    // Helper function to count the number of lines in a string
+    private int getLineCount(String s) {
+        if (s.isEmpty()) return 0;
+        Matcher m = Pattern.compile("\r\n|\r|\n").matcher(s);
+        int lines = 1;
+        while (m.find()) lines += 1;
+        return lines;
+    }
+
+    private void resetNext() {
+        _next_type = NextType.NONE;
+        _next_match = null;
+    }
+
+    public void close() {
+        _sc.close();
+    }
+
+
+    public static void main(String[] args) {
+        DiffScanner ds = null;
+        try {
+            ds = new DiffScanner(new File(args[0]));
+        } catch (FileNotFoundException fnfe) {
+            System.err.println("ERROR: Could not find file '" + args[0] + "'");
+            System.exit(1);
+        } catch (ArrayIndexOutOfBoundsException aioobe) {
+            System.out.println("ERROR: File name required");
+            System.exit(1);
+        }
+        System.out.println(ds.hasNextBlockHeader());
+        System.out.println(ds.hasNextBlock());
+        System.out.println(ds.hasNextFile());
+        System.out.println(ds.nextFile());
+        System.out.println(ds.hasNextBlockHeader());
+        System.out.println(ds.hasNextBlock());
+        System.out.println(ds.hasNextFile());
+        System.out.println(ds.nextBlockHeader());
+        System.out.println(ds.hasNextBlockHeader());
+        System.out.println(ds.hasNextBlock());
+        System.out.println(ds.hasNextFile());
+        System.out.println(ds.nextBlock());
+        System.out.println(ds.hasNextBlockHeader());
+        System.out.println(ds.hasNextBlock());
+        System.out.println(ds.hasNextFile());
+        System.out.println(ds.nextBlock());
+        System.out.println(ds.hasNextBlockHeader());
+        System.out.println(ds.hasNextBlock());
+        System.out.println(ds.hasNextFile());
+    }
+}
+
+class DiffFile {
+    private final File _old_file;
+    private final File _new_file;
+    private Comparator<DiffBlock> _comp;
+    private List<DiffBlock> _diffs;
+
+    public DiffFile(File file) {
+        this(file, file);
+    }
+
+    public DiffFile(File file, List<DiffBlock> diffs) {
+        this(file, file, diffs);
+    }
+
+    public DiffFile(File old_file, File new_file) {
+        this(old_file, new_file, new ArrayList<DiffBlock>());
+    }
+
+    public DiffFile(File old_file, File new_file, List<DiffBlock> diffs) {
+        _old_file = old_file;
+        _new_file = new_file;
+        _diffs = diffs;
+        sortByRemove();
+    }
+
+    public boolean pushDiff(DiffBlock diff) {
+        return _diffs.add(diff);
+    }
+
+    public Iterator<DiffBlock> iterator() {
+        Collections.sort(_diffs, _comp);
+        return _diffs.iterator();
+    }
+
+    public void sortByInsert() {
+        _comp = new Comparator<DiffBlock>() {
+            public int compare(DiffBlock o1, DiffBlock o2) {
+                return o1.getInsertStart() - o2.getInsertStart();
+            }
+        };
+    }
+
+    public void sortByRemove() {
+        _comp = new Comparator<DiffBlock>() {
+            public int compare(DiffBlock o1, DiffBlock o2) {
+                return o1.getRemoveStart() - o2.getRemoveStart();
+            }
+        };
+    }
+
+    public String toString() {
+        Iterator<DiffBlock> i = iterator();
+        String out = "DiffFile: '" + _old_file.getName() + "' -> '" + _new_file.getName() + "'";
+        while (i.hasNext()) out += "\n\t" + i.next().toString();
+        return out;
+    }
+
+}
+
+class DiffBlockHeader {
+    private final int _remove_start; // Starts with 0, add 1 for actual line number
+    private final int _insert_start; // Starts with 0, add 1 for actual line number
+
+    private int _remove_length;
+    private int _insert_length;
+
+    public DiffBlockHeader(int remove_start, int remove_length, int insert_start, int insert_length) {
+        if (remove_start < 0 || remove_length < 0 || insert_start < 0 || insert_length < 0) {
+            throw new InvalidParameterException("Parameters must be non-negative");
+        } else {
+            _remove_start = remove_start;
+            _remove_length = remove_length;
+            _insert_start = insert_start;
+            _insert_length = remove_length;
+        }
+    }
+
+    public int getRemoveStart() {
+        return _remove_start;
+    }
+
+    public int getRemoveLength() {
+        return _remove_length;
+    }
+
+    public int getInsertStart() {
+        return _insert_start;
+    }
+
+    public int getInsertLength() {
+        return _insert_length;
+    }
+
+    public String toString() {
+        String out = String.format("DiffBlockHeader: (%d, %d) -> (%d, %d)", _remove_start, _remove_length, _insert_start, _insert_length);
+        return out;
+    }
+}
+
+class DiffBlock {
+    private final int _remove_start; // Starts with 0, add 1 for actual line number
+    private final int _insert_start; // Starts with 0, add 1 for actual line number
+
+    private int _remove_length;
+    private int _insert_length;
+
+    public DiffBlock(int remove_start, int insert_start) {
+        this(remove_start, 0, insert_start, 0);
+    }
+
+    public DiffBlock(int remove_start, int remove_length, int insert_start, int insert_length) {
+        if (remove_start < 0 || remove_length < 0 || insert_start < 0 || insert_length < 0) {
+            throw new InvalidParameterException("Parameters must be non-negative");
+        } else {
+            _remove_start = remove_start;
+            _remove_length = remove_length;
+            _insert_start = insert_start;
+            _insert_length = remove_length;
+        }
+    }
+
+    public void pushRemove() {
+        _remove_length += 1;
+    }
+
+    public void pushRemove(int num) {
+        if (num > 0) {
+            _remove_length += num;
+        } else {
+            throw new InvalidParameterException("Parameter must be greater than zero");
+        }
+    }
+
+    public void pushInsert() {
+        _insert_length += 1;
+    }
+
+    public void pushInsert(int num) {
+        if (num > 0) {
+            _insert_length += num;
+        } else {
+            throw new InvalidParameterException("Parameter must be greater than zero");
+        }
+    }
+
+    public int getRemoveStart() {
+        return _remove_start;
+    }
+
+    public int getRemoveLength() {
+        return _remove_length;
+    }
+
+    public int getInsertStart() {
+        return _insert_start;
+    }
+
+    public int getInsertLength() {
+        return _insert_length;
+    }
+
+    public boolean isEmpty() {
+        return (_insert_length == 0 && _remove_length == 0);
+    }
+
+    public String toString() {
+        String out = String.format("DiffBlock: (%d, %d) -> (%d, %d)", _remove_start, _remove_length, _insert_start, _insert_length);
+        return out;
+    }
+
+}
diff --git a/jacoco/coverage_tool/out.txt b/jacoco/coverage_tool/out.txt
new file mode 100644
index 0000000..11f451f
--- /dev/null
+++ b/jacoco/coverage_tool/out.txt
@@ -0,0 +1,2095 @@
+diff --git a/h2o-algos/src/main/java/hex/deeplearning/DeepLearning.java b/h2o-algos/src/main/java/hex/deeplearning/DeepLearning.java
+index bbaeb9b..1215534 100755
+--- a/h2o-algos/src/main/java/hex/deeplearning/DeepLearning.java
++++ b/h2o-algos/src/main/java/hex/deeplearning/DeepLearning.java
+@@ -40,7 +40,7 @@ public class DeepLearning extends ModelBuilder<DeepLearningModel,DeepLearningMod
+   @Override public boolean isSupervised() { return !_parms._autoencoder; }
+ 
+   @Override protected int nModelsInParallel() {
+-    if (!_parms._parallelize_cross_validation) return 1; //user demands serial building
++    if (!_parms._parallelize_cross_validation || _parms._max_runtime_secs != 0) return 1; //user demands serial building (or we need to honor the time constraints for all CV models equally)
+     if (_train.byteSize() < 1e6) return _parms._nfolds; //for small data, parallelize over CV models
+     return 1;
+   }
+diff --git a/h2o-algos/src/main/java/hex/tree/gbm/GBM.java b/h2o-algos/src/main/java/hex/tree/gbm/GBM.java
+index 4af32b3..8ab3c4d 100755
+--- a/h2o-algos/src/main/java/hex/tree/gbm/GBM.java
++++ b/h2o-algos/src/main/java/hex/tree/gbm/GBM.java
+@@ -36,7 +36,7 @@ public class GBM extends SharedTree<GBMModel,GBMModel.GBMParameters,GBMModel.GBM
+   public GBM(boolean startup_once) { super(new GBMModel.GBMParameters(),startup_once); }
+ 
+   @Override protected int nModelsInParallel() {
+-    if (!_parms._parallelize_cross_validation) return 1; //user demands serial building
++    if (!_parms._parallelize_cross_validation || _parms._max_runtime_secs != 0) return 1; //user demands serial building (or we need to honor the time constraints for all CV models equally)
+     if (_train.byteSize() < 1e6) return _parms._nfolds; //for small data, parallelize over CV models
+     return 2; //GBM always has some serial work, so it's fine to build two models at once
+   }
+diff --git a/h2o-core/src/main/java/hex/Model.java b/h2o-core/src/main/java/hex/Model.java
+index 01ce4ae..7b776a4 100755
+--- a/h2o-core/src/main/java/hex/Model.java
++++ b/h2o-core/src/main/java/hex/Model.java
+@@ -119,7 +119,6 @@ public abstract class Model<M extends Model<M,P,O>, P extends Model.Parameters,
+ 
+     /**
+      * Maximum allowed runtime in seconds for model training. Use 0 to disable.
+-     * For cross-validation and grid searches, this time limit applies to all sub-models.
+      */
+     public double _max_runtime_secs = 0;
+ 
+diff --git a/h2o-core/src/main/java/hex/ModelBuilder.java b/h2o-core/src/main/java/hex/ModelBuilder.java
+index 7b98580..ac8889e 100644
+--- a/h2o-core/src/main/java/hex/ModelBuilder.java
++++ b/h2o-core/src/main/java/hex/ModelBuilder.java
+@@ -194,7 +194,7 @@ abstract public class ModelBuilder<M extends Model<M,P,O>, P extends Model.Param
+    * @return How many models to train in parallel during cross-validation
+    */
+   protected int nModelsInParallel() {
+-    if (!_parms._parallelize_cross_validation) return 1; //user demands serial building
++    if (!_parms._parallelize_cross_validation || _parms._max_runtime_secs != 0) return 1; //user demands serial building (or we need to honor the time constraints for all CV models equally)
+     if (_train.byteSize() < 1e6) return _parms._nfolds; //for small data, parallelize over CV models
+     return 1; //safe fallback
+   }
+@@ -356,7 +356,7 @@ abstract public class ModelBuilder<M extends Model<M,P,O>, P extends Model.Param
+       Log.info("Building cross-validation model " + (i + 1) + " / " + N + ".");
+       cvModelBuilders[i]._start_time = System.currentTimeMillis();
+       submodel_tasks[i] = H2O.submitTask(cvModelBuilders[i].trainModelImpl());
+-      if(nRunning++ == nModelsInParallel()) //piece-wise advance in training the CV models
++      if(++nRunning == nModelsInParallel()) //piece-wise advance in training the CV models
+         while (nRunning>0) submodel_tasks[i+1-nRunning--].join();
+     }
+     for( int i=0; i<N; ++i ) //all sub-models must be completed before the main model can be built
+diff --git a/h2o-core/src/main/java/hex/grid/GridSearch.java b/h2o-core/src/main/java/hex/grid/GridSearch.java
+index 9c58c83..9755c6b 100644
+--- a/h2o-core/src/main/java/hex/grid/GridSearch.java
++++ b/h2o-core/src/main/java/hex/grid/GridSearch.java
+@@ -74,6 +74,11 @@ public final class GridSearch<MP extends Model.Parameters> extends Keyed<GridSea
+    *  used only locally to fire new model builders.  */
+   private final transient HyperSpaceWalker<MP> _hyperSpaceWalker;
+ 
++  /** For advanced search methods we can put a time limit on the overall grid search.  This doesn't make much sense
++   * for strict Cartesian.
++   */
++  private long _max_time_ms = Long.MAX_VALUE;
++
+ 
+   private GridSearch(Key<Grid> gkey, HyperSpaceWalker<MP> hyperSpaceWalker) {
+     _result = gkey;
+@@ -153,6 +158,11 @@ public final class GridSearch<MP extends Model.Parameters> extends Keyed<GridSea
+       int counter = 0;
+       while (it.hasNext(model)) {
+         if(_job.stop_requested() ) return;  // Handle end-user cancel request
++        if  (it.timeRemaining() < 0) {
++          Log.info("Grid max_time_ms has expired; stopping early.");
++          return;
++        }
++
+         MP params;
+         try {
+           // Get parameters for next model
+@@ -160,7 +170,7 @@ public final class GridSearch<MP extends Model.Parameters> extends Keyed<GridSea
+           // Sequential model building, should never propagate
+           // exception up, just mark combination of model parameters as wrong
+           try {
+-            model = buildModel(params, grid, counter++, protoModelKey);
++            model = buildModel(params, grid, counter++, protoModelKey); // TODO: pass in remaining time!
+           } catch (RuntimeException e) { // Catch everything
+             StringWriter sw = new StringWriter();
+             PrintWriter pw = new PrintWriter(sw);
+diff --git a/h2o-core/src/main/java/hex/grid/HyperSpaceWalker.java b/h2o-core/src/main/java/hex/grid/HyperSpaceWalker.java
+index 1897787..05174de 100644
+--- a/h2o-core/src/main/java/hex/grid/HyperSpaceWalker.java
++++ b/h2o-core/src/main/java/hex/grid/HyperSpaceWalker.java
+@@ -35,6 +35,8 @@ public interface HyperSpaceWalker<MP extends Model.Parameters> {
+      */
+     boolean hasNext(Model previousModel);
+ 
++    long timeRemaining();
++
+     /**
+      * Inform the Iterator that a model build failed in case it needs to adjust its internal state.
+      * @param failedModel
+@@ -244,6 +246,9 @@ public interface HyperSpaceWalker<MP extends Model.Parameters> {
+         }
+ 
+         @Override
++        public long timeRemaining() { return Long.MAX_VALUE; }
++
++        @Override
+         public void modelFailed(Model failedModel) {
+           // nada
+         }
+@@ -318,6 +323,9 @@ public interface HyperSpaceWalker<MP extends Model.Parameters> {
+         /** One-based count of the permutations we've visited, primarily used as an index into _visitedHyperparamIndices. */
+         private int _currentPermutationNum = 0;
+ 
++        /** Start time of this grid */
++        private long _start_time = System.currentTimeMillis();
++
+         // TODO: override into a common subclass:
+         @Override
+         public MP nextModelParameters(Model previousModel) {
+@@ -352,6 +360,11 @@ public interface HyperSpaceWalker<MP extends Model.Parameters> {
+         }
+ 
+         @Override
++        public long timeRemaining() {
++          return _max_time_ms - (System.currentTimeMillis() - _start_time);
++        }
++
++        @Override
+         public void modelFailed(Model failedModel) {
+           // Leave _visitedPermutations, _visitedPermutationHashes and _currentHyperparamIndices alone
+           // so we don't revisit bad parameters. Note that if a model build fails for other reasons we
+diff --git a/h2o-core/src/main/java/water/api/ModelParametersSchema.java b/h2o-core/src/main/java/water/api/ModelParametersSchema.java
+index a9f7a07..db06251 100644
+--- a/h2o-core/src/main/java/water/api/ModelParametersSchema.java
++++ b/h2o-core/src/main/java/water/api/ModelParametersSchema.java
+@@ -133,6 +133,7 @@ public class ModelParametersSchema<P extends Model.Parameters, S extends ModelPa
+ 
+     impl._train = (null == this.  training_frame ? null : Key.<Frame>make(this.  training_frame.name));
+     impl._valid = (null == this.validation_frame ? null : Key.<Frame>make(this.validation_frame.name));
++    impl._max_runtime_secs = nfolds > 0 ? max_runtime_secs / (nfolds+1) : max_runtime_secs;
+ 
+     return impl;
+   }
+diff --git a/h2o-core/src/main/java/water/fvec/CStrChunk.java b/h2o-core/src/main/java/water/fvec/CStrChunk.java
+index 7c33427..6e3d7d7 100644
+--- a/h2o-core/src/main/java/water/fvec/CStrChunk.java
++++ b/h2o-core/src/main/java/water/fvec/CStrChunk.java
+@@ -150,6 +150,35 @@ public class CStrChunk extends Chunk {
+     }
+     return nc;
+   }
++  
++  /**
++   * Optimized substring() method for a buffer of only ASCII characters.
++   * The presence of UTF-8 multi-byte characters would give incorrect results
++   * for the string length, which is required here.
++   *
++   * @param nc NewChunk to be filled with substrings in this chunk
++   * @param startIndex The beginning index of the substring, inclusive
++   * @param endIndex The ending index of the substring, exclusive
++   * @return Filled NewChunk
++   */
++  public NewChunk asciiSubstring(NewChunk nc, int startIndex, int endIndex) {
++    // copy existing data
++    nc = this.inflate_impl(nc);
++    
++    //update offsets and byte array
++    for (int i = 0; i < _len; i++) {
++      int off = UnsafeUtils.get4(_mem, (i << 2) + _OFF);
++      if (off != NA) {
++        int len = 0;
++        while (_mem[_valstart + off + len] != 0) len++; //Find length
++        nc._is[i] = startIndex < len ? off + startIndex : off + len;
++        for (; len > endIndex - 1; len--) {
++          nc._ss[off + len] = 0; //Set new end
++        }
++      }
++    }
++    return nc;
++  }
+ 
+   /**
+    * Optimized length() method for a buffer of only ASCII characters.
+diff --git a/h2o-core/src/main/java/water/fvec/NewChunk.java b/h2o-core/src/main/java/water/fvec/NewChunk.java
+index e1b32fa..5360c41 100644
+--- a/h2o-core/src/main/java/water/fvec/NewChunk.java
++++ b/h2o-core/src/main/java/water/fvec/NewChunk.java
+@@ -63,7 +63,7 @@ public class NewChunk extends Chunk {
+   public int _timCnt = 0;
+   protected static final int MIN_SPARSE_RATIO = 32;
+   private int _sparseRatio = MIN_SPARSE_RATIO;
+-  public boolean _isAllASCII = false; //For cat/string col, are all characters in chunk ASCII? FIXME: this is never updated anywhere... setting to false
++  public boolean _isAllASCII = true; //For cat/string col, are all characters in chunk ASCII?
+ 
+   public NewChunk( Vec vec, int cidx ) { _vec = vec; _cidx = cidx; }
+ 
+diff --git a/h2o-core/src/main/java/water/rapids/AST.java b/h2o-core/src/main/java/water/rapids/AST.java
+index 785ed90..a988581 100644
+--- a/h2o-core/src/main/java/water/rapids/AST.java
++++ b/h2o-core/src/main/java/water/rapids/AST.java
+@@ -212,6 +212,7 @@ abstract public class AST extends Iced<AST> {
+     init(new ASTCountMatches());
+     init(new ASTToUpper());
+     init(new ASTStrLength());
++    init(new ASTSubstring());
+ 
+     // Functional data mungers
+     init(new ASTApply());
+diff --git a/h2o-core/src/main/java/water/rapids/ASTStrOp.java b/h2o-core/src/main/java/water/rapids/ASTStrOp.java
+index 0a07ee0..c75b111 100644
+--- a/h2o-core/src/main/java/water/rapids/ASTStrOp.java
++++ b/h2o-core/src/main/java/water/rapids/ASTStrOp.java
+@@ -5,11 +5,9 @@ import water.MRTask;
+ import water.MemoryManager;
+ import water.fvec.*;
+ import water.parser.BufferedString;
++import water.util.VecUtils;
+ 
+-import java.util.ArrayList;
+-import java.util.Arrays;
+-import java.util.HashSet;
+-import java.util.Locale;
++import java.util.*;
+ 
+ public class ASTStrOp { /*empty*/}
+ 
+@@ -280,7 +278,7 @@ class ASTToLower extends ASTPrim {
+   }
+ 
+   private Vec toLowerCategoricalCol(Vec vec) {
+-    String[] dom = vec.domain();
++    String[] dom = vec.domain().clone();
+     for (int i = 0; i < dom.length; ++i)
+       dom[i] = dom[i].toLowerCase(Locale.ENGLISH);
+ 
+@@ -344,7 +342,7 @@ class ASTToUpper extends ASTPrim {
+   }
+ 
+   private Vec toUpperCategoricalCol(Vec vec) {
+-    String[] dom = vec.domain();
++    String[] dom = vec.domain().clone();
+     for (int i = 0; i < dom.length; ++i)
+       dom[i] = dom[i].toUpperCase(Locale.ENGLISH);
+ 
+@@ -495,7 +493,7 @@ class ASTReplaceAll extends ASTPrim {
+   }
+ 
+   private Vec replaceAllCategoricalCol(Vec vec, String pattern, String replacement, boolean ignoreCase) {
+-    String[] doms = vec.domain();
++    String[] doms = vec.domain().clone();
+     for (int i = 0; i < doms.length; ++i)
+       doms[i] = ignoreCase
+           ? doms[i].toLowerCase(Locale.ENGLISH).replaceAll(pattern, replacement)
+@@ -567,12 +565,28 @@ class ASTTrim extends ASTPrim {
+     return new ValFrame(new Frame(nvs));
+   }
+ 
+-  // FIXME: this should resolve any categoricals that now have the same value after the trim
+   private Vec trimCategoricalCol(Vec vec) {
+-    String[] doms = vec.domain();
+-    for (int i = 0; i < doms.length; ++i) doms[i] = doms[i].trim();
+-    Vec v = vec.makeCopy(doms);
+-    return v;
++    String[] doms = vec.domain().clone();
++    
++    HashMap<String, ArrayList<Integer>> trimmedToOldDomainIndices = new HashMap<>();
++    String trimmed;
++    for (int i = 0; i < doms.length; ++i) {
++      trimmed = doms[i].trim();
++      doms[i] = trimmed;
++      
++      if(!trimmedToOldDomainIndices.containsKey(trimmed)) {
++        ArrayList<Integer> val = new ArrayList<>();
++        val.add(i);
++        trimmedToOldDomainIndices.put(trimmed, val);
++      } else {
++        trimmedToOldDomainIndices.get(trimmed).add(i);
++      }
++    }
++    //Check for duplicated domains
++    if (trimmedToOldDomainIndices.size() < doms.length)
++      return VecUtils.DomainDedupe.domainDeduper(vec, trimmedToOldDomainIndices);
++    
++    return vec.makeCopy(doms);
+   }
+ 
+   private Vec trimStringCol(Vec vec) {
+@@ -622,9 +636,9 @@ class ASTStrLength extends ASTPrim {
+   }
+ 
+   private Vec lengthCategoricalCol(Vec vec) {
+-    String[] doms = vec.domain();
+-    int[] catLengths = new int[doms.length];
+-    for (int i = 0; i < doms.length; ++i) catLengths[i] = doms[i].length();
++    //String[] doms = vec.domain();
++    //int[] catLengths = new int[doms.length];
++    //for (int i = 0; i < doms.length; ++i) catLengths[i] = doms[i].length();
+     Vec res = new MRTask() {
+         transient int[] catLengths;
+         @Override public void setupLocal() {
+@@ -665,3 +679,95 @@ class ASTStrLength extends ASTPrim {
+     }.doAll(new byte[]{Vec.T_NUM}, vec).outputFrame().anyVec();
+   }
+ }
++
++class ASTSubstring extends ASTPrim {
++  @Override public String[] args() { return new String[]{"ary", "startIndex", "endIndex"}; }
++  @Override int nargs() {return 4; } // (substring x startIndex [endIndex])
++  @Override public String str() { return "substring"; }
++  @Override ValFrame apply( Env env, Env.StackHelp stk, AST asts[] ) {
++    Frame fr = stk.track(asts[1].exec(env)).getFrame();
++    int startIndex = (int) asts[2].exec(env).getNum();
++    if (startIndex < 0) startIndex = 0;
++    int endIndex = asts[3] instanceof ASTNumList ? Integer.MAX_VALUE : (int) asts[3].exec(env).getNum();
++    // Type check
++    for (Vec v : fr.vecs())
++      if (!(v.isCategorical() || v.isString()))
++        throw new IllegalArgumentException("substring() requires a string or categorical column. "
++                +"Received "+fr.anyVec().get_type_str()
++                +". Please convert column to a string or categorical first.");
++    
++    // Transform each vec
++    Vec nvs[] = new Vec[fr.numCols()];
++    int i = 0;
++    for (Vec v: fr.vecs()) {
++      if (v.isCategorical())
++        nvs[i] = substringCategoricalCol(v, startIndex, endIndex);
++      else
++        nvs[i] = substringStringCol(v, startIndex, endIndex);
++      i++;
++    }
++    
++    return new ValFrame(new Frame(nvs));
++  }
++
++  private Vec substringCategoricalCol(Vec vec, int startIndex, int endIndex) {
++    if (startIndex >= endIndex) {
++      Vec v = Vec.makeZero(vec.length());
++      v.setDomain(new String[]{""});
++      return v;
++    }
++    String[] dom = vec.domain().clone();
++    
++    HashMap<String, ArrayList<Integer>> substringToOldDomainIndices = new HashMap<>();
++    String substr;
++    for (int i = 0; i < dom.length; i++) {
++      substr = dom[i].substring(startIndex < dom[i].length() ? startIndex : dom[i].length(),
++              endIndex < dom[i].length() ? endIndex : dom[i].length());
++      dom[i] = substr;
++
++      if (!substringToOldDomainIndices.containsKey(substr)) {
++        ArrayList<Integer> val = new ArrayList<>();
++        val.add(i);
++        substringToOldDomainIndices.put(substr, val);
++      } else {
++        substringToOldDomainIndices.get(substr).add(i);
++      }
++    }
++    //Check for duplicated domains
++    if (substringToOldDomainIndices.size() < dom.length)
++      return VecUtils.DomainDedupe.domainDeduper(vec, substringToOldDomainIndices);
++    
++    return vec.makeCopy(dom);
++  }
++  
++  private Vec substringStringCol(Vec vec, final int startIndex, final int endIndex) {
++    return new MRTask() {
++      @Override
++      public void map(Chunk chk, NewChunk newChk) {
++        if (chk instanceof C0DChunk) // all NAs
++          for (int i = 0; i < chk.len(); i++)
++            newChk.addNA();
++        else if (startIndex >= endIndex) {
++          for (int i = 0; i < chk.len(); i++)
++            newChk.addStr("");
++        }
++        else if (((CStrChunk) chk)._isAllASCII) { // fast-path operations
++          ((CStrChunk) chk).asciiSubstring(newChk, startIndex, endIndex);
++        } 
++        else { //UTF requires Java string methods
++          BufferedString tmpStr = new BufferedString();
++          for (int i = 0; i < chk._len; i++) {
++            if (chk.isNA(i))
++              newChk.addNA();
++            else {
++              String str = chk.atStr(tmpStr, i).toString();
++              newChk.addStr(str.substring(startIndex < str.length() ? startIndex : str.length(), 
++                      endIndex < str.length() ? endIndex : str.length()));
++            }
++          }
++        }
++      }
++    }.doAll(new byte[]{Vec.T_STR}, vec).outputFrame().anyVec();
++  }
++  
++}
+diff --git a/h2o-core/src/main/java/water/util/VecUtils.java b/h2o-core/src/main/java/water/util/VecUtils.java
+index 608f8c1..3fe46fb 100644
+--- a/h2o-core/src/main/java/water/util/VecUtils.java
++++ b/h2o-core/src/main/java/water/util/VecUtils.java
+@@ -15,7 +15,7 @@ import water.nbhm.NonBlockingHashMapLong;
+ import water.parser.BufferedString;
+ import water.parser.Categorical;
+ 
+-import java.util.Arrays;
++import java.util.*;
+ 
+ public class VecUtils {
+   /**
+@@ -430,6 +430,44 @@ public class VecUtils {
+     }
+   }
+ 
++  /**
++   * Create a new categorical {@link Vec} with deduplicated domains from a categorical {@link Vec}.
++   * 
++   * Categoricals may have the same values after munging, and should have the same domain index in the numerical chunk 
++   * representation. Unify categoricals that are the same by remapping their domain indices. 
++   * 
++   * Could be more efficient with a vec copy and replace domain indices as needed. PUBDEV-2587
++   */
++
++  public static class DomainDedupe extends MRTask<DomainDedupe> {
++    private final HashMap<Integer, Integer> _oldToNewDomainIndex;
++    public DomainDedupe(HashMap<Integer, Integer> oldToNewDomainIndex) {_oldToNewDomainIndex = oldToNewDomainIndex; }
++    @Override public void map(Chunk c, NewChunk nc) {
++      for( int row=0; row < c._len; row++) {
++        if ( !c.isNA(row) ) {
++          int oldDomain = (int) c.at8(row);
++          nc.addNum(_oldToNewDomainIndex.get(oldDomain));
++        } else {
++          nc.addNA();
++        }
++      }
++    }
++    public static Vec domainDeduper(Vec vec, HashMap<String, ArrayList<Integer>> substringToOldDomainIndices) {
++      HashMap<Integer, Integer> oldToNewDomainIndex = new HashMap<>();
++      int newDomainIndex = 0;
++      SortedSet<String> alphabetizedSubstrings = new TreeSet<>(substringToOldDomainIndices.keySet());
++      for (String sub : alphabetizedSubstrings) {
++        for (int oldDomainIndex : substringToOldDomainIndices.get(sub)) {
++          oldToNewDomainIndex.put(oldDomainIndex, newDomainIndex);
++        }
++        newDomainIndex++;
++      }
++      VecUtils.DomainDedupe domainDedupe = new VecUtils.DomainDedupe(oldToNewDomainIndex);
++      String[][] dom2D = {Arrays.copyOf(alphabetizedSubstrings.toArray(), alphabetizedSubstrings.size(), String[].class)};
++      return domainDedupe.doAll(new byte[]{Vec.T_CAT}, vec).outputFrame(null, null, dom2D).anyVec();
++    }
++  }
++
+   // >11x faster than CollectDomain
+   /** (Optimized for positive ints) Collect numeric domain of given {@link Vec}
+    *  A map-reduce task to collect up the unique values of an integer {@link Vec}
+@@ -455,7 +493,7 @@ public class VecUtils {
+       for (int i = 0; i < _u.length;++i)
+         if (_u[i])
+           _d[id++]=i;
+-      Arrays.sort(_d);
++      Arrays.sort(_d); //is this necessary? 
+     }
+ 
+     /** Returns exact numeric domain of given {@link Vec} computed by this task.
+diff --git a/h2o-docs/src/product/architecture/Architecture.md b/h2o-docs/src/product/architecture/Architecture.md
+index be95abf..d284432 100644
+--- a/h2o-docs/src/product/architecture/Architecture.md
++++ b/h2o-docs/src/product/architecture/Architecture.md
+@@ -16,7 +16,7 @@ JVM process.
+ The color scheme in the diagram shows each layer in a consistent color
+ but always shows user-added customer algorithm code as gray.
+ 
+-![H2O stack](images/h2o_stack.pdf)
++![H2O stack](images/h2o_stack.png)
+ 
+ 
+ ### REST API Clients
+@@ -110,13 +110,13 @@ The following sequence of three steps shows how an R program tells an H2O cluste
+ 
+ #### Step 1: The R user calls the importFile function
+ 
+-![](images/r_hdfs_read_step1.pdf)
++![](images/r_hdfs_read_step1.png)
+ 
+ #### Step 2: The R client tells the cluster to read the data
+ 
+ The thin arrows show control information.
+ 
+-![](images/r_hdfs_read_step2.pdf)
++![](images/r_hdfs_read_step2.png)
+ 
+ #### Step 3: The data is returned from HDFS into a distributed H2O Frame
+ 
+@@ -124,7 +124,7 @@ The thin arrows show control information.
+ The thick arrows show data being returned from HDFS.
+ The blocks of data live in the distributed H2O Frame cluster memory.
+ 
+-![](images/r_hdfs_read_step3.pdf)
++![](images/r_hdfs_read_step3.png)
+ 
+ 
+ ### How R Scripts Call H2O GLM
+@@ -147,14 +147,14 @@ In the R program, the different components are:
+ * dependent packages (RCurl, rjson, etc.)
+ * the R core runtime
+ 
+-![](images/start_glm_from_r.pdf)
++![](images/start_glm_from_r.png)
+ 
+ The following diagram shows the R program retrieving the resulting GLM
+ model.  (Not shown: the GLM model executing subtasks within
+ H2O and depositing the result into the K/V store or R
+ polling the /3/Jobs URL for the GLM model to complete.)
+ 
+-![](images/retrieve_glm_result_from_r.pdf)
++![](images/retrieve_glm_result_from_r.png)
+ 
+ An end-to-end sequence diagram of the same transaction is below.
+ This gives a different perspective of the R and H2O interactions for the same 
+diff --git a/h2o-docs/src/product/architecture/images/h2o_stack.pdf b/h2o-docs/src/product/architecture/images/h2o_stack.pdf
+deleted file mode 100644
+index 92d082a..0000000
+Binary files a/h2o-docs/src/product/architecture/images/h2o_stack.pdf and /dev/null differ
+diff --git a/h2o-docs/src/product/architecture/images/h2o_stack.png b/h2o-docs/src/product/architecture/images/h2o_stack.png
+new file mode 100644
+index 0000000..3b7dac9
+Binary files /dev/null and b/h2o-docs/src/product/architecture/images/h2o_stack.png differ
+diff --git a/h2o-docs/src/product/architecture/images/r_hdfs_read_step1.pdf b/h2o-docs/src/product/architecture/images/r_hdfs_read_step1.pdf
+deleted file mode 100644
+index 636ade0..0000000
+Binary files a/h2o-docs/src/product/architecture/images/r_hdfs_read_step1.pdf and /dev/null differ
+diff --git a/h2o-docs/src/product/architecture/images/r_hdfs_read_step1.png b/h2o-docs/src/product/architecture/images/r_hdfs_read_step1.png
+new file mode 100644
+index 0000000..d26e4fc
+Binary files /dev/null and b/h2o-docs/src/product/architecture/images/r_hdfs_read_step1.png differ
+diff --git a/h2o-docs/src/product/architecture/images/r_hdfs_read_step2.pdf b/h2o-docs/src/product/architecture/images/r_hdfs_read_step2.pdf
+deleted file mode 100644
+index 205cd27..0000000
+Binary files a/h2o-docs/src/product/architecture/images/r_hdfs_read_step2.pdf and /dev/null differ
+diff --git a/h2o-docs/src/product/architecture/images/r_hdfs_read_step2.png b/h2o-docs/src/product/architecture/images/r_hdfs_read_step2.png
+new file mode 100644
+index 0000000..e8024a7
+Binary files /dev/null and b/h2o-docs/src/product/architecture/images/r_hdfs_read_step2.png differ
+diff --git a/h2o-docs/src/product/architecture/images/r_hdfs_read_step3.pdf b/h2o-docs/src/product/architecture/images/r_hdfs_read_step3.pdf
+deleted file mode 100644
+index 0675121..0000000
+Binary files a/h2o-docs/src/product/architecture/images/r_hdfs_read_step3.pdf and /dev/null differ
+diff --git a/h2o-docs/src/product/architecture/images/r_hdfs_read_step3.png b/h2o-docs/src/product/architecture/images/r_hdfs_read_step3.png
+new file mode 100644
+index 0000000..62d4e47
+Binary files /dev/null and b/h2o-docs/src/product/architecture/images/r_hdfs_read_step3.png differ
+diff --git a/h2o-docs/src/product/architecture/images/retrieve_glm_result_from_r.pdf b/h2o-docs/src/product/architecture/images/retrieve_glm_result_from_r.pdf
+deleted file mode 100644
+index 2cc62cc..0000000
+Binary files a/h2o-docs/src/product/architecture/images/retrieve_glm_result_from_r.pdf and /dev/null differ
+diff --git a/h2o-docs/src/product/architecture/images/retrieve_glm_result_from_r.png b/h2o-docs/src/product/architecture/images/retrieve_glm_result_from_r.png
+new file mode 100644
+index 0000000..8049f7a
+Binary files /dev/null and b/h2o-docs/src/product/architecture/images/retrieve_glm_result_from_r.png differ
+diff --git a/h2o-docs/src/product/architecture/images/start_glm_from_r.pdf b/h2o-docs/src/product/architecture/images/start_glm_from_r.pdf
+deleted file mode 100644
+index a6c7ef9..0000000
+Binary files a/h2o-docs/src/product/architecture/images/start_glm_from_r.pdf and /dev/null differ
+diff --git a/h2o-docs/src/product/architecture/images/start_glm_from_r.png b/h2o-docs/src/product/architecture/images/start_glm_from_r.png
+new file mode 100644
+index 0000000..9c15df9
+Binary files /dev/null and b/h2o-docs/src/product/architecture/images/start_glm_from_r.png differ
+diff --git a/h2o-py/h2o/estimators/estimator_base.py b/h2o-py/h2o/estimators/estimator_base.py
+index dc367e1..e41d174 100644
+--- a/h2o-py/h2o/estimators/estimator_base.py
++++ b/h2o-py/h2o/estimators/estimator_base.py
+@@ -96,7 +96,6 @@ class H2OEstimator(ModelBase):
+       H2OFrame with validation data to be scored on while training.
+     max_runtime_secs : float
+       Maximum allowed runtime in seconds for model training. Use 0 to disable.
+-      For cross-validation and grid searches, this time limit applies to all sub-models.
+     """
+     algo_params = locals()
+     parms = self._parms.copy()
+diff --git a/h2o-py/h2o/frame.py b/h2o-py/h2o/frame.py
+index 9e28040..92cf475 100644
+--- a/h2o-py/h2o/frame.py
++++ b/h2o-py/h2o/frame.py
+@@ -588,7 +588,7 @@ class H2OFrame(object):
+     lol = H2OFrame._expr(expr=ExprNode("levels", self)).as_data_frame(False)
+     lol.pop(0)  # Remove column headers
+     lol = list(zip(*lol))
+-    lol = [[ll for ll in l if ll!=''] for l in lol]
++    lol = [[ll for ll in l] for l in lol]
+     
+     return lol
+ 
+@@ -1602,6 +1602,28 @@ class H2OFrame(object):
+     fr._ex._cache.nrows = self.nrow
+     fr._ex._cache.ncol = self.ncol
+     return fr
++  
++  def substring(self, start_index, end_index=None):
++    """For each string, return a new string that is a substring of the original string. If end_index is not 
++    specified, then the substring extends to the end of the original string. If the start_index is longer than
++    the length of the string, or is greater than or equal to the end_index, an empty string is returned. Negative
++    start_index is coerced to 0. 
++
++    Parameters
++    ----------
++    start_index : int
++      The index of the original string at which to start the substring, inclusive.
++    end_index: int, optional
++      The index of the original string at which to end the substring, exclusive. 
++
++    Returns
++    -------
++      An H2OFrame containing the specified substrings.
++    """
++    fr = H2OFrame._expr(expr=ExprNode("substring", self, start_index, end_index)) 
++    fr._ex._cache.nrows = self.nrow
++    fr._ex._cache.ncol = self.ncol
++    return fr
+ 
+   def nchar(self):
+     """Count the number of characters in each string of single-column H2OFrame.
+diff --git a/h2o-py/tests/testdir_algos/deeplearning/pyunit_DEPRECATED_anomaly_largeDeepLearning.py b/h2o-py/tests/testdir_algos/deeplearning/pyunit_DEPRECATED_anomaly_largeDeepLearning.py
+deleted file mode 100644
+index 1afc682..0000000
+--- a/h2o-py/tests/testdir_algos/deeplearning/pyunit_DEPRECATED_anomaly_largeDeepLearning.py
++++ /dev/null
+@@ -1,57 +0,0 @@
+-from __future__ import print_function
+-from builtins import range
+-import sys, os
+-sys.path.insert(1, os.path.join("..","..",".."))
+-import h2o
+-from tests import pyunit_utils
+-
+-
+-def anomaly():
+-    
+-
+-    print("Deep Learning Anomaly Detection MNIST")
+-
+-    train = h2o.import_file(pyunit_utils.locate("bigdata/laptop/mnist/train.csv.gz"))
+-    test = h2o.import_file(pyunit_utils.locate("bigdata/laptop/mnist/test.csv.gz"))
+-
+-    predictors = list(range(0,784))
+-    resp = 784
+-
+-    # unsupervised -> drop the response column (digit: 0-9)
+-    train = train[predictors]
+-    test = test[predictors]
+-
+-    # 1) LEARN WHAT'S NORMAL
+-    # train unsupervised Deep Learning autoencoder model on train_hex
+-
+-    ae_model = h2o.deeplearning(x=train[predictors],
+-                                       autoencoder=True,
+-                                       activation="Tanh",
+-                                       hidden=[2],
+-                                       l1=1e-5,
+-                                       ignore_const_cols=False,
+-                                       epochs=1
+-                                       )
+-
+-    ae_model.anomaly(test).show()
+-
+-    # 2) DETECT OUTLIERS
+-    # anomaly app computes the per-row reconstruction error for the test data set
+-    # (passing it through the autoencoder model and computing mean square error (MSE) for each row)
+-    test_rec_error = ae_model.anomaly(test)
+-
+-    # 3) VISUALIZE OUTLIERS
+-    # Let's look at the test set points with low/median/high reconstruction errors.
+-    # We will now visualize the original test set points and their reconstructions obtained
+-    # by propagating them through the narrow neural net.
+-
+-    # Convert the test data into its autoencoded representation (pass through narrow neural net)
+-    test_recon = ae_model.predict(test)
+-
+-    # In python, the visualization could be done with tools like numpy/matplotlib or numpy/PIL
+-
+-
+-if __name__ == "__main__":
+-    pyunit_utils.standalone_test(anomaly)
+-else:
+-    anomaly()
+diff --git a/h2o-py/tests/testdir_algos/deeplearning/pyunit_DEPRECATED_autoencoderDeepLearning_large.py b/h2o-py/tests/testdir_algos/deeplearning/pyunit_DEPRECATED_autoencoderDeepLearning_large.py
+deleted file mode 100644
+index e71f534..0000000
+--- a/h2o-py/tests/testdir_algos/deeplearning/pyunit_DEPRECATED_autoencoderDeepLearning_large.py
++++ /dev/null
+@@ -1,69 +0,0 @@
+-import sys, os
+-sys.path.insert(1, os.path.join("..","..",".."))
+-import h2o
+-from tests import pyunit_utils
+-
+-
+-def deeplearning_autoencoder():
+-
+-
+-    resp = 784
+-    nfeatures = 20 # number of features (smallest hidden layer)
+-
+-    train_hex = h2o.upload_file(pyunit_utils.locate("bigdata/laptop/mnist/train.csv.gz"))
+-    train_hex[resp] = train_hex[resp].asfactor()
+-
+-    test_hex = h2o.upload_file(pyunit_utils.locate("bigdata/laptop/mnist/test.csv.gz"))
+-    test_hex[resp] = test_hex[resp].asfactor()
+-
+-    # split data into two parts
+-    sid = train_hex[0].runif(0)
+-
+-    # unsupervised data for autoencoder
+-    train_unsupervised = train_hex[sid >= 0.5]
+-    train_unsupervised.pop(resp)
+-    #train_unsupervised.describe()
+-
+-    # supervised data for drf
+-    train_supervised = train_hex[sid < 0.5]
+-    #train_supervised.describe()
+-
+-    # train autoencoder
+-    ae_model = h2o.deeplearning(x=train_unsupervised[0:resp],
+-                                activation="Tanh",
+-                                autoencoder=True,
+-                                hidden=[nfeatures],
+-                                epochs=1,
+-                                reproducible=True, #slow, turn off for real problems
+-                                seed=1234)
+-
+-    # convert train_supervised with autoencoder to lower-dimensional space
+-    train_supervised_features = ae_model.deepfeatures(train_supervised[0:resp], 0)
+-
+-    assert train_supervised_features.ncol == nfeatures, "Dimensionality of reconstruction is wrong!"
+-
+-    train_supervised_features = train_supervised_features.cbind(train_supervised[resp])
+-    # Train DRF on extracted feature space
+-    drf_model = h2o.random_forest(x=train_supervised_features[0:nfeatures],
+-                                  y=train_supervised_features[train_supervised_features.ncol-1],
+-                                  ntrees=10,
+-                                  min_rows=10,
+-                                  seed=1234)
+-
+-    # Test the DRF model on the test set (processed through deep features)
+-    test_features = ae_model.deepfeatures(test_hex[0:resp], 0)
+-    test_features = test_features.cbind(test_hex[resp])
+-
+-    # Confusion Matrix and assertion
+-    cm = drf_model.confusion_matrix(test_features)
+-    cm.show()
+-
+-    # 10% error +/- 0.001
+-    assert abs(cm.cell_values[10][10] - 0.0882) < 0.001, "Error. Expected 0.0882, but got {0}".format(cm.cell_values[10][10])
+-
+-if __name__ == "__main__":
+-  pyunit_utils.standalone_test(deeplearning_autoencoder)
+-else:
+-  deeplearning_autoencoder()
+-
+-
+diff --git a/h2o-py/tests/testdir_algos/deeplearning/pyunit_DEPRECATED_cv_cars_mediumDeepLearning.py b/h2o-py/tests/testdir_algos/deeplearning/pyunit_DEPRECATED_cv_cars_mediumDeepLearning.py
+deleted file mode 100644
+index 0cd8f15..0000000
+--- a/h2o-py/tests/testdir_algos/deeplearning/pyunit_DEPRECATED_cv_cars_mediumDeepLearning.py
++++ /dev/null
+@@ -1,119 +0,0 @@
+-from __future__ import print_function
+-from builtins import zip
+-from builtins import range
+-import sys, os
+-sys.path.insert(1, os.path.join("..","..",".."))
+-import h2o
+-from tests import pyunit_utils
+-import random
+-
+-
+-def cv_carsDL():
+-
+-    # read in the dataset and construct training set (and validation set)
+-    cars =  h2o.import_file(path=pyunit_utils.locate("smalldata/junit/cars_20mpg.csv"))
+-
+-    # choose the type model-building exercise (multinomial classification or regression). 0:regression, 1:binomial,
+-    # 2:multinomial
+-    problem = random.sample(list(range(3)),1)[0]
+-
+-    # pick the predictors and the correct response column
+-    predictors = ["displacement","power","weight","acceleration","year"]
+-    if problem == 1   :
+-        response_col = "economy_20mpg"
+-        cars[response_col] = cars[response_col].asfactor()
+-    elif problem == 2 :
+-        response_col = "cylinders"
+-        cars[response_col] = cars[response_col].asfactor()
+-    else              :
+-        response_col = "economy"
+-
+-    print("Response column: {0}".format(response_col))
+-
+-    ## cross-validation
+-    # 1. basic
+-    dl = h2o.deeplearning(y=cars[response_col], x=cars[predictors], nfolds=random.randint(3,10), fold_assignment="Modulo")
+-
+-    # 2. check that cv metrics are different over repeated "Random" runs
+-    nfolds = random.randint(3,10)
+-    dl1 = h2o.deeplearning(y=cars[response_col], x=cars[predictors], nfolds=nfolds, fold_assignment="Random")
+-    dl2 = h2o.deeplearning(y=cars[response_col], x=cars[predictors], nfolds=nfolds, fold_assignment="Random")
+-    try:
+-        pyunit_utils.check_models(dl1, dl2, True)
+-        assert False, "Expected models to be different over repeated Random runs"
+-    except AssertionError:
+-        assert True
+-
+-    # 3. folds_column
+-    num_folds = random.randint(2,5)
+-    fold_assignments = h2o.H2OFrame([[random.randint(0,num_folds-1)] for _ in range(cars.nrow)])
+-    fold_assignments.set_names(["fold_assignments"])
+-    cars = cars.cbind(fold_assignments)
+-    dl = h2o.deeplearning(y=cars[response_col], x=cars[predictors], training_frame=cars,
+-                          fold_column="fold_assignments", keep_cross_validation_predictions=True)
+-    num_cv_models = len(dl._model_json['output']['cross_validation_models'])
+-    assert num_cv_models==num_folds, "Expected {0} cross-validation models, but got " \
+-                                     "{1}".format(num_folds, num_cv_models)
+-    cv_model1 = h2o.get_model(dl._model_json['output']['cross_validation_models'][0]['name'])
+-    cv_model2 = h2o.get_model(dl._model_json['output']['cross_validation_models'][1]['name'])
+-    assert isinstance(cv_model1, type(dl)), "Expected cross-validation model to be the same model type as the " \
+-                                            "constructed model, but got {0} and {1}".format(type(cv_model1),type(dl))
+-    assert isinstance(cv_model2, type(dl)), "Expected cross-validation model to be the same model type as the " \
+-                                            "constructed model, but got {0} and {1}".format(type(cv_model2),type(dl))
+-
+-    # 4. keep_cross_validation_predictions
+-    cv_predictions = dl1._model_json['output']['cross_validation_predictions']
+-    assert cv_predictions is None, "Expected cross-validation predictions to be None, but got {0}".format(cv_predictions)
+-
+-    cv_predictions = dl._model_json['output']['cross_validation_predictions']
+-    assert len(cv_predictions)==num_folds, "Expected the same number of cross-validation predictions " \
+-                                           "as folds, but got {0}".format(len(cv_predictions))
+-
+-
+-    ## boundary cases
+-    # 1. nfolds = number of observations (leave-one-out cross-validation)
+-    dl = h2o.deeplearning(y=cars[response_col], x=cars[predictors], nfolds=cars.nrow, fold_assignment="Modulo")
+-
+-    # 2. nfolds = 0
+-    dl = h2o.deeplearning(y=cars[response_col], x=cars[predictors], nfolds=0)
+-
+-    # 3. cross-validation and regular validation attempted
+-    dl = h2o.deeplearning(y=cars[response_col], x=cars[predictors], nfolds=random.randint(3,10),
+-                           validation_y=cars[response_col], validation_x=cars[predictors])
+-
+-
+-    ## error cases
+-    # 1. nfolds == 1 or < 0
+-    try:
+-        dl = h2o.deeplearning(y=cars[response_col], x=cars[predictors], nfolds=random.sample([-1,1], 1)[0])
+-        assert False, "Expected model-build to fail when nfolds is 1 or < 0"
+-    except EnvironmentError:
+-        assert True
+-
+-    # 2. more folds than observations
+-    try:
+-        dl = h2o.deeplearning(y=cars[response_col], x=cars[predictors], nfolds=cars.nrow+1, fold_assignment="Modulo")
+-        assert False, "Expected model-build to fail when nfolds > nobs"
+-    except EnvironmentError:
+-        assert True
+-
+-    # 3. fold_column and nfolds both specified
+-    try:
+-        rf = h2o.deeplearning(y=cars[response_col], x=cars[predictors], nfolds=3, fold_column="fold_assignments",
+-                              training_frame=cars)
+-        assert False, "Expected model-build to fail when fold_column and nfolds both specified"
+-    except EnvironmentError:
+-        assert True
+-
+-    # # 4. fold_column and fold_assignment both specified
+-    # try:
+-    #     rf = h2o.deeplearning(y=cars[response_col], x=cars[predictors], fold_assignment="Random",
+-    #                           fold_column="fold_assignments", training_frame=cars)
+-    #     assert False, "Expected model-build to fail when fold_column and fold_assignment both specified"
+-    # except EnvironmentError:
+-    #     assert True
+-
+-if __name__ == "__main__":
+-  pyunit_utils.standalone_test(cv_carsDL)
+-else:
+-  cv_carsDL()
+\ No newline at end of file
+diff --git a/h2o-py/tests/testdir_algos/deeplearning/pyunit_DEPRECATED_imbalance_largeDeepLearning.py b/h2o-py/tests/testdir_algos/deeplearning/pyunit_DEPRECATED_imbalance_largeDeepLearning.py
+deleted file mode 100644
+index 78e4e07..0000000
+--- a/h2o-py/tests/testdir_algos/deeplearning/pyunit_DEPRECATED_imbalance_largeDeepLearning.py
++++ /dev/null
+@@ -1,50 +0,0 @@
+-from __future__ import print_function
+-import sys
+-sys.path.insert(1,"../../../")
+-import h2o
+-from tests import pyunit_utils
+-
+-
+-def imbalance():
+-    
+-
+-    print("Test checks if Deep Learning works fine with an imbalanced dataset")
+-
+-    covtype = h2o.upload_file(pyunit_utils.locate("smalldata/covtype/covtype.20k.data"))
+-    covtype[54] = covtype[54].asfactor()
+-    hh_imbalanced = h2o.deeplearning(x=covtype[0:54], y=covtype[54], l1=1e-5, activation="Rectifier", loss="CrossEntropy",
+-                                     hidden=[200,200], epochs=1, training_frame=covtype, balance_classes=False,
+-                                     reproducible=True, seed=1234)
+-    print(hh_imbalanced)
+-
+-    hh_balanced = h2o.deeplearning(x=covtype[0:54], y=covtype[54], l1=1e-5, activation="Rectifier", loss="CrossEntropy",
+-                                   hidden=[200,200], epochs=1, training_frame=covtype, balance_classes=True,
+-                                   reproducible=True, seed=1234)
+-    print(hh_balanced)
+-
+-    #compare overall logloss
+-    class_6_err_imbalanced = hh_imbalanced.logloss()
+-    class_6_err_balanced = hh_balanced.logloss()
+-
+-    if class_6_err_imbalanced < class_6_err_balanced:
+-        print("--------------------")
+-        print("")
+-        print("FAIL, balanced error greater than imbalanced error")
+-        print("")
+-        print("")
+-        print("class_6_err_imbalanced")
+-        print(class_6_err_imbalanced)
+-        print("")
+-        print("class_6_err_balanced")
+-        print(class_6_err_balanced)
+-        print("")
+-        print("--------------------")
+-
+-    assert class_6_err_imbalanced >= class_6_err_balanced, "balance_classes makes it worse!"
+-
+-
+-
+-if __name__ == "__main__":
+-    pyunit_utils.standalone_test(imbalance)
+-else:
+-    imbalance()
+diff --git a/h2o-py/tests/testdir_algos/gbm/pyunit_DEPRECATED_bernoulli_synthetic_data_mediumGBM.py b/h2o-py/tests/testdir_algos/gbm/pyunit_DEPRECATED_bernoulli_synthetic_data_mediumGBM.py
+deleted file mode 100644
+index 00648ab..0000000
+--- a/h2o-py/tests/testdir_algos/gbm/pyunit_DEPRECATED_bernoulli_synthetic_data_mediumGBM.py
++++ /dev/null
+@@ -1,70 +0,0 @@
+-from builtins import zip
+-import sys, os
+-sys.path.insert(1, os.path.join("..","..",".."))
+-import h2o
+-from tests import pyunit_utils
+-from h2o import H2OFrame
+-import numpy as np
+-import scipy.stats
+-from sklearn import ensemble
+-from sklearn.metrics import roc_auc_score
+-
+-
+-def bernoulli_synthetic_data_gbm_medium():
+-
+-  # Generate training dataset (adaptation of http://www.stat.missouri.edu/~speckman/stat461/boost.R)
+-  train_rows = 10000
+-  train_cols = 10
+-
+-  #  Generate variables V1, ... V10
+-  X_train = np.random.randn(train_rows, train_cols)
+-
+-  #  y = +1 if sum_i x_{ij}^2 > chisq median on 10 df
+-  y_train = np.asarray([1 if rs > scipy.stats.chi2.ppf(0.5, 10) else -1 for rs in [sum(r) for r in
+-                                                                                   np.multiply(X_train,X_train).tolist()]])
+-
+-  # Train scikit gbm
+-  # TODO: grid-search
+-  distribution = "bernoulli"
+-  ntrees = 150
+-  min_rows = 1
+-  max_depth = 2
+-  learn_rate = .01
+-  nbins = 20
+-
+-  gbm_sci = ensemble.GradientBoostingClassifier(learning_rate=learn_rate, n_estimators=ntrees, max_depth=max_depth,
+-                                                min_samples_leaf=min_rows, max_features=None)
+-  gbm_sci.fit(X_train,y_train)
+-
+-  # Generate testing dataset
+-  test_rows = 2000
+-  test_cols = 10
+-
+-  #  Generate variables V1, ... V10
+-  X_test = np.random.randn(test_rows, test_cols)
+-
+-  #  y = +1 if sum_i x_{ij}^2 > chisq median on 10 df
+-  y_test = np.asarray([1 if rs > scipy.stats.chi2.ppf(0.5, 10) else -1 for rs in [sum(r) for r in
+-                                                                                  np.multiply(X_test,X_test).tolist()]])
+-
+-  # Score (AUC) the scikit gbm model on the test data
+-  auc_sci = roc_auc_score(y_test, gbm_sci.predict_proba(X_test)[:,1])
+-
+-  # Compare this result to H2O
+-  train_h2o = H2OFrame(np.column_stack((y_train, X_train)).tolist())
+-  test_h2o = H2OFrame(np.column_stack((y_test, X_test)).tolist())
+-
+-  gbm_h2o = h2o.gbm(x=train_h2o[1:], y=train_h2o["C1"].asfactor(), distribution=distribution, ntrees=ntrees,
+-                    min_rows=min_rows, max_depth=max_depth, learn_rate=learn_rate, nbins=nbins)
+-  gbm_perf = gbm_h2o.model_performance(test_h2o)
+-  auc_h2o = gbm_perf.auc()
+-
+-  #Log.info(paste("scikit AUC:", auc_sci, "\tH2O AUC:", auc_h2o))
+-  assert abs(auc_h2o - auc_sci) < 1e-2, "h2o (auc) performance degradation, with respect to scikit. h2o auc: {0} " \
+-                                        "scickit auc: {1}".format(auc_h2o, auc_sci)
+-
+-
+-if __name__ == "__main__":
+-  pyunit_utils.standalone_test(bernoulli_synthetic_data_gbm_medium)
+-else:
+-  bernoulli_synthetic_data_gbm_medium()
+diff --git a/h2o-py/tests/testdir_algos/gbm/pyunit_DEPRECATED_cup98_01GBM_medium.py b/h2o-py/tests/testdir_algos/gbm/pyunit_DEPRECATED_cup98_01GBM_medium.py
+deleted file mode 100755
+index fcd92e6..0000000
+--- a/h2o-py/tests/testdir_algos/gbm/pyunit_DEPRECATED_cup98_01GBM_medium.py
++++ /dev/null
+@@ -1,26 +0,0 @@
+-import sys
+-sys.path.insert(1,"../../../")
+-import h2o
+-from tests import pyunit_utils
+-
+-
+-def cupMediumGBM():
+-  
+-
+-  train = h2o.import_file(path=pyunit_utils.locate("bigdata/laptop/usecases/cup98LRN_z.csv"))
+-  test = h2o.import_file(path=pyunit_utils.locate("bigdata/laptop/usecases/cup98VAL_z.csv"))
+-
+-  train["TARGET_B"] = train["TARGET_B"].asfactor()
+-
+-  # Train H2O GBM Model:
+-  train_cols = train.names
+-  for c in ['C1', "TARGET_D", "TARGET_B", "CONTROLN"]:
+-    train_cols.remove(c)
+-  model = h2o.gbm(x=train[train_cols], y=train["TARGET_B"], distribution = "bernoulli", ntrees = 5)
+-
+-
+-
+-if __name__ == "__main__":
+-    pyunit_utils.standalone_test(cupMediumGBM)
+-else:
+-   cupMediumGBM()
+diff --git a/h2o-py/tests/testdir_algos/gbm/pyunit_DEPRECATED_milsongs_largeGBM.py b/h2o-py/tests/testdir_algos/gbm/pyunit_DEPRECATED_milsongs_largeGBM.py
+deleted file mode 100644
+index f1eb166..0000000
+--- a/h2o-py/tests/testdir_algos/gbm/pyunit_DEPRECATED_milsongs_largeGBM.py
++++ /dev/null
+@@ -1,56 +0,0 @@
+-from __future__ import print_function
+-from builtins import range
+-import sys
+-sys.path.insert(1,"../../../")
+-import h2o
+-from tests import pyunit_utils
+-import os
+-
+-import random
+-
+-def milsong_checkpoint():
+-
+-    milsong_train = h2o.upload_file(pyunit_utils.locate("bigdata/laptop/milsongs/milsongs-train.csv.gz"))
+-    milsong_valid = h2o.upload_file(pyunit_utils.locate("bigdata/laptop/milsongs/milsongs-test.csv.gz"))
+-    distribution = "gaussian"
+-
+-    # build first model
+-    ntrees1 = random.sample(list(range(50,100)),1)[0]
+-    max_depth1 = random.sample(list(range(2,6)),1)[0]
+-    min_rows1 = random.sample(list(range(10,16)),1)[0]
+-    print("ntrees model 1: {0}".format(ntrees1))
+-    print("max_depth model 1: {0}".format(max_depth1))
+-    print("min_rows model 1: {0}".format(min_rows1))
+-    model1 = h2o.gbm(x=milsong_train[1:],y=milsong_train[0],ntrees=ntrees1,max_depth=max_depth1, min_rows=min_rows1,
+-                     distribution=distribution,validation_x=milsong_valid[1:],validation_y=milsong_valid[0])
+-
+-    # save the model, then load the model
+-    path = pyunit_utils.locate("results")
+-
+-    assert os.path.isdir(path), "Expected save directory {0} to exist, but it does not.".format(path)
+-    model_path = h2o.save_model(model1, path=path, force=True)
+-
+-    assert os.path.isfile(model_path), "Expected load file {0} to exist, but it does not.".format(model_path)
+-    restored_model = h2o.load_model(model_path)
+-
+-    # continue building the model
+-    ntrees2 = ntrees1 + 50
+-    max_depth2 = max_depth1
+-    min_rows2 = min_rows1
+-    print("ntrees model 2: {0}".format(ntrees2))
+-    print("max_depth model 2: {0}".format(max_depth2))
+-    print("min_rows model 2: {0}".format(min_rows2))
+-    model2 = h2o.gbm(x=milsong_train[1:],y=milsong_train[0],ntrees=ntrees2,max_depth=max_depth2, min_rows=min_rows2,
+-                     distribution=distribution,validation_x=milsong_valid[1:],validation_y=milsong_valid[0],
+-                     checkpoint=restored_model.model_id)
+-
+-    # build the equivalent of model 2 in one shot
+-    model3 = h2o.gbm(x=milsong_train[1:],y=milsong_train[0],ntrees=ntrees2,max_depth=max_depth2, min_rows=min_rows2,
+-                     distribution=distribution,validation_x=milsong_valid[1:],validation_y=milsong_valid[0])
+-
+-
+-
+-if __name__ == "__main__":
+-    pyunit_utils.standalone_test(milsong_checkpoint)
+-else:
+-    milsong_checkpoint()
+diff --git a/h2o-py/tests/testdir_algos/gbm/pyunit_DEPRECATED_mnist_manyCols_largeGBM.py b/h2o-py/tests/testdir_algos/gbm/pyunit_DEPRECATED_mnist_manyCols_largeGBM.py
+deleted file mode 100644
+index 16dea15..0000000
+--- a/h2o-py/tests/testdir_algos/gbm/pyunit_DEPRECATED_mnist_manyCols_largeGBM.py
++++ /dev/null
+@@ -1,27 +0,0 @@
+-import sys
+-sys.path.insert(1,"../../../")
+-import h2o
+-from tests import pyunit_utils
+-
+-
+-
+-
+-def mnist_manyCols_largeGBM():
+-    
+-    
+-
+-    #Log.info("Importing mnist train data...\n")
+-    train = h2o.import_file(path=pyunit_utils.locate("bigdata/laptop/mnist/train.csv.gz"))
+-    #Log.info("Check that tail works...")
+-    train.tail()
+-
+-    #Log.info("Doing gbm on mnist training data.... \n")
+-    gbm_mnist = h2o.gbm(x=train[0:784], y=train[784], ntrees=1, max_depth=1, min_rows=10, learn_rate=0.01)
+-    gbm_mnist.show()
+-
+-
+-
+-if __name__ == "__main__":
+-    pyunit_utils.standalone_test(mnist_manyCols_largeGBM)
+-else:
+-    mnist_manyCols_largeGBM()
+diff --git a/h2o-py/tests/testdir_algos/glm/pyunit_DEPRECATED_NOFEATURE_getLambdaModel_mediumGLM.py b/h2o-py/tests/testdir_algos/glm/pyunit_DEPRECATED_NOFEATURE_getLambdaModel_mediumGLM.py
+deleted file mode 100644
+index 25cf3cc..0000000
+--- a/h2o-py/tests/testdir_algos/glm/pyunit_DEPRECATED_NOFEATURE_getLambdaModel_mediumGLM.py
++++ /dev/null
+@@ -1,51 +0,0 @@
+-from __future__ import print_function
+-from builtins import range
+-import sys
+-sys.path.insert(1,"../../../")
+-import h2o
+-from tests import pyunit_utils
+-
+-
+-
+-import random
+-
+-def getLambdaModel():
+-	
+-	
+-
+-	print("Read data")
+-	prostate = h2o.import_file(path=pyunit_utils.locate("smalldata/logreg/prostate.csv"))
+-
+-	myX = ["AGE","RACE","DPROS","DCAPS","PSA","VOL","GLEASON"]
+-	myY = "CAPSULE"
+-	family = random.choice(["gaussian","binomial"])
+-	print(family)
+-
+-	print("Do lambda search and build models")
+-	if family == "gaussian":
+-		model = h2o.glm(x=prostate[myX], y=prostate[myY], family=family, standardize=True, use_all_factor_levels=True, lambda_search=True)
+-	else:
+-		model = h2o.glm(x=prostate[myX], y=prostate[myY].asfactor(), family=family, standardize=True, use_all_factor_levels=True, lambda_search=True)
+-
+-	print("the models were built over the following lambda values: ")
+-	all_lambdas = model.models(1).lambda_all()
+-	print(all_lambdas)
+-
+-	for i in range(10):
+-		Lambda = random.sample(all_lambdas,1)
+-		print("For Lambda we get this model:")
+-		m1 = h2o.getGLMLambdaModel(model.models(random.randint(0,len(model.models()-1)),Lambda=Lambda))
+-		m1.show()
+-		print("this model should be same as the one above:")
+-		m2 = h2o.getGLMLambdaModel(model.models(random.randint(0,len(model.models()-1)),Lambda=Lambda))
+-		m2.show()
+-		assert m1==m2, "expected models to be equal"
+-
+-
+-
+-
+-
+-if __name__ == "__main__":
+-    pyunit_utils.standalone_test(getLambdaModel)
+-else:
+-	getLambdaModel()
+diff --git a/h2o-py/tests/testdir_algos/glm/pyunit_DEPRECATED_NOFEATURE_lambda_search_largeGLM.py b/h2o-py/tests/testdir_algos/glm/pyunit_DEPRECATED_NOFEATURE_lambda_search_largeGLM.py
+deleted file mode 100644
+index 656308b..0000000
+--- a/h2o-py/tests/testdir_algos/glm/pyunit_DEPRECATED_NOFEATURE_lambda_search_largeGLM.py
++++ /dev/null
+@@ -1,48 +0,0 @@
+-import sys
+-sys.path.insert(1,"../../../")
+-import h2o
+-from tests import pyunit_utils
+-
+-
+-
+-import random
+-
+-def lambda_search():
+-    
+-    
+-
+-    #Log.info("Importing prostate.csv data...\n")
+-    prostate = h2o.import_file(pyunit_utils.locate("smalldata/logreg/prostate.csv"))
+-    #prostate.summary()
+-
+-    # GLM without lambda search, lambda is single user-provided value
+-    #Log.info("H2O GLM (binomial) with parameters: lambda_search = TRUE, nfolds: 2\n")
+-    prostate_nosearch = h2o.glm(x=prostate[2:9], y=prostate[1], training_frame = prostate.hex, family = "binomial", nlambdas = 5, lambda_search = False, n_folds = 2)
+-    params_nosearch = prostate_nosearch.params()
+-
+-    try:
+-      prostate_nosearch.getGLMLambdaModel(0.5)
+-      assert False, "expected an error"
+-    except EnvironmentError:
+-      assert True
+-
+-    # GLM with lambda search, return only model corresponding to best lambda as determined by H2O
+-    #Log.info("H2O GLM (binomial) with parameters: lambda_search: TRUE, nfolds: 2\n")
+-    prostate_search = h2o.glm(x=prostate[2:9], y=prostate[1], training_frame = prostate.hex, family = "binomial", nlambdas = 5, lambda_search = True, n_folds = 2)
+-    params_search = prostate_search.params()
+-
+-    random_lambda = random.choice(prostate_search.lambda_all())
+-    #Log.info(cat("Retrieving model corresponding to randomly chosen lambda", random_lambda, "\n"))
+-    random_model = prostate_search.getGLMLambdaModel(random_lambda)
+-    assert random_model.getLambda() == random_lambda, "expected equal lambdas"
+-
+-    #Log.info(cat("Retrieving model corresponding to best lambda", params.bestlambda$lambda_best, "\n"))
+-    best_model = prostate_search.getGLMLambdaModel(params_search.bestlambda())
+-    assert best_model.model() == prostate_search.model(), "expected models to be equal"
+-  
+-
+-
+-if __name__ == "__main__":
+-    pyunit_utils.standalone_test(lambda_search)
+-else:
+-    lambda_search()
+diff --git a/h2o-py/tests/testdir_algos/glm/pyunit_DEPRECATED_NOPASS_random_attack_medium.py b/h2o-py/tests/testdir_algos/glm/pyunit_DEPRECATED_NOPASS_random_attack_medium.py
+deleted file mode 100644
+index 6e0c871..0000000
+--- a/h2o-py/tests/testdir_algos/glm/pyunit_DEPRECATED_NOPASS_random_attack_medium.py
++++ /dev/null
+@@ -1,120 +0,0 @@
+-from __future__ import print_function
+-from builtins import zip
+-from builtins import range
+-import sys
+-sys.path.insert(1,"../../../")
+-import h2o
+-from tests import pyunit_utils
+-
+-
+-
+-import random
+-
+-def random_attack():
+-
+-
+-
+-    def attack(family, train, valid, x, y):
+-        kwargs = {}
+-        kwargs['family'] = family
+-        gaussian_links = ["inverse", "log", "identity"]
+-        binomial_links = ["logit"]
+-        poisson_links =  ["log", "identity"]
+-        gamma_links = ["inverse", "log", "identity"]
+-
+-        # randomly select parameters and their corresponding values
+-        if random.randint(0,1): kwargs['max_iterations'] = random.randint(1,50)
+-        if random.random() > 0.8: kwargs['beta_epsilon'] = random.random()
+-        if random.randint(0,1): kwargs['solver'] = ["AUTO", "IRLSM", "L_BFGS", "COORDINATE_DESCENT_NAIVE",
+-                                                    "COORDINATE_DESCENT"][random.randint(0,1)]
+-        if random.randint(0,1): kwargs['standardize'] = [True, False][random.randint(0,1)]
+-        if random.randint(0,1):
+-            if   family == "gaussian": kwargs['link'] = gaussian_links[random.randint(0,2)]
+-            elif family == "binomial": kwargs['link'] = binomial_links[random.randint(0,0)]
+-            elif family == "poisson" : kwargs['link'] = poisson_links[random.randint(0,1)]
+-            elif family == "gamma"   : kwargs['link'] = gamma_links[random.randint(0,2)]
+-        if random.randint(0,1): kwargs['alpha'] = [random.random()]
+-        if family == "binomial":
+-            if random.randint(0,1): kwargs['prior'] = random.random()
+-        if random.randint(0,1): kwargs['lambda_search'] = [True, False][random.randint(0,1)]
+-        if 'lambda_search' in list(kwargs.keys()):
+-            if random.randint(0,1): kwargs['nlambdas'] = random.randint(2,10)
+-        do_validation = [True, False][random.randint(0,1)]
+-        # beta constraints
+-        if random.randint(0,1):
+-            bc = []
+-            for n in x:
+-                if train[n].isnumeric():
+-                    name = train.names[n]
+-                    lower_bound = random.uniform(-1,1)
+-                    upper_bound = lower_bound + random.random()
+-                    bc.append([name, lower_bound, upper_bound])
+-            if len(bc) > 0:
+-                beta_constraints = h2o.H2OFrame(bc)
+-                beta_constraints.set_names(['names', 'lower_bounds', 'upper_bounds'])
+-                kwargs['beta_constraints'] = beta_constraints.frame_id
+-
+-        # display the parameters and their corresponding values
+-        print("-----------------------")
+-        print("x: {0}".format(x))
+-        print("y: {0}".format(y))
+-        print("validation: {0}".format(do_validation))
+-        for k, v in zip(list(kwargs.keys()), list(kwargs.values())):
+-            if k == 'beta_constraints':
+-                print(k + ": ")
+-                beta_constraints.show()
+-            else:
+-                print(k + ": {0}".format(v))
+-        if do_validation: h2o.glm(x=train[x], y=train[y], validation_x=valid[x], validation_y=valid[y], **kwargs)
+-        else: h2o.glm(x=train[x], y=train[y], **kwargs)
+-        print("-----------------------")
+-
+-    print("Import and data munging...")
+-    seed = random.randint(1,10000)
+-    print("SEED: {0}".format(seed))
+-    pros = h2o.upload_file(pyunit_utils.locate("smalldata/prostate/prostate.csv.zip"))
+-    pros[1] = pros[1].asfactor()
+-    r = pros[0].runif(seed=seed) # a column of length pros.nrow with values between 0 and 1
+-    # ~80/20 train/validation split
+-    pros_train = pros[r > .2]
+-    pros_valid = pros[r <= .2]
+-
+-    cars = h2o.upload_file(pyunit_utils.locate("smalldata/junit/cars.csv"))
+-    r = cars[0].runif(seed=seed)
+-    cars_train = cars[r > .2]
+-    cars_valid = cars[r <= .2]
+-
+-    print()
+-    print("======================================================================")
+-    print("============================== Binomial ==============================")
+-    print("======================================================================")
+-    for i in range(10):
+-        attack("binomial", pros_train, pros_valid, random.sample([2,3,4,5,6,7,8],random.randint(1,7)), 1)
+-
+-    print()
+-    print("======================================================================")
+-    print("============================== Gaussian ==============================")
+-    print("======================================================================")
+-    for i in range(10):
+-        attack("gaussian", cars_train, cars_valid, random.sample([2,3,4,5,6,7],random.randint(1,6)), 1)
+-
+-    print()
+-    print("======================================================================")
+-    print("============================== Poisson  ==============================")
+-    print("======================================================================")
+-    for i in range(10):
+-        attack("poisson", cars_train, cars_valid, random.sample([1,3,4,5,6,7],random.randint(1,6)), 2)
+-
+-    print()
+-    print("======================================================================")
+-    print("==============================  Gamma   ==============================")
+-    print("======================================================================")
+-    for i in range(10):
+-        attack("gamma", pros_train, pros_valid, random.sample([1,2,3,5,6,7,8],random.randint(1,7)), 4)
+-
+-
+-
+-if __name__ == "__main__":
+-    pyunit_utils.standalone_test(random_attack)
+-else:
+-    random_attack()
+diff --git a/h2o-py/tests/testdir_algos/glm/pyunit_DEPRECATED_link_correct_default_largeGLM.py b/h2o-py/tests/testdir_algos/glm/pyunit_DEPRECATED_link_correct_default_largeGLM.py
+deleted file mode 100644
+index 15bad16..0000000
+--- a/h2o-py/tests/testdir_algos/glm/pyunit_DEPRECATED_link_correct_default_largeGLM.py
++++ /dev/null
+@@ -1,47 +0,0 @@
+-from __future__ import print_function
+-import sys
+-sys.path.insert(1,"../../../")
+-import h2o
+-from tests import pyunit_utils
+-
+-
+-
+-
+-def link_correct_default():
+-	
+-	
+-
+-	print("Reading in original prostate data.")
+-	h2o_data = h2o.upload_file(path=pyunit_utils.locate("smalldata/prostate/prostate.csv.zip"))
+-
+-	print("Compare models with link unspecified and canonical link specified.")
+-	print("GAUSSIAN: ")
+-	h2o_model_unspecified = h2o.glm(x=h2o_data[1:8], y=h2o_data[8], family="gaussian")
+-	h2o_model_specified = h2o.glm(x=h2o_data[1:8], y=h2o_data[8], family="gaussian", link="identity")
+-	assert h2o_model_specified._model_json['output']['coefficients_table'].cell_values == \
+-		   h2o_model_unspecified._model_json['output']['coefficients_table'].cell_values, "coefficient should be equal"
+-
+-	print("BINOMIAL: ")
+-	h2o_model_unspecified = h2o.glm(x=h2o_data[2:9], y=h2o_data[1], family="binomial")
+-	h2o_model_specified = h2o.glm(x=h2o_data[2:9], y=h2o_data[1], family="binomial", link="logit")
+-	assert h2o_model_specified._model_json['output']['coefficients_table'].cell_values == \
+-		   h2o_model_unspecified._model_json['output']['coefficients_table'].cell_values, "coefficient should be equal"
+-
+-	print("POISSON: ")
+-	h2o_model_unspecified = h2o.glm(x=h2o_data[2:9], y=h2o_data[1], family="poisson")
+-	h2o_model_specified = h2o.glm(x=h2o_data[2:9], y=h2o_data[1], family="poisson", link="log")
+-	assert h2o_model_specified._model_json['output']['coefficients_table'].cell_values == \
+-		   h2o_model_unspecified._model_json['output']['coefficients_table'].cell_values, "coefficient should be equal"
+-
+-	print("GAMMA: ")
+-	h2o_model_unspecified = h2o.glm(x=h2o_data[3:9], y=h2o_data[2], family="gamma")
+-	h2o_model_specified = h2o.glm(x=h2o_data[3:9], y=h2o_data[2], family="gamma", link="inverse")
+-	assert h2o_model_specified._model_json['output']['coefficients_table'].cell_values == \
+-		   h2o_model_unspecified._model_json['output']['coefficients_table'].cell_values, "coefficient should be equal"
+-
+-
+-
+-if __name__ == "__main__":
+-    pyunit_utils.standalone_test(link_correct_default)
+-else:
+-	link_correct_default()
+diff --git a/h2o-py/tests/testdir_algos/glm/pyunit_DEPRECATED_shuffling_largeGLM.py b/h2o-py/tests/testdir_algos/glm/pyunit_DEPRECATED_shuffling_largeGLM.py
+deleted file mode 100644
+index 20ccbaa..0000000
+--- a/h2o-py/tests/testdir_algos/glm/pyunit_DEPRECATED_shuffling_largeGLM.py
++++ /dev/null
+@@ -1,53 +0,0 @@
+-from __future__ import print_function
+-from builtins import zip
+-import sys
+-sys.path.insert(1,"../../../")
+-import h2o
+-from tests import pyunit_utils
+-
+-
+-
+-
+-def shuffling_large():
+-    
+-    
+-
+-    print("Reading in Arcene training data for binomial modeling.")
+-    train_data = h2o.upload_file(path=pyunit_utils.locate("smalldata/arcene/shuffle_test_version/arcene.csv"))
+-    train_data_shuffled = h2o.upload_file(path=pyunit_utils.locate("smalldata/arcene/shuffle_test_version/arcene_shuffled.csv"))
+-
+-
+-    print("Create model on original Arcene dataset.")
+-    h2o_model = h2o.glm(x=train_data[0:1000], y=train_data[1000], family="binomial", lambda_search=True, alpha=[0.5])
+-
+-    print("Create second model on original Arcene dataset.")
+-    h2o_model_2 = h2o.glm(x=train_data[0:1000], y=train_data[1000], family="binomial", lambda_search=True, alpha=[0.5])
+-
+-    print("Create model on shuffled Arcene dataset.")
+-    h2o_model_s = h2o.glm(x=train_data_shuffled[0:1000], y=train_data_shuffled[1000], family="binomial",
+-                          lambda_search=True, alpha=[0.5])
+-
+-    print("Assert that number of predictors remaining and their respective coefficients are equal.")
+-
+-    for x, y in zip(h2o_model._model_json['output']['coefficients_table'].cell_values,h2o_model_2.
+-            _model_json['output']['coefficients_table'].cell_values):
+-        assert (type(x[1]) == type(y[1])) and (type(x[2]) == type(y[2])), "coefficients should be the same type"
+-        if isinstance(x[1],float):
+-            assert abs(x[1] - y[1]) < 5e-10, "coefficients should be equal"
+-        if isinstance(x[2],float):
+-            assert abs(x[2] - y[2]) < 5e-10, "coefficients should be equal"
+-
+-    for x, y in zip(h2o_model._model_json['output']['coefficients_table'].cell_values,h2o_model_s.
+-            _model_json['output']['coefficients_table'].cell_values):
+-        assert (type(x[1]) == type(y[1])) and (type(x[2]) == type(y[2])), "coefficients should be the same type"
+-        if isinstance(x[1],float):
+-            assert abs(x[1] - y[1]) < 5e-10, "coefficients should be equal"
+-        if isinstance(x[2],float):
+-            assert abs(x[2] - y[2]) < 5e-10, "coefficients should be equal"
+-
+-
+-
+-if __name__ == "__main__":
+-    pyunit_utils.standalone_test(shuffling_large)
+-else:
+-    shuffling_large()
+diff --git a/h2o-py/tests/testdir_algos/glm/pyunit_DEPRECATED_wide_dataset_largeGLM.py b/h2o-py/tests/testdir_algos/glm/pyunit_DEPRECATED_wide_dataset_largeGLM.py
+deleted file mode 100644
+index 0e6941e..0000000
+--- a/h2o-py/tests/testdir_algos/glm/pyunit_DEPRECATED_wide_dataset_largeGLM.py
++++ /dev/null
+@@ -1,43 +0,0 @@
+-from __future__ import print_function
+-from builtins import zip
+-import sys
+-sys.path.insert(1,"../../../")
+-import h2o
+-from tests import pyunit_utils
+-
+-
+-
+-import numpy as np
+-
+-def wide_dataset_large():
+-
+-
+-
+-    print("Reading in Arcene training data for binomial modeling.")
+-    trainDataResponse = np.genfromtxt(pyunit_utils.locate("smalldata/arcene/arcene_train_labels.labels"), delimiter=' ')
+-    trainDataResponse = np.where(trainDataResponse == -1, 0, 1)
+-    trainDataFeatures = np.genfromtxt(pyunit_utils.locate("smalldata/arcene/arcene_train.data"), delimiter=' ')
+-    trainData = h2o.H2OFrame(np.column_stack((trainDataResponse, trainDataFeatures)).tolist())
+-
+-    print("Run model on 3250 columns of Arcene with strong rules off.")
+-    model = h2o.glm(x=trainData[1:3250], y=trainData[0].asfactor(), family="binomial", lambda_search=False, alpha=[1])
+-
+-    print("Test model on validation set.")
+-    validDataResponse = np.genfromtxt(pyunit_utils.locate("smalldata/arcene/arcene_valid_labels.labels"), delimiter=' ')
+-    validDataResponse = np.where(validDataResponse == -1, 0, 1)
+-    validDataFeatures = np.genfromtxt(pyunit_utils.locate("smalldata/arcene/arcene_valid.data"), delimiter=' ')
+-    validData = h2o.H2OFrame(np.column_stack((validDataResponse, validDataFeatures)).tolist())
+-    prediction = model.predict(validData)
+-
+-    print("Check performance of predictions.")
+-    performance = model.model_performance(validData)
+-
+-    print("Check that prediction AUC better than guessing (0.5).")
+-    assert performance.auc() > 0.5, "predictions should be better then pure chance"
+-
+-
+-
+-if __name__ == "__main__":
+-    pyunit_utils.standalone_test(wide_dataset_large)
+-else:
+-    wide_dataset_large()
+diff --git a/h2o-py/tests/testdir_algos/kmeans/pyunit_DEPRECATED_random_attack_medium.py b/h2o-py/tests/testdir_algos/kmeans/pyunit_DEPRECATED_random_attack_medium.py
+deleted file mode 100644
+index 0d9874f..0000000
+--- a/h2o-py/tests/testdir_algos/kmeans/pyunit_DEPRECATED_random_attack_medium.py
++++ /dev/null
+@@ -1,59 +0,0 @@
+-from __future__ import print_function
+-from builtins import zip
+-from builtins import range
+-import sys
+-sys.path.insert(1,"../../../")
+-import h2o
+-from tests import pyunit_utils
+-
+-
+-
+-import random
+-
+-def random_attack():
+-
+-
+-
+-    def attack(train, x):
+-        kwargs = {}
+-
+-        # randomly select parameters and their corresponding values
+-        kwargs['k'] = random.randint(1,20)
+-        if random.randint(0,1): kwargs['model_id'] = "my_model"
+-        if random.randint(0,1): kwargs['max_iterations'] = random.randint(1,1000)
+-        if random.randint(0,1): kwargs['standardize'] = [True, False][random.randint(0,1)]
+-        if random.randint(0,1):
+-            method = random.randint(0,3)
+-            if method == 3:
+-                s = [[random.uniform(train[c].mean()[0]-100,train[c].mean()[0]+100) for p in range(kwargs['k'])] for c in x]
+-                print("s: {0}".format(s))
+-                start = h2o.H2OFrame(list(zip(*s)))
+-                kwargs['user_points'] = start
+-            else:
+-                kwargs['init'] = ["Furthest","Random", "PlusPlus"][method]
+-        if random.randint(0,1): kwargs['seed'] = random.randint(1,10000)
+-
+-        # display the parameters and their corresponding values
+-        print("-----------------------")
+-        print("x: {0}".format(x))
+-        for k, v in zip(list(kwargs.keys()), list(kwargs.values())):
+-            if k == 'user_points':
+-                print(k + ": ")
+-                start.show()
+-            else:
+-                print(k + ": {0}".format(v))
+-        h2o.kmeans(x=train[x],  **kwargs)
+-        print("-----------------------")
+-
+-    print("Import and data munging...")
+-    ozone = h2o.import_file(path=pyunit_utils.locate("smalldata/glm_test/ozone.csv"))
+-
+-    for i in range(50):
+-        attack(ozone, random.sample([0,1,2,3],random.randint(1,4)))
+-
+-
+-
+-if __name__ == "__main__":
+-    pyunit_utils.standalone_test(random_attack)
+-else:
+-    random_attack()
+diff --git a/h2o-py/tests/testdir_algos/rf/pyunit_DEPRECATED_czechboard_mediumRF.py b/h2o-py/tests/testdir_algos/rf/pyunit_DEPRECATED_czechboard_mediumRF.py
+deleted file mode 100644
+index 3425492..0000000
+--- a/h2o-py/tests/testdir_algos/rf/pyunit_DEPRECATED_czechboard_mediumRF.py
++++ /dev/null
+@@ -1,28 +0,0 @@
+-import sys
+-sys.path.insert(1,"../../../")
+-import h2o
+-from tests import pyunit_utils
+-
+-
+-
+-
+-def czechboardRF():
+-
+-    
+-    
+-
+-    # Training set has checkerboard pattern
+-    board = h2o.import_file(path=pyunit_utils.locate("smalldata/gbm_test/czechboard_300x300.csv"))
+-    board["C3"] = board["C3"].asfactor()
+-    board.summary()
+-
+-    # Train H2O DRF Model:
+-    model = h2o.random_forest(x=board[["C1", "C2"]], y=board["C3"], ntrees=50, max_depth=20, nbins=500)
+-    model.show()
+-  
+-
+-
+-if __name__ == "__main__":
+-    pyunit_utils.standalone_test(czechboardRF)
+-else:
+-    czechboardRF()
+diff --git a/h2o-py/tests/testdir_algos/rf/pyunit_DEPRECATED_milsongs_largeRF.py b/h2o-py/tests/testdir_algos/rf/pyunit_DEPRECATED_milsongs_largeRF.py
+deleted file mode 100644
+index 0eb31e3..0000000
+--- a/h2o-py/tests/testdir_algos/rf/pyunit_DEPRECATED_milsongs_largeRF.py
++++ /dev/null
+@@ -1,58 +0,0 @@
+-from __future__ import print_function
+-from builtins import range
+-import sys
+-sys.path.insert(1,"../../../")
+-import h2o
+-from tests import pyunit_utils
+-import os
+-
+-import random
+-
+-def milsong_checkpoint():
+-
+-    milsong_train = h2o.upload_file(pyunit_utils.locate("bigdata/laptop/milsongs/milsongs-train.csv.gz"))
+-    milsong_valid = h2o.upload_file(pyunit_utils.locate("bigdata/laptop/milsongs/milsongs-test.csv.gz"))
+-
+-    # build first model
+-    ntrees1 = random.sample(list(range(50,100)),1)[0]
+-    max_depth1 = random.sample(list(range(2,6)),1)[0]
+-    min_rows1 = random.sample(list(range(10,16)),1)[0]
+-    print("ntrees model 1: {0}".format(ntrees1))
+-    print("max_depth model 1: {0}".format(max_depth1))
+-    print("min_rows model 1: {0}".format(min_rows1))
+-    model1 = h2o.random_forest(x=milsong_train[1:],y=milsong_train[0],ntrees=ntrees1,max_depth=max_depth1, min_rows=min_rows1,
+-                               validation_x=milsong_valid[1:],validation_y=milsong_valid[0],seed=1234)
+-
+-    # save the model, then load the model
+-    path = pyunit_utils.locate("results")
+-
+-    assert os.path.isdir(path), "Expected save directory {0} to exist, but it does not.".format(path)
+-    model_path = h2o.save_model(model1, path=path, force=True)
+-
+-    assert os.path.isfile(model_path), "Expected load file {0} to exist, but it does not.".format(model_path)
+-    restored_model = h2o.load_model(model_path)
+-
+-    # continue building the model
+-    ntrees2 = ntrees1 + 50
+-    max_depth2 = max_depth1
+-    min_rows2 = min_rows1
+-    print("ntrees model 2: {0}".format(ntrees2))
+-    print("max_depth model 2: {0}".format(max_depth2))
+-    print("min_rows model 2: {0}".format(min_rows2))
+-    model2 = h2o.random_forest(x=milsong_train[1:],y=milsong_train[0],ntrees=ntrees2,max_depth=max_depth2, min_rows=min_rows2,
+-                               validation_x=milsong_valid[1:],validation_y=milsong_valid[0],
+-                               checkpoint=restored_model._id,seed=1234)
+-
+-    # build the equivalent of model 2 in one shot
+-    model3 = h2o.random_forest(x=milsong_train[1:],y=milsong_train[0],ntrees=ntrees2,max_depth=max_depth2, min_rows=min_rows2,
+-                               validation_x=milsong_valid[1:],validation_y=milsong_valid[0],seed=1234)
+-
+-    assert isinstance(model2,type(model3))
+-    assert model2.mse(valid=True)==model3.mse(valid=True), "Expected Model 2 MSE: {0} to be the same as Model 4 MSE: {1}".format(model2.mse(valid=True), model3.mse(valid=True))
+-
+-
+-
+-if __name__ == "__main__":
+-    pyunit_utils.standalone_test(milsong_checkpoint)
+-else:
+-    milsong_checkpoint()
+diff --git a/h2o-py/tests/testdir_algos/rf/pyunit_DEPRECATED_random_attack_medium.py b/h2o-py/tests/testdir_algos/rf/pyunit_DEPRECATED_random_attack_medium.py
+deleted file mode 100644
+index 9ce8037..0000000
+--- a/h2o-py/tests/testdir_algos/rf/pyunit_DEPRECATED_random_attack_medium.py
++++ /dev/null
+@@ -1,88 +0,0 @@
+-from __future__ import print_function
+-from builtins import zip
+-from builtins import range
+-import sys
+-sys.path.insert(1,"../../../")
+-import h2o
+-from tests import pyunit_utils
+-
+-
+-
+-import random
+-
+-def random_attack():
+-    
+-    
+-
+-    def attack(train, valid, x, y):
+-        kwargs = {}
+-
+-        # randomly select parameters and their corresponding values
+-        if random.randint(0,1): kwargs['mtries'] = random.randint(1,len(x))
+-        if random.randint(0,1): kwargs['sample_rate'] = random.random()
+-        if random.randint(0,1): kwargs['build_tree_one_node'] = True
+-        if random.randint(0,1): kwargs['ntrees'] = random.randint(1,10)
+-        if random.randint(0,1): kwargs['max_depth'] = random.randint(1,5)
+-        if random.randint(0,1): kwargs['min_rows'] = random.randint(1,10)
+-        if random.randint(0,1): kwargs['nbins'] = random.randint(2,20)
+-        if random.randint(0,1):
+-            kwargs['balance_classes'] = True
+-            if random.randint(0,1): kwargs['max_after_balance_size'] = random.uniform(0,10)
+-        if random.randint(0,1): kwargs['seed'] = random.randint(1,10000)
+-        do_validation = [True, False][random.randint(0,1)]
+-
+-        # display the parameters and their corresponding values
+-        print("-----------------------")
+-        print("x: {0}".format(x))
+-        print("y: {0}".format(y))
+-        print("validation: {0}".format(do_validation))
+-        for k, v in zip(list(kwargs.keys()), list(kwargs.values())): print(k + ": {0}".format(v))
+-        if do_validation: h2o.random_forest(x=train[x], y=train[y], validation_x=valid[x], validation_y=valid[y], **kwargs)
+-        else: h2o.random_forest(x=train[x], y=train[y], **kwargs)
+-        print("-----------------------")
+-
+-    print("Import and data munging...")
+-    pros = h2o.upload_file(pyunit_utils.locate("smalldata/prostate/prostate.csv.zip"))
+-    pros[1] = pros[1].asfactor()
+-    pros[4] = pros[4].asfactor()
+-    pros[5] = pros[5].asfactor()
+-    pros[8] = pros[8].asfactor()
+-    r = pros[0].runif() # a column of length pros.nrow with values between 0 and 1
+-    # ~80/20 train/validation split
+-    pros_train = pros[r > .2]
+-    pros_valid = pros[r <= .2]
+-
+-    cars = h2o.upload_file(pyunit_utils.locate("smalldata/junit/cars.csv"))
+-    r = cars[0].runif()
+-    cars_train = cars[r > .2]
+-    cars_valid = cars[r <= .2]
+-
+-    print()
+-    print("======================================================================")
+-    print("============================== Binomial ==============================")
+-    print("======================================================================")
+-    for i in range(10):
+-        attack(pros_train, pros_valid, random.sample([2,3,4,5,6,7,8],random.randint(1,7)), 1)
+-
+-    print()
+-    print("======================================================================")
+-    print("============================== Gaussian ==============================")
+-    print("======================================================================")
+-    for i in range(10):
+-        attack(cars_train, cars_valid, random.sample([2,3,4,5,6,7],random.randint(1,6)), 1)
+-
+-    print()
+-    print("======================================================================")
+-    print("============================= Multinomial ============================")
+-    print("======================================================================")
+-    cars_train[2] = cars_train[2].asfactor()
+-    cars_valid[2] = cars_valid[2].asfactor()
+-    for i in range(10):
+-        attack(cars_train, cars_valid, random.sample([1,3,4,5,6,7],random.randint(1,6)), 2)
+-
+-
+-
+-if __name__ == "__main__":
+-    pyunit_utils.standalone_test(random_attack)
+-else:
+-    random_attack()
+diff --git a/h2o-py/tests/testdir_munging/pyunit_substring.py b/h2o-py/tests/testdir_munging/pyunit_substring.py
+new file mode 100644
+index 0000000..6e10d71
+--- /dev/null
++++ b/h2o-py/tests/testdir_munging/pyunit_substring.py
+@@ -0,0 +1,47 @@
++import sys
++sys.path.insert(1,"../../")
++import h2o
++from tests import pyunit_utils
++
++
++def substring_check():
++
++  for parse_type in ('string', 'enum'):
++    frame = h2o.import_file(path=pyunit_utils.locate("smalldata/iris/iris.csv"), col_types={"C5":parse_type})
++    py_data = frame["C5"].as_data_frame()[1:]
++    indices = [(1,3),(0,22),(5,6),(6,5),(5,None),(9,9)]
++    for s_i, e_i in indices:
++      g = frame["C5"].substring(s_i, e_i)
++      assert g[0,0] == py_data[0][0][s_i:e_i]
++      if parse_type == 'enum':
++        data_levels = set(map(lambda x: x[s_i:e_i], list(zip(*py_data))[0]))
++        assert set(g.levels()[0]) == data_levels
++        assert g.nlevels()[0] == len(data_levels)
++
++
++  #test negative index args
++  string = h2o.H2OFrame.from_python(("nothing",), column_types=['string'])
++  enum = h2o.H2OFrame.from_python(("nothing",), column_types=['enum'])
++  assert string.substring(-4)[0,0] == 'nothing'
++  assert string.substring(-4,-9)[0,0] == ''
++  assert enum.substring(-5)[0,0] == 'nothing'
++  assert enum.substring(-43,-3)[0,0] == ''
++  
++  #test NA values
++  string = h2o.H2OFrame.from_python([["nothing"],["NA"]], column_types=['string'], na_strings=["NA"])
++  enum = h2o.H2OFrame.from_python([["nothing"],["NA"]], column_types=['enum'], na_strings=["NA"])
++  assert ((string.substring(2,5)).isna() == h2o.H2OFrame([[0],[1]])).all()
++  assert ((enum.substring(2,5)).isna() == h2o.H2OFrame([[0],[1]])).all()
++  
++  #test empty strings
++  string = h2o.H2OFrame.from_python([''], column_types=['string'])
++  enum = h2o.H2OFrame.from_python([''], column_types=['enum'])
++  assert string.substring(3,6)[0,0] == ''
++  assert string.substring(0,0)[0,0] == ''
++  assert enum.substring(3,6)[0,0] == ''
++  assert enum.substring(0,0)[0,0] == ''
++
++if __name__ == "__main__":
++  pyunit_utils.standalone_test(substring_check)
++else:
++  substring_check()
+diff --git a/h2o-r/H2O_Load.R b/h2o-r/H2O_Load.R
+index e07b39c..39442fd 100755
+--- a/h2o-r/H2O_Load.R
++++ b/h2o-r/H2O_Load.R
+@@ -2,7 +2,7 @@
+ CLIFF.ROOT.PATH <- "C:/Users/cliffc/Desktop/"
+ SPENCER.ROOT.PATH <- "/Users/spencer/0xdata/"
+ LUDI.ROOT.PATH <- "/Users/ludirehak/"
+-ROOT.PATH <- SPENCER.ROOT.PATH
++ROOT.PATH <- LUDI.ROOT.PATH
+ DEV.PATH  <- "h2o-3/h2o-r/h2o-package/R/"
+ FULL.PATH <- paste(ROOT.PATH, DEV.PATH, sep="")
+ 
+diff --git a/h2o-r/h2o-package/R/communication.R b/h2o-r/h2o-package/R/communication.R
+index cd99111..da258a4 100755
+--- a/h2o-r/h2o-package/R/communication.R
++++ b/h2o-r/h2o-package/R/communication.R
+@@ -672,11 +672,33 @@ h2o.clusterInfo <- function() {
+ #' @export
+ h2o.is_client <- function() get("IS_CLIENT", .pkg.env)
+ 
++
++#'
++#' Disable Progress Bar
++#' 
++#' @export
++h2o.no_progress <- function() assign("PROGRESS_BAR", FALSE, .pkg.env)
++
++#'
++#' Enable Progress Bar
++#' 
++#' @export
++h2o.show_progress <- function() assign("PROGRESS_BAR", TRUE, .pkg.env)
++
++#'
++#' Check if Progress Bar is Enabled 
++#' 
++.h2o.is_progress <- function() get("PROGRESS_BAR", .pkg.env)
++
++h2o.show_progress()
++
++
+ #-----------------------------------------------------------------------------------------------------------------------
+ #   Job Polling
+ #-----------------------------------------------------------------------------------------------------------------------
+ 
+-.h2o.__waitOnJob <- function(job_key, pollInterval = 1, progressBar = TRUE) {
++.h2o.__waitOnJob <- function(job_key, pollInterval = 1) {
++  progressBar <- .h2o.is_progress()
+   if (progressBar) pb <- txtProgressBar(style = 3L)
+   keepRunning <- TRUE
+   tryCatch({
+diff --git a/h2o-r/h2o-package/R/deeplearning.R b/h2o-r/h2o-package/R/deeplearning.R
+index 1106ea6..6b09d0e 100755
+--- a/h2o-r/h2o-package/R/deeplearning.R
++++ b/h2o-r/h2o-package/R/deeplearning.R
+@@ -75,7 +75,6 @@
+ #' @param stopping_tolerance Relative tolerance for metric-based stopping criterion (if relative
+ #'        improvement is not at least this much, stop).
+ #' @param max_runtime_secs Maximum allowed runtime in seconds for model training. Use 0 to disable.
+-#'        For cross-validation and grid searches, this time limit applies to all sub-models.
+ #' @param quiet_mode Enable quiet mode for less output to standard output.
+ #' @param max_confusion_matrix_size Max. size (number of classes) for confusion matrices to be shown
+ #' @param max_hit_ratio_k Max number (top K) of predictions to use for hit ratio computation (for
+diff --git a/h2o-r/h2o-package/R/frame.R b/h2o-r/h2o-package/R/frame.R
+index 856df41..3a52282 100644
+--- a/h2o-r/h2o-package/R/frame.R
++++ b/h2o-r/h2o-package/R/frame.R
+@@ -643,6 +643,15 @@ h2o.table <- function(x, y = NULL, dense = TRUE) {
+ #' @export
+ table.H2OFrame <- h2o.table
+ 
++
++#' H2O Unique
++#'
++#' Extract unique values in the column.
++#'
++#' @param x An H2OFrame object.
++#' @export
++h2o.unique <- function(x) .newExpr("unique", x)
++
+ #' H2O Median
+ #'
+ #' Compute the median of an H2OFrame.
+@@ -2753,3 +2762,22 @@ h2o.trim <- function(x) .newExpr("trim", x)
+ #' @param x The column whose string lengths will be returned.
+ #' @export
+ h2o.nchar <- function(x) .newExpr("length", x)
++
++#'
++#' Substring
++#'
++#' 
++#' Returns a copy of the target column that is a substring at the specified start 
++#' and stop indices, inclusive. If the stop index is not specified, then the substring extends
++#' to the end of the original string. If start is longer than the number of characters
++#' in the original string, or is greater than stop, an empty string is returned. Negative start
++#' is coerced to 0. 
++#'
++#' @param x The column on which to operate.
++#' @param start The index of the first element to be included in the substring.
++#' @param stop Optional, The index of the last element to be included in the substring. 
++#' @export
++h2o.substring <- function(x, start, stop="[]") .newExpr("substring", x, start-1, stop)
++
++#' @rdname h2o.substring
++h2o.substr <- h2o.substring
+\ No newline at end of file
+diff --git a/h2o-r/h2o-package/R/gbm.R b/h2o-r/h2o-package/R/gbm.R
+index 8a97005..386640d 100755
+--- a/h2o-r/h2o-package/R/gbm.R
++++ b/h2o-r/h2o-package/R/gbm.R
+@@ -54,7 +54,6 @@
+ #' @param stopping_tolerance Relative tolerance for metric-based stopping criterion (if relative
+ #'        improvement is not at least this much, stop)
+ #' @param max_runtime_secs Maximum allowed runtime in seconds for model training. Use 0 to disable.
+-#'        For cross-validation and grid searches, this time limit applies to all sub-models.
+ #' @param offset_column Specify the offset column.
+ #' @param weights_column Specify the weights column.
+ #' @seealso \code{\link{predict.H2OModel}} for prediction.
+diff --git a/h2o-r/h2o-package/R/glm.R b/h2o-r/h2o-package/R/glm.R
+index 48d33f8..2b9240f 100755
+--- a/h2o-r/h2o-package/R/glm.R
++++ b/h2o-r/h2o-package/R/glm.R
+@@ -59,7 +59,6 @@
+ #' @param compute_p_values (Optional)  Logical, compute p-values, only allowed with IRLSM solver and no regularization. May fail if there are collinear predictors.
+ #' @param remove_collinear_columns (Optional)  Logical, valid only with no regularization. If set, co-linear columns will be automatically ignored (coefficient will be 0).
+ #' @param max_runtime_secs Maximum allowed runtime in seconds for model training. Use 0 to disable.
+-#'        For cross-validation and grid searches, this time limit applies to all sub-models.
+ #' @param ... (Currently Unimplemented)
+ #'        coefficients.
+ #'
+diff --git a/h2o-r/h2o-package/R/glrm.R b/h2o-r/h2o-package/R/glrm.R
+index c148952..9a6ab67 100644
+--- a/h2o-r/h2o-package/R/glrm.R
++++ b/h2o-r/h2o-package/R/glrm.R
+@@ -79,7 +79,6 @@
+ #'        should be recovered during post-processing of the generalized low rank decomposition.
+ #' @param seed (Optional) Random seed used to initialize the X and Y matrices.
+ #' @param max_runtime_secs Maximum allowed runtime in seconds for model training. Use 0 to disable.
+-#'        For cross-validation and grid searches, this time limit applies to all sub-models.
+ #' @return Returns an object of class \linkS4class{H2ODimReductionModel}.
+ #' @seealso \code{\link{h2o.kmeans}, \link{h2o.svd}}, \code{\link{h2o.prcomp}}
+ #' @references M. Udell, C. Horn, R. Zadeh, S. Boyd (2014). {Generalized Low Rank Models}[http://arxiv.org/abs/1410.0342]. Unpublished manuscript, Stanford Electrical Engineering Department.
+diff --git a/h2o-r/h2o-package/R/kmeans.R b/h2o-r/h2o-package/R/kmeans.R
+index 868cd0e..fb73c8a 100755
+--- a/h2o-r/h2o-package/R/kmeans.R
++++ b/h2o-r/h2o-package/R/kmeans.R
+@@ -36,7 +36,6 @@
+ #'        Must be "AUTO", "Random" or "Modulo"
+ #' @param keep_cross_validation_predictions Whether to keep the predictions of the cross-validation models
+ #' @param max_runtime_secs Maximum allowed runtime in seconds for model training. Use 0 to disable.
+-#'        For cross-validation and grid searches, this time limit applies to all sub-models.
+ #' @return Returns an object of class \linkS4class{H2OClusteringModel}.
+ #' @seealso \code{\link{h2o.cluster_sizes}}, \code{\link{h2o.totss}}, \code{\link{h2o.num_iterations}},
+ #'          \code{\link{h2o.betweenss}}, \code{\link{h2o.tot_withinss}}, \code{\link{h2o.withinss}},
+diff --git a/h2o-r/h2o-package/R/naivebayes.R b/h2o-r/h2o-package/R/naivebayes.R
+index 2e96b83..6b80a77 100644
+--- a/h2o-r/h2o-package/R/naivebayes.R
++++ b/h2o-r/h2o-package/R/naivebayes.R
+@@ -25,7 +25,6 @@
+ #' @param compute_metrics A logical value indicating whether model metrics should be computed. Set to
+ #'        FALSE to reduce the runtime of the algorithm.
+ #' @param max_runtime_secs Maximum allowed runtime in seconds for model training. Use 0 to disable.
+-#'        For cross-validation and grid searches, this time limit applies to all sub-models.
+ #' @details The naive Bayes classifier assumes independence between predictor variables conditional
+ #'        on the response, and a Gaussian distribution of numeric predictors with mean and standard
+ #'        deviation computed from the training dataset. When building a naive Bayes classifier,
+diff --git a/h2o-r/h2o-package/R/pca.R b/h2o-r/h2o-package/R/pca.R
+index 0b66313..1ab3f8a 100644
+--- a/h2o-r/h2o-package/R/pca.R
++++ b/h2o-r/h2o-package/R/pca.R
+@@ -42,7 +42,6 @@
+ #' @param seed (Optional) Random seed used to initialize the right singular vectors
+ #'        at the beginning of each power method iteration.
+ #' @param max_runtime_secs Maximum allowed runtime in seconds for model training. Use 0 to disable.
+-#'        For cross-validation and grid searches, this time limit applies to all sub-models.
+ #' @return Returns an object of class \linkS4class{H2ODimReductionModel}.
+ #' @references N. Halko, P.G. Martinsson, J.A. Tropp. {Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions}[http://arxiv.org/abs/0909.4061]. SIAM Rev., Survey and Review section, Vol. 53, num. 2, pp. 217-288, June 2011.
+ #' @seealso \code{\link{h2o.svd}}, \code{\link{h2o.glrm}}
+diff --git a/h2o-r/h2o-package/R/randomforest.R b/h2o-r/h2o-package/R/randomforest.R
+index e583618..35aa520 100644
+--- a/h2o-r/h2o-package/R/randomforest.R
++++ b/h2o-r/h2o-package/R/randomforest.R
+@@ -55,7 +55,6 @@
+ #' @param stopping_tolerance Relative tolerance for metric-based stopping criterion (if relative
+ #'        improvement is not at least this much, stop)
+ #' @param max_runtime_secs Maximum allowed runtime in seconds for model training. Use 0 to disable.
+-#'        For cross-validation and grid searches, this time limit applies to all sub-models.
+ #' @param ... (Currently Unimplemented)
+ #' @return Creates a \linkS4class{H2OModel} object of the right type.
+ #' @seealso \code{\link{predict.H2OModel}} for prediction.
+diff --git a/h2o-r/h2o-package/R/svd.R b/h2o-r/h2o-package/R/svd.R
+index 6d08def..837498d 100644
+--- a/h2o-r/h2o-package/R/svd.R
++++ b/h2o-r/h2o-package/R/svd.R
+@@ -31,7 +31,6 @@
+ #'        If FALSE, the indicator column corresponding to the first factor level
+ #'        of every categorical variable will be dropped. Defaults to TRUE.
+ #' @param max_runtime_secs Maximum allowed runtime in seconds for model training. Use 0 to disable.
+-#'        For cross-validation and grid searches, this time limit applies to all sub-models.
+ #' @return Returns an object of class \linkS4class{H2ODimReductionModel}.
+ #' @references N. Halko, P.G. Martinsson, J.A. Tropp. {Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions}[http://arxiv.org/abs/0909.4061]. SIAM Rev., Survey and Review section, Vol. 53, num. 2, pp. 217-288, June 2011.
+ #' @examples
+diff --git a/py/h2o_test_utils.py b/py/h2o_test_utils.py
+index 569c7db..5bca230 100644
+--- a/py/h2o_test_utils.py
++++ b/py/h2o_test_utils.py
+@@ -615,9 +615,11 @@ class GridSpec(dict):
+         for k, vals in self['grid_params'].iteritems():
+             combos *= len(vals)
+ 
++        # NOTE: if we have a stopping critereon which is not a fixed number we don't know how many models to expect
++        expected = None
+         if self['search_criteria'] is None or self['search_criteria']['strategy'] is 'Cartesian':
+             expected = combos
+-        elif self['search_criteria'] is not None and 'max_models' in self['search_criteria']:
++        elif self['search_criteria'] is not None and 'max_models' in self['search_criteria'] and 'max_time_ms' not in self['search_criteria']:
+             expected = min(combos, self['search_criteria']['max_models'])
+ 
+         if expected is not None:
+diff --git a/py/rest_tests/test_models.py b/py/rest_tests/test_models.py
+index d625533..44c90e1 100644
+--- a/py/rest_tests/test_models.py
++++ b/py/rest_tests/test_models.py
+@@ -62,6 +62,7 @@ def build_and_test(a_node, pp, datasets, algos, algo_additional_default_params):
+ 
+         # Test stopping criteria:
+         GridSpec.for_dataset('gbm_prostate_regression_grid_max_3', 'gbm', datasets['prostate_regression'], { 'max_depth': 3 }, { 'ntrees': [1, 2, 4], 'distribution': ["gaussian", "poisson", "gamma", "tweedie"] }, { 'strategy': "Random", 'max_models': 3 } ),
++        GridSpec.for_dataset('gbm_prostate_regression_grid_max_10mS', 'gbm', datasets['prostate_regression'], { 'max_depth': 3 }, { 'ntrees': [1, 2, 4], 'distribution': ["gaussian", "poisson", "gamma", "tweedie"] }, { 'strategy': "Random", 'max_time_ms': 10 } ),
+        ]
+     
+     for grid_spec in grids_to_build:
diff --git a/jacoco/jacocoagent.jar b/jacoco/jacocoagent.jar
new file mode 100644
index 0000000..a786c95
Binary files /dev/null and b/jacoco/jacocoagent.jar differ
diff --git a/py/h2o.py b/py/h2o.py
index 1247b65..85c6e92 100644
--- a/py/h2o.py
+++ b/py/h2o.py
@@ -856,12 +856,7 @@ class H2O(object):
         post_parameters.update(parameters)
         post_parameters['hyper_parameters'] = grid_parameters
         # gridParams['grid_parameters'] = json.dumps(hyperParameters)
-
-        if search_criteria is not None:
-            if 'strategy' in search_criteria: post_parameters['strategy'] = search_criteria['strategy']
-            if 'max_models' in search_criteria: post_parameters['max_models'] = search_criteria['max_models']
-            if 'max_time_ms' in search_criteria: post_parameters['max_time_ms'] = search_criteria['max_time_ms']
-            if 'seed' in search_criteria: post_parameters['seed'] = search_criteria['seed']
+        post_parameters['search_criteria'] = search_criteria
 
         # print("post_parameters: " + repr(post_parameters))
 
diff --git a/py/rest_tests/test_models.py b/py/rest_tests/test_models.py
index 43c4c7c..defc826 100644
--- a/py/rest_tests/test_models.py
+++ b/py/rest_tests/test_models.py
@@ -61,8 +61,8 @@ def build_and_test(a_node, pp, datasets, algos, algo_additional_default_params):
         # TODO: this should trigger a parameter validation error, but instead the non-grid ntrees silently overrides the grid values:        GridSpec.for_dataset('gbm_iris_multinomial_grid', 'gbm', datasets['iris_multinomial'], { 'ntrees': 5, 'distribution': 'multinomial' }, { 'ntrees': [1, 5, 10], 'max_depth': [1, 3, 5] } ),
 
         # Test stopping criteria:
-        GridSpec.for_dataset('gbm_prostate_regression_grid_max_3', 'gbm', datasets['prostate_regression'], { 'max_depth': 3 }, { 'ntrees': [1, 2, 4], 'distribution': ["gaussian", "poisson", "gamma", "tweedie"] }, { 'strategy': "Random", 'max_models': 3 } ),
-        GridSpec.for_dataset('gbm_prostate_regression_grid_max_20mS', 'gbm', datasets['prostate_regression'], { 'max_depth': 3 }, { 'ntrees': [1, 2, 4], 'distribution': ["gaussian", "poisson", "gamma", "tweedie"] }, { 'strategy': "Random", 'max_time_ms': 20 } ),
+        GridSpec.for_dataset('gbm_prostate_regression_grid_max_3', 'gbm', datasets['prostate_regression'], { 'max_depth': 3 }, { 'ntrees': [1, 2, 4], 'distribution': ["gaussian", "poisson", "gamma", "tweedie"] }, { 'strategy': "RandomDiscrete", 'max_models': 3 } ),
+        GridSpec.for_dataset('gbm_prostate_regression_grid_max_20mS', 'gbm', datasets['prostate_regression'], { 'max_depth': 3 }, { 'ntrees': [1, 2, 4], 'distribution': ["gaussian", "poisson", "gamma", "tweedie"] }, { 'strategy': "RandomDiscrete", 'max_time_ms': 20 } ),
        ]
     
     for grid_spec in grids_to_build:
diff --git a/scripts/mr_unit_failure_report.py b/scripts/mr_unit_failure_report.py
new file mode 100644
index 0000000..27a8586
--- /dev/null
+++ b/scripts/mr_unit_failure_report.py
@@ -0,0 +1,58 @@
+import sys
+import MySQLdb
+import traceback
+import time
+
+def failure_report(args):
+  mr_unit = MySQLdb.connect(host='172.16.2.178', user='root', passwd=args[1], db='mr_unit')
+  mr_unit.autocommit(False)
+  cursor = mr_unit.cursor()
+
+  # the jenkins job is scheduled to run every morning at 8 am (local). By default, we report the failures starting from
+  # 8 pm (local) of the previous day.
+  try:
+    start_time = args[2]
+  except IndexError:
+    yesterday = time.localtime(time.time() - 24 * 60 * 60)
+    start_time = time.mktime(yesterday) - yesterday.tm_hour*60*60 - yesterday.tm_min*60 - yesterday.tm_sec + 20*60*60
+
+  try:
+    end_time = args[3]
+  except IndexError:
+    end_time = time.time()
+
+  try:
+    cursor.execute('select * from perf where (`start_time` > {0} and `end_time` < {1} and '
+                   '`pass` = 0);'.format(start_time, end_time))
+    failures = cursor.fetchall()
+  except:
+    cursor.close()
+    traceback.print_exc()
+    print "Failed to retrieve failures from the perf table in mr_unit database for the period from {0} to {1}"\
+      .format(time.strftime('%Y-%m-%d:%H:%M:%S', time.localtime(start_time)),
+              time.strftime('%Y-%m-%d:%H:%M:%S', time.localtime(end_time)))
+    raise
+  cursor.close()
+
+  print "***********************************************************************"
+  print "Failures for the period from {0} to {1}".format(time.strftime('%Y-%m-%d:%H:%M:%S', time.localtime(start_time)),
+                                                         time.strftime('%Y-%m-%d:%H:%M:%S', time.localtime(end_time)))
+  print "***********************************************************************\n"
+
+  for idx, failure in enumerate(failures):
+    print '\nFAILURE {0}'.format(idx+1)
+    print '------------------------------------------------------------------------------'
+    print 'git branch:                   {0}'.format(failure[3])
+    print 'git hash:                     {0}'.format(failure[2])
+    print 'job name:                     {0}'.format(failure[11])
+    print 'build id:                     {0}'.format(failure[1])
+    print 'test name:                    {0}'.format(failure[5])
+    print 'duration (seconds):           {0}'.format(failure[7] - failure[6])
+    print 'machine ip:                   {0}'.format(failure[4])
+    print 'operating system:             {0}'.format(failure[10])
+    print 'number of cpus:               {0}'.format(failure[9])
+    print 'datetime (%Y-%m-%d:%H:%M:%S): {0}'.format(time.strftime('%Y-%m-%d:%H:%M:%S', time.localtime(failure[6])))
+    print '------------------------------------------------------------------------------\n'
+
+if __name__ == '__main__':
+  failure_report(sys.argv)
\ No newline at end of file
diff --git a/scripts/run.py b/scripts/run.py
index e2506cb..9fa0af2 100755
--- a/scripts/run.py
+++ b/scripts/run.py
@@ -303,6 +303,24 @@ class H2OCloudNode:
                "-baseport", str(self.my_base_port),
                "-ga_opt_out"]
 
+
+        # If the jacoco flag was included, then modify cmd to generate coverage
+        # data using the jacoco agent
+        if g_jacoco_include:
+            root_dir = os.path.abspath(os.path.join(os.path.dirname(__file__),".."))
+            agent_dir = os.path.join(root_dir,"jacoco","jacocoagent.jar")
+            jresults_dir = os.path.join(self.output_dir,"jacoco")
+            if not os.path.exists(jresults_dir):
+                os.mkdir(jresults_dir)
+            jresults_dir += "{cloud}_{node}".format(cloud = self.cloud_num, node = self.node_num)
+            jacoco = "-javaagent:" + agent_dir + "=destfile=" + \
+                     os.path.join(jresults_dir,"{cloud}_{node}.exec".format(cloud=self.cloud_num,
+                                                                            node=self.node_num)) + \
+                     ",includes={inc},excludes={ex}:jsr166y".format(inc=g_jacoco_options[0].replace(',',':'),
+                                                                    ex=g_jacoco_options[1].replace(',',':'))
+            cmd = cmd[:1] + [jacoco] + cmd[1:]
+
+
         # Add S3N credentials to cmd if they exist.
         # ec2_hdfs_config_file_name = os.path.expanduser("~/.ec2/core-site.xml")
         # if (os.path.exists(ec2_hdfs_config_file_name)):
@@ -675,7 +693,6 @@ class Test:
         self.output_file_name = \
             os.path.join(self.output_dir, test_short_dir_with_no_slashes + self.test_name + ".out.txt")
         f = open(self.output_file_name, "w")
-
         self.child = subprocess.Popen(args=cmd, stdout=f, stderr=subprocess.STDOUT, cwd=self.test_dir)
         self.pid = self.child.pid
 
@@ -852,6 +869,8 @@ class Test:
       elif is_ipython_notebook(test_name): cmd = cmd + ["--ipynb"]
       elif is_pydemo(test_name):           cmd = cmd + ["--pyDemo"]
       else:                                cmd = cmd + ["--pyBooklet"]
+      if g_jacoco_include: cmd = cmd + ["--forceConnect"] # When using JaCoCo we don't want the test to return an error
+                                                        # if a cloud reports as unhealthy
       return cmd
 
     def _javascript_cmd(self, test_name, ip, port):
@@ -941,6 +960,7 @@ class TestRunner:
         self.start_seconds = time.time()
         self.terminated = False
         self.clouds = []
+        self.suspicious_clouds = []
         self.bad_clouds = []
         self.tests = []
         self.tests_not_started = []
@@ -1290,14 +1310,16 @@ class TestRunner:
         while (len(self.tests_not_started) > 0):
             if (self.terminated):
                 return
-            completed_test = self._wait_for_one_test_to_complete()
+            cld = self._wait_for_available_cloud(nopass)
+            # Check if no cloud was found
+            if cld is None:
+                self._log('NO GOOD CLOUDS REMAINING...')
+                self.terminate()
+            available_ip, available_port = cld
             if (self.terminated):
                 return
-            self._report_test_result(completed_test, nopass)
-            ip_of_completed_test = completed_test.get_ip()
-            port_of_completed_test = completed_test.get_port()
-            if self._h2o_exists_and_healthy(ip_of_completed_test, port_of_completed_test):
-                self._start_next_test_on_ip_port(ip_of_completed_test, port_of_completed_test)
+            if self._h2o_exists_and_healthy(available_ip, available_port):
+                self._start_next_test_on_ip_port(available_ip, available_port)
 
         # Wait for remaining running tests to complete.
         while (len(self.tests_running) > 0):
@@ -1599,6 +1621,39 @@ class TestRunner:
                 self._log('WAITING FOR ONE TEST TO COMPLETE, BUT THERE ARE NO RUNNING TESTS. EXITING...')
                 sys.exit(1)
 
+    def _wait_for_available_cloud(self, nopass, timeout=60):
+        """
+        Waits for an available cloud to appear by either a test completing or by a cloud on the suspicious_clouds list
+        reporting as healthy, and then returns a tuple containing its ip and port. If no tests are running and no clouds
+        are reporting as healthy, then the function will wait until the designated timeout time expires before returning
+        None.
+        """
+        timer_on = False
+        t_start = None
+        while True:
+            if timer_on:
+                if time.time() - t_start > timeout:
+                    return None
+
+            for ip, port in self.suspicious_clouds:
+                if (self._h2o_exists_and_healthy(ip, port)):
+                    self.suspicious_clouds.remove([ip, port])
+                    return (ip, port)
+
+            if len(self.tests_running) > 0:
+                for test in self.tests_running:
+                    if (test.is_completed()):
+                        self.tests_running.remove(test)
+                        self._report_test_result(test, nopass)
+                        return (test.get_ip(), test.get_port())
+            elif len(self.suspicious_clouds) == 0:
+                self._log('WAITING FOR ONE TEST TO COMPLETE, BUT THERE ARE NO RUNNING TESTS. EXITING...')
+                sys.exit(1)
+            else:
+                t_start = time.time()
+                timer_on = True
+
+
     def _report_test_result(self, test, nopass):
         port = test.get_port()
         finish_seconds = time.time()
@@ -1744,7 +1799,11 @@ class TestRunner:
             http = requests.get("http://{}:{}/{}/{}".format(ip,port,__H2O_REST_API_VERSION__,"Cloud?skip_ticks=true"))
             h2o_okay = http.json()['cloud_healthy']
         except exceptions.ConnectionError: pass
-        if not h2o_okay: self._remove_cloud(ip, port)
+        if not h2o_okay:
+            # JaCoCo tends to cause clouds to temporarily report as unhealthy even when they aren't,
+            # so we'll just consider an unhealthy cloud as suspicious
+            if g_jacoco_include: self._suspect_cloud(ip, port)
+            else: self._remove_cloud(ip, port)
         return h2o_okay
 
     def _remove_cloud(self, ip, port):
@@ -1765,6 +1824,13 @@ class TestRunner:
             self._log('NO GOOD CLOUDS REMAINING...')
             self.terminate()
 
+    def _suspect_cloud(self, ip, port):
+        """
+        add the ip, port to TestRunner's suspicious cloud list. This way the cloud is considered to have the potential
+        to report as being healthy sometime in the future. Unlike _remove_cloud(), the suspicious cloud is not removed from the
+        TestRunner's cloud list.
+        """
+        if not [ip, str(port)] in self.suspicious_clouds: self.suspicious_clouds.append([ip, str(port)])
 # --------------------------------------------------------------------
 # Main program
 # --------------------------------------------------------------------
@@ -1795,6 +1861,8 @@ g_jvm_xmx = "1g"
 g_nopass = False
 g_nointernal = False
 g_convenient = False
+g_jacoco_include = False
+g_jacoco_options = ["",""]
 g_path_to_h2o_jar = None
 g_path_to_tar = None
 g_path_to_whl = None
@@ -1934,6 +2002,10 @@ def usage():
     print("                     pass, ncpus, os, and job name of each test to perf.csv in the results directory.")
     print("                     Takes three parameters: git hash, git branch, and build id, job name in that order.")
     print("")
+    print("    --jacoco         Generate code coverage data using JaCoCo. Class includes and excludes may optionally")
+    print("                     follow in the format of [includes]:[excludes] where [...] denotes a list of")
+    print("                     classes, each separated by a comma (,). Wildcard characters (* and ?) may be used.")
+    print("")
     print("    --geterrs        Generate xml file that contains the actual unit test errors and the actual Java error.")
     print("")
     print("    If neither --test nor --testlist is specified, then the list of tests is")
@@ -1967,6 +2039,8 @@ def usage():
     print("")
     print("    Run tests on a pre-existing cloud (e.g. in a debugger), keeping old random seeds:")
     print("        "+g_script_name+" --wipe --usecloud ip:port")
+    print("    Run tests with JaCoCo enabled, excluding org.example1 and org.example2")
+    print("        "+g_script_name+" --jacoco :org.example1,org.example2")
     sys.exit(1)
 
 
@@ -2019,6 +2093,8 @@ def parse_args(argv):
     global g_path_to_h2o_jar
     global g_path_to_tar
     global g_path_to_whl
+    global g_jacoco_include
+    global g_jacoco_options
     global g_produce_unit_reports
     global g_phantomjs_to
     global g_phantomjs_packs
@@ -2046,27 +2122,27 @@ def parse_args(argv):
 
         if (s == "--baseport"):
             i += 1
-            if (i > len(argv)):
+            if (i >= len(argv)):
                 usage()
             g_base_port = int(argv[i])
         elif s == "--py3":
             i += 1
-            if i > len(argv):
+            if i >= len(argv):
                 usage()
             g_py3 = True
         elif s == "--coverage":
             i += 1
-            if i > len(argv):
+            if i >= len(argv):
               usage()
             g_pycoverage = True
         elif (s == "--numclouds"):
             i += 1
-            if (i > len(argv)):
+            if (i >= len(argv)):
                 usage()
             g_num_clouds = int(argv[i])
         elif (s == "--numnodes"):
             i += 1
-            if (i > len(argv)):
+            if (i >= len(argv)):
                 usage()
             g_nodes_per_cloud = int(argv[i])
         elif (s == "--wipeall"):
@@ -2076,27 +2152,27 @@ def parse_args(argv):
             g_wipe_output_dir = True
         elif (s == "--test"):
             i += 1
-            if (i > len(argv)):
+            if (i >= len(argv)):
                 usage()
             g_test_to_run = TestRunner.find_test(argv[i])
         elif (s == "--testlist"):
             i += 1
-            if (i > len(argv)):
+            if (i >= len(argv)):
                 usage()
             g_test_list_file = argv[i]
         elif (s == "--excludelist"):
             i += 1
-            if (i > len(argv)):
+            if (i >= len(argv)):
                 usage()
             g_exclude_list_file = argv[i]
         elif (s == "--testgroup"):
             i += 1
-            if (i > len(argv)):
+            if (i >= len(argv)):
                 usage()
             g_test_group = argv[i]
         elif (s == "--testsize"):
             i += 1
-            if (i > len(argv)):
+            if (i >= len(argv)):
                 usage()
             v = argv[i]
             if (re.match(r'(s)?(m)?(l)?', v)):
@@ -2112,7 +2188,7 @@ def parse_args(argv):
                 bad_arg(s)
         elif (s == "--usecloud"):
             i += 1
-            if (i > len(argv)):
+            if (i >= len(argv)):
                 usage()
             s = argv[i]
             m = re.match(r'(\S+):([1-9][0-9]*)', s)
@@ -2124,7 +2200,7 @@ def parse_args(argv):
             g_use_port = int(port_string)
         elif (s == "--usecloud2"):
             i += 1
-            if (i > len(argv)):
+            if (i >= len(argv)):
                 usage()
             s = argv[i]
             if (s is None):
@@ -2156,13 +2232,21 @@ def parse_args(argv):
             g_path_to_whl = os.path.abspath(argv[i])
         elif (s == "--jvm.xmx"):
             i += 1
-            if (i > len(argv)):
+            if (i >= len(argv)):
                 usage()
             g_jvm_xmx = argv[i]
         elif (s == "--norun"):
             g_no_run = True
         elif (s == "--noxunit"):
             g_produce_unit_reports = False
+        elif (s == "--jacoco"):
+            g_jacoco_include = True
+            if (i + 1 < len(argv)):
+                s = argv[i + 1]
+                m = re.match(r'(?P<includes>([^:,]+(,[^:,]+)*)?):(?P<excludes>([^:,]+(,[^:,]+)*)?)$', s)
+                if m is not None:
+                    g_jacoco_options[0] = m.group("includes")
+                    g_jacoco_options[1] = m.group("excludes")
         elif (s == "-h" or s == "--h" or s == "-help" or s == "--help"):
             usage()
         elif (s == "--rPkgVerChk"):
@@ -2171,35 +2255,35 @@ def parse_args(argv):
             g_on_hadoop = True
         elif (s == "--hadoopNamenode"):
             i += 1
-            if (i > len(argv)):
+            if (i >= len(argv)):
                 usage()
             g_hadoop_namenode = argv[i]
         elif (s == "--perf"):
             g_perf = True
 
             i += 1
-            if (i > len(argv)):
+            if (i >= len(argv)):
                 usage()
             g_git_hash = argv[i]
 
             i += 1
-            if (i > len(argv)):
+            if (i >= len(argv)):
                 usage()
             g_git_branch = argv[i]
 
             i += 1
-            if (i > len(argv)):
+            if (i >= len(argv)):
                 usage()
             g_build_id = argv[i]
 
             i += 1
-            if (i > len(argv)):
+            if (i >= len(argv)):
                 usage()
             g_job_name = argv[i]
         elif (s == "--geterrs"):
             g_use_xml2 = True
         else:
-            unknown_arg(s)
+           unknown_arg(s)
 
         i += 1
 
diff --git a/vagrant/bootstrap.sh b/vagrant/bootstrap.sh
index 2a45e1a..6b91c7e 100644
--- a/vagrant/bootstrap.sh
+++ b/vagrant/bootstrap.sh
@@ -35,10 +35,10 @@ sudo apt-get install -y \
   python-pip
 
 # Python packages
-sudo pip install grip tabulate wheel numpy scikit-learn scipy
+sudo pip install grip tabulate wheel numpy scikit-learn scipy requests future
 
 # R
-sudo apt-get install -y r-base
+sudo apt-get install -y r-base-core=3.2.2-1trusty0
 
 # R packages
 export R_LIBS_USER=$HOME/R/libs
