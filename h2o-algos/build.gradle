//
// H2O Algos Module
//
description = "H2O Algorithms"

dependencies {

  compile files('bazel-lib/guava-19.0.jar')
  compile project(":h2o-core")
  testCompile "junit:junit:${junitVersion}"
  testCompile project(path: ":h2o-core", configuration: "testArchives")
  compile files('water.gpu.jar')
  compile files('javacpp.jar')
  compile files('tensorflow.jar')
  compile files('tensorflow-linux-x86_64.jar')

  compile files("cuda.jar")
  compile files("cuda-linux-x86_64.jar")

  compile "com.google.protobuf:protobuf-java:3.0.0"

  compile files('jython-standalone-2.7.0.jar')

  compile files('jnumeric-0.2-SNAPSHOT.jar')

  compile files('byte-buddy-dep-0.7-rc6.jar')
  testCompile files("libtestutil.jar")
}

apply from: "${rootDir}/gradle/dataCheck.gradle"

// The default 'test' behavior is broken in that it does not grok clusters.
// For H2O, all tests need to be run on a cluster, where each JVM is
// "free-running" - it's stdout/stderr are NOT hooked by another process.  If
// they are hooked (e.g., by the gradle driver process) then the stdout/err get
// buffered and when all CPUs are maxed out (happens over a large fraction of
// the test run) no output is visible.  If the JVMs then crash (again, common
// enough), we get NO output for the test run.  So instead we need to arrange a
// complete cluster of free-running JVMs and redirect all output (at the OS
// level) to files - then scrape the files later for test results.
test {
  dependsOn ":h2o-core:testJar"
  dependsOn smalldataCheck, cpLibs, jar, testJar, testSingleNode, testMultiNode

  // Defeat task 'test' by running no tests.
  exclude '**'
}

testMultiNode.shouldRunAfter testSingleNode
