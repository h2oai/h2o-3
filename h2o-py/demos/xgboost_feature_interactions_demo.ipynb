{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost feature interactions demo\n",
    "\n",
    "This demo demonstrates how to use two XGBoost improvements using H2O XGBoost integration - **feature interaction contraints** and geting **feature interactions** from the model.\n",
    "\n",
    "**More information:**\n",
    "\n",
    "- H2O XGboost interaction constraints documentation: http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/algo-params/interaction_constraints.html\n",
    "- Native XGBoost interaction contraints tutorial: https://xgboost.readthedocs.io/en/latest/tutorials/feature_interaction_constraint.html\n",
    "- H2O XGboost feature interaction documentation: https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/xgboost.html#xgboost-feature-interactions\n",
    "- Original XGBFI package: https://github.com/Far0n/xgbfi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Interaction Constraints\n",
    "\n",
    "Feature interaction constraints allow users to decide which variables are allowed to interact and which are not.\n",
    "\n",
    "**Potential benefits include:**\n",
    "\n",
    "- Better predictive performance from focusing on interactions that work â€“ whether through domain specific knowledge or algorithms that rank interactions\n",
    "- Less noise in predictions; better generalization\n",
    "- More control to the user on what the model can fit. For example, the user may want to exclude some interactions even if they perform well due to regulatory constraints\n",
    "\n",
    "(Source: http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/algo-params/interaction_constraints.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start h2o\n",
    "import h2o\n",
    "h2o.init(strict_version_check=False, port=54321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2o.estimators.xgboost import *\n",
    "# check if the H2O XGBoostEstimator is available\n",
    "assert H2OXGBoostEstimator.available() is True\n",
    "\n",
    "# import data\n",
    "data = h2o.import_file(path = \"../../smalldata/logreg/prostate.csv\")\n",
    "\n",
    "x = list(range(1, data.ncol-2))\n",
    "y = data.names[len(data.names) - 1]\n",
    "\n",
    "ntree = 5\n",
    "\n",
    "h2o_params = {\n",
    "    'eta': 0.3, \n",
    "    'max_depth': 3,  \n",
    "    'ntrees': ntree,\n",
    "    'tree_method': 'hist'\n",
    "} \n",
    "\n",
    "# define interactions as a list of list of names of colums\n",
    "# the lists defines allowed interaction\n",
    "# the interactions of each column with itself are always allowed\n",
    "# so you cannot specified list with one column e.g. [\"PSA\"]\n",
    "h2o_params[\"interaction_constraints\"] = [[\"CAPSULE\", \"AGE\"], [\"PSA\", \"DPROS\"]]\n",
    "\n",
    "# train h2o XGBoost model\n",
    "h2o_model = H2OXGBoostEstimator(**h2o_params)\n",
    "h2o_model.train(x=x, y=y, training_frame=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the trees have allowed structure\n",
    "# so in each tree can be as split feature only \n",
    "from h2o.tree import H2OTree\n",
    "for i in range(0, ntree):\n",
    "    print(\"Tree index:\"+str(i))\n",
    "    tree = H2OTree(h2o_model, i)\n",
    "    for i in range(0, len(tree)):\n",
    "        if tree.left_children[i] == -1:\n",
    "            print(\"Leaf ID {0}.\".format(tree.node_ids[i]))\n",
    "        else:\n",
    "            print(\"Node ID {0} has left child node with index {1} and right child node with index {2}. The split feature is {3}.\".format(tree.node_ids[i], tree.left_children[i], tree.right_children[i], tree.features[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import xgboost as xgb\n",
    "    import pandas as pd\n",
    "    data = pd.read_csv(\"../../smalldata/logreg/prostate.csv\")\n",
    "\n",
    "    y = data[\"GLEASON\"]\n",
    "    x_names = data.columns.to_list()\n",
    "    x_names.remove(\"GLEASON\")\n",
    "    x_names.remove(\"ID\")\n",
    "    x = data[x_names]\n",
    "\n",
    "\n",
    "    D_train = xgb.DMatrix(x, label=y)\n",
    "\n",
    "    param = {\n",
    "        'eta': 0.3, \n",
    "        'max_depth': 3,  \n",
    "        'interaction_constraints': '[[0,1], [3, 5]]', # same as [[\"CAPSULE\", \"AGE\"], [\"PSA\", \"DPROS\"]]\n",
    "        'tree_method': 'hist'\n",
    "    } \n",
    "\n",
    "    steps = ntree\n",
    "\n",
    "    xgboost_model = xgb.train(param, D_train, steps)\n",
    "    # you can compare the H2O XGBoost and native XGBoost have the same tree structure\n",
    "    xgboost_model.trees_to_dataframe()\n",
    "except ImportError as e:\n",
    "    print(e)\n",
    "    xgboost_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot xgboost trees\n",
    "try:\n",
    "    if xgboost_model is not None:\n",
    "        from xgboost import plot_tree\n",
    "        import matplotlib.pyplot as plt\n",
    "        from matplotlib.pylab import rcParams\n",
    "\n",
    "        rcParams['figure.figsize'] = 50, 80\n",
    "        for i in range(0, ntree):\n",
    "            plot_tree(xgboost_model, num_trees=i)\n",
    "except Exception as e: print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show xgboost model variable importance\n",
    "if xgboost_model is not None:\n",
    "    print(xgboost_model.get_score(importance_type='total_gain'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show h2o xgboost model variable importance\n",
    "h2o_model.varimp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBFI-like Feature Interaction\n",
    "\n",
    "In the code below, multiple table output provides comprendious insights into higher order interactions between the features of xgboost trees visualized above. Also additional usefull tables summarizing leaf statistics and split value histograms per each feature are provided. Measures used are either one of:\n",
    "\n",
    "**Gain** implies the relative contribution of the corresponding feature to the model calculated by taking each feature's contribution for each tree in the model. A higher value of this metric when compared to another feature implies it is more important for generating a prediction.\n",
    "\n",
    "**Cover** is a metric to measure the number of observations affected by the split. Counted over the specific feature it measures the relative quantity of observations concerned by a feature.\n",
    "\n",
    "**Frequency (FScore)** is the number of times a feature is used in all generated trees. Please note that it does not take the tree-depth nor tree-index of splits a feature occurs into consideration, neither the amount of possible splits of a feature. Hence, it is often suboptimal measure for importance.\n",
    "\n",
    "\n",
    "or their averaged / weighed / ranked alternatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate multi-level feature interactions\n",
    "h2o_model.feature_interaction()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
