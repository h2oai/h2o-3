# This file is auto-generated by h2o-3/h2o-bindings/bin/gen_R.py
# Copyright 2016 H2O.ai;  Apache License Version 2.0 (see LICENSE for details) 
#'
# -------------------------- XGBoost -------------------------- #
#' 
#' Builds a eXtreme Gradient Boosting model using the native XGBoost backend
#' 
#' @param x A vector containing the names or indices of the predictor variables to use in building the model.
#'        If x is missing,then all columns except y are used.
#' @param y The name of the response variable in the model.If the data does not contain a header, this is the column index
#'        number starting at 0, and increasing from left to right. (The response must be either an integer or a
#'        categorical variable).
#' @param model_id Destination id for this model; auto-generated if not specified.
#' @param training_frame Id of the training data frame (Not required, to allow initial validation of model parameters).
#' @param validation_frame Id of the validation data frame.
#' @param nfolds Number of folds for N-fold cross-validation (0 to disable or >= 2). Defaults to 0.
#' @param keep_cross_validation_predictions \code{Logical}. Whether to keep the predictions of the cross-validation models. Defaults to FALSE.
#' @param keep_cross_validation_fold_assignment \code{Logical}. Whether to keep the cross-validation fold assignment. Defaults to FALSE.
#' @param score_each_iteration \code{Logical}. Whether to score during each iteration of model training. Defaults to FALSE.
#' @param fold_assignment Cross-validation fold assignment scheme, if fold_column is not specified. The 'Stratified' option will
#'        stratify the folds based on the response variable, for classification problems. Must be one of: "AUTO",
#'        "Random", "Modulo", "Stratified". Defaults to AUTO.
#' @param fold_column Column with cross-validation fold index assignment per observation.
#' @param ignore_const_cols \code{Logical}. Ignore constant columns. Defaults to TRUE.
#' @param offset_column Offset column. This will be added to the combination of columns before applying the link function.
#' @param weights_column Column with observation weights. Giving some observation a weight of zero is equivalent to excluding it from
#'        the dataset; giving an observation a relative weight of 2 is equivalent to repeating that row twice. Negative
#'        weights are not allowed.
#' @param stopping_rounds Early stopping based on convergence of stopping_metric. Stop if simple moving average of length k of the
#'        stopping_metric does not improve for k:=stopping_rounds scoring events (0 to disable) Defaults to 0.
#' @param stopping_metric Metric to use for early stopping (AUTO: logloss for classification, deviance for regression) Must be one of:
#'        "AUTO", "deviance", "logloss", "MSE", "RMSE", "MAE", "RMSLE", "AUC", "lift_top_group", "misclassification",
#'        "mean_per_class_error". Defaults to AUTO.
#' @param stopping_tolerance Relative tolerance for metric-based stopping criterion (stop if relative improvement is not at least this
#'        much) Defaults to 0.001.
#' @param max_runtime_secs Maximum allowed runtime in seconds for model training. Use 0 to disable. Defaults to 0.
#' @param seed Seed for random numbers (affects certain parts of the algo that are stochastic and those might or might not be enabled by default)
#'        Defaults to -1 (time-based random number).
#' @param distribution Distribution function Must be one of: "AUTO", "bernoulli", "multinomial", "gaussian", "poisson", "gamma",
#'        "tweedie", "laplace", "quantile", "huber". Defaults to AUTO.
#' @param tweedie_power Tweedie power for Tweedie regression, must be between 1 and 2. Defaults to 1.5.
#' @param ntrees (same as n_estimators) Number of trees. Defaults to 50.
#' @param max_depth Maximum tree depth. Defaults to 5.
#' @param min_rows (same as min_child_weight) Fewest allowed (weighted) observations in a leaf. Defaults to 10.
#' @param min_child_weight (same as min_rows) Fewest allowed (weighted) observations in a leaf. Defaults to 0.
#' @param learn_rate (same as eta) Learning rate (from 0.0 to 1.0) Defaults to 0.1.
#' @param eta (same as learn_rate) Learning rate (from 0.0 to 1.0) Defaults to 0.
#' @param sample_rate (same as subsample) Row sample rate per tree (from 0.0 to 1.0) Defaults to 1.
#' @param subsample (same as sample_rate) Row sample rate per tree (from 0.0 to 1.0) Defaults to 0.
#' @param col_sample_rate (same as colsample_bylevel) Column sample rate (from 0.0 to 1.0) Defaults to 1.
#' @param colsample_bylevel (same as col_sample_rate) Column sample rate (from 0.0 to 1.0) Defaults to 0.
#' @param col_sample_rate_per_tree (same as colsample_bytree) Column sample rate per tree (from 0.0 to 1.0) Defaults to 1.
#' @param colsample_bytree (same as col_sample_rate_per_tree) Column sample rate per tree (from 0.0 to 1.0) Defaults to 0.
#' @param max_abs_leafnode_pred (same as max_delta_step) Maximum absolute value of a leaf node prediction Defaults to 3.4028235e+38.
#' @param max_delta_step (same as max_abs_leafnode_pred) Maximum absolute value of a leaf node prediction Defaults to 0.0.
#' @param score_tree_interval Score the model after every so many trees. Disabled if set to 0. Defaults to 0.
#' @param min_split_improvement (same as gamma) Minimum relative improvement in squared error reduction for a split to happen Defaults to 0.0.
#' @param max_bin For tree_method=hist only: maximum number of bins Defaults to 255.
#' @param num_leaves For tree_method=hist only: maximum number of leaves Defaults to 255.
#' @param min_sum_hessian_in_leaf For tree_method=hist only: the mininum sum of hessian in a leaf to keep splitting Defaults to 100.0.
#' @param min_data_in_leaf For tree_method=hist only: the mininum data in a leaf to keep splitting Defaults to 0.0.
#' @param tree_method Tree method Must be one of: "auto", "exact", "approx", "hist". Defaults to auto.
#' @param grow_policy Grow policy - depthwise is standard GBM, lossguide is LightGBM Must be one of: "depthwise", "lossguide".
#'        Defaults to depthwise.
#' @param booster Booster type Must be one of: "gbtree", "gblinear", "dart". Defaults to gbtree.
#' @param gamma (same as min_split_improvement) Minimum relative improvement in squared error reduction for a split to happen
#'        Defaults to 0.0.
#' @param reg_lambda L2 regularization Defaults to 1.0.
#' @param reg_alpha L1 regularization Defaults to 0.0.
#' @export
h2o.xgboost <- function(x, y, training_frame,
                        model_id = NULL,
                        validation_frame = NULL,
                        nfolds = 0,
                        keep_cross_validation_predictions = FALSE,
                        keep_cross_validation_fold_assignment = FALSE,
                        score_each_iteration = FALSE,
                        fold_assignment = c("AUTO", "Random", "Modulo", "Stratified"),
                        fold_column = NULL,
                        ignore_const_cols = TRUE,
                        offset_column = NULL,
                        weights_column = NULL,
                        stopping_rounds = 0,
                        stopping_metric = c("AUTO", "deviance", "logloss", "MSE", "RMSE", "MAE", "RMSLE", "AUC", "lift_top_group", "misclassification", "mean_per_class_error"),
                        stopping_tolerance = 0.001,
                        max_runtime_secs = 0,
                        seed = -1,
                        distribution = c("AUTO", "bernoulli", "multinomial", "gaussian", "poisson", "gamma", "tweedie", "laplace", "quantile", "huber"),
                        tweedie_power = 1.5,
                        ntrees = 50,
                        max_depth = 5,
                        min_rows = 10,
                        min_child_weight = 0,
                        learn_rate = 0.1,
                        eta = 0,
                        sample_rate = 1,
                        subsample = 0,
                        col_sample_rate = 1,
                        colsample_bylevel = 0,
                        col_sample_rate_per_tree = 1,
                        colsample_bytree = 0,
                        max_abs_leafnode_pred = 3.4028235e+38,
                        max_delta_step = 0.0,
                        score_tree_interval = 0,
                        min_split_improvement = 0.0,
                        max_bin = 255,
                        num_leaves = 255,
                        min_sum_hessian_in_leaf = 100.0,
                        min_data_in_leaf = 0.0,
                        tree_method = c("auto", "exact", "approx", "hist"),
                        grow_policy = c("depthwise", "lossguide"),
                        booster = c("gbtree", "gblinear", "dart"),
                        gamma = 0.0,
                        reg_lambda = 1.0,
                        reg_alpha = 0.0
                        ) 
{
  #If x is missing, then assume user wants to use all columns as features.
  if(missing(x)){
     if(is.numeric(y)){
         x <- setdiff(col(training_frame),y)
     }else{
         x <- setdiff(colnames(training_frame),y)
     }
  }

  # Required args: training_frame
  if( missing(training_frame) ) stop("argument 'training_frame' is missing, with no default")
  # Training_frame must be a key or an H2OFrame object
  if (!is.H2OFrame(training_frame))
     tryCatch(training_frame <- h2o.getFrame(training_frame),
           error = function(err) {
             stop("argument 'training_frame' must be a valid H2OFrame or key")
           })
  # Validation_frame must be a key or an H2OFrame object
  if (!is.null(validation_frame)) {
     if (!is.H2OFrame(validation_frame))
         tryCatch(validation_frame <- h2o.getFrame(validation_frame),
             error = function(err) {
                 stop("argument 'validation_frame' must be a valid H2OFrame or key")
             })
  }
  # Parameter list to send to model builder
  parms <- list()
  parms$training_frame <- training_frame
  args <- .verify_dataxy(training_frame, x, y)
  if( !missing(offset_column) && !is.null(offset_column))  args$x_ignore <- args$x_ignore[!( offset_column == args$x_ignore )]
  if( !missing(weights_column) && !is.null(weights_column)) args$x_ignore <- args$x_ignore[!( weights_column == args$x_ignore )]
  if( !missing(fold_column) && !is.null(fold_column)) args$x_ignore <- args$x_ignore[!( fold_column == args$x_ignore )]
  parms$ignored_columns <- args$x_ignore
  parms$response_column <- args$y

  if (!missing(model_id))
    parms$model_id <- model_id
  if (!missing(validation_frame))
    parms$validation_frame <- validation_frame
  if (!missing(nfolds))
    parms$nfolds <- nfolds
  if (!missing(keep_cross_validation_predictions))
    parms$keep_cross_validation_predictions <- keep_cross_validation_predictions
  if (!missing(keep_cross_validation_fold_assignment))
    parms$keep_cross_validation_fold_assignment <- keep_cross_validation_fold_assignment
  if (!missing(score_each_iteration))
    parms$score_each_iteration <- score_each_iteration
  if (!missing(fold_assignment))
    parms$fold_assignment <- fold_assignment
  if (!missing(fold_column))
    parms$fold_column <- fold_column
  if (!missing(ignore_const_cols))
    parms$ignore_const_cols <- ignore_const_cols
  if (!missing(offset_column))
    parms$offset_column <- offset_column
  if (!missing(weights_column))
    parms$weights_column <- weights_column
  if (!missing(stopping_rounds))
    parms$stopping_rounds <- stopping_rounds
  if (!missing(stopping_metric))
    parms$stopping_metric <- stopping_metric
  if (!missing(stopping_tolerance))
    parms$stopping_tolerance <- stopping_tolerance
  if (!missing(max_runtime_secs))
    parms$max_runtime_secs <- max_runtime_secs
  if (!missing(seed))
    parms$seed <- seed
  if (!missing(distribution))
    parms$distribution <- distribution
  if (!missing(tweedie_power))
    parms$tweedie_power <- tweedie_power
  if (!missing(ntrees))
    parms$ntrees <- ntrees
  if (!missing(max_depth))
    parms$max_depth <- max_depth
  if (!missing(min_rows))
    parms$min_rows <- min_rows
  if (!missing(min_child_weight))
    parms$min_child_weight <- min_child_weight
  if (!missing(learn_rate))
    parms$learn_rate <- learn_rate
  if (!missing(eta))
    parms$eta <- eta
  if (!missing(sample_rate))
    parms$sample_rate <- sample_rate
  if (!missing(subsample))
    parms$subsample <- subsample
  if (!missing(col_sample_rate))
    parms$col_sample_rate <- col_sample_rate
  if (!missing(colsample_bylevel))
    parms$colsample_bylevel <- colsample_bylevel
  if (!missing(col_sample_rate_per_tree))
    parms$col_sample_rate_per_tree <- col_sample_rate_per_tree
  if (!missing(colsample_bytree))
    parms$colsample_bytree <- colsample_bytree
  if (!missing(max_abs_leafnode_pred))
    parms$max_abs_leafnode_pred <- max_abs_leafnode_pred
  if (!missing(max_delta_step))
    parms$max_delta_step <- max_delta_step
  if (!missing(score_tree_interval))
    parms$score_tree_interval <- score_tree_interval
  if (!missing(min_split_improvement))
    parms$min_split_improvement <- min_split_improvement
  if (!missing(max_bin))
    parms$max_bin <- max_bin
  if (!missing(num_leaves))
    parms$num_leaves <- num_leaves
  if (!missing(min_sum_hessian_in_leaf))
    parms$min_sum_hessian_in_leaf <- min_sum_hessian_in_leaf
  if (!missing(min_data_in_leaf))
    parms$min_data_in_leaf <- min_data_in_leaf
  if (!missing(tree_method))
    parms$tree_method <- tree_method
  if (!missing(grow_policy))
    parms$grow_policy <- grow_policy
  if (!missing(booster))
    parms$booster <- booster
  if (!missing(gamma))
    parms$gamma <- gamma
  if (!missing(reg_lambda))
    parms$reg_lambda <- reg_lambda
  if (!missing(reg_alpha))
    parms$reg_alpha <- reg_alpha
  # Error check and build model
  .h2o.modelJob('xgboost', parms, h2oRestApiVersion=3) 
}

#' Ask the H2O server whether a XGBoost model can be built (depends on availability of native backend)
#' Returns True if a XGBoost model can be built, or False otherwise.
#' @param h2oRestApiVersion (Optional) Specific version of the REST API to use
#'
h2o.xgboost.available <- function(h2oRestApiVersion = .h2o.__REST_API_VERSION) {
visibility = .h2o.__remoteSend(method = "GET", h2oRestApiVersion = h2oRestApiVersion, .h2o.__MODEL_BUILDERS("xgboost"))$model_builders[["xgboost"]][["visibility"]]
if (visibility == "Experimental") {
print("Cannot build a XGboost model - no backend found.")
return(FALSE)
} else {
return(TRUE)
}
}

